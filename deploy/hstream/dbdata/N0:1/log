I0917 02:10:17.011020     308 [logdeviced-main] main.cpp:399] main() Command line arguments: --local-log-store-path /data/store/N0:1/db --num-shards 2 --server-id mqni60ugqy --log-file /data/store/N0:1/log --name Node0 --config-path file:/data/store/logdevice.conf --client-thrift-api-port 39371 --gossip-port 39365 --server-thrift-api-port 39370 --test-mode true --server-to-server-port 39369 --admin-port 39366 --loglevel info --port 39363 --sequencers lazy 
I0917 02:10:17.012002     308 [logdeviced-main] main.cpp:403] main() Plugins loaded: {"Thrift client factory plugin":"built-in","ZookeeperClient factory":"built-in","ConfigSource factory":["built-in"],"PermissionChecker factory":"built-in","Build Info":"built-in","Plugin provider":["Built-in plugin provider","Static plug-in loader","Dynamic plugin loader"]}
I0917 02:10:17.012621     308 [logdeviced-main] main.cpp:405] main() server starting
I0917 02:10:17.013443     308 [logdeviced-main] main.cpp:410] main() version 99.99.99
I0917 02:10:17.014060     308 [logdeviced-main] main.cpp:418] main() asserts off (NDEBUG set)
I0917 02:10:17.015810     308 [logdeviced-main] TextConfigUpdater.cpp:56] fetchFromSource() Requesting main config "/data/store/logdevice.conf" (source: file)
I0917 02:10:17.017914     308 [logdeviced-main] TextConfigUpdater.cpp:116] onContents() Main config from file, hash = 9egjnxotid
I0917 02:10:17.194017     308 [logdeviced-main] Server.cpp:185] operator()() Updating settings from config took 174ms
I0917 02:10:17.194754     308 [logdeviced-main] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 175ms, config update: 0ms, notifications: 0ms
I0917 02:10:17.195458     308 [logdeviced-main] TextConfigUpdater.cpp:306] pushServerConfig() Updated config (version: 1 - hash: 9egjnxotid)
I0917 02:10:17.196223     308 [logdeviced-main] TextConfigUpdater.cpp:397] pushLogsConfig() Ignoring changes in the 'logs' section in the config file because LogsConfigManager is ENABLED
I0917 02:10:17.196948     308 [logdeviced-main] TextConfigUpdater.cpp:174] update() Config update result: ServerConfig=updated, ZK=skipped, LogsConfig=updated
I0917 02:10:17.198203     308 [logdeviced-main] FileBasedVersionedConfigStore.cpp:35] FileBasedVersionedConfigStore() FileBasedVersionedConfigStore threads started.
I0917 02:10:17.201915     308 [logdeviced-main] NodesConfigurationInit.cpp:233] operator()() Fetching initial nodes configuration (Attempt 1)...
I0917 02:10:17.209151     321 [ld:file-ncs] NodesConfigurationInit.cpp:290] operator()() Got a NodesConfiguration of version: 5
I0917 02:10:17.210333     321 [ld:file-ncs] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 0ms, config update: 0ms, notifications: 0ms
I0917 02:10:17.211662     308 [logdeviced-main] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 0ms, config update: 0ms, notifications: 0ms
I0917 02:10:17.212576     308 [logdeviced-main] Server.cpp:510] init() My Node ID is N0:1
I0917 02:10:17.213159     308 [logdeviced-main] Server.cpp:517] init() My version is 0
I0917 02:10:17.213998     308 [logdeviced-main] NoopTraceLogger.cpp:22] NoopTraceLogger() NOOP TraceLogger is ON, no trace samples will be published.
I0917 02:10:17.215821     308 [logdeviced-main] FileBasedVersionedConfigStore.cpp:46] stopAndJoin() FileBasedVersionedConfigStore threads stopped.
I0917 02:10:17.216574     308 [logdeviced-main] main.cpp:144] set_fd_limit() Limit on number of file descriptors is 1048576
W0917 02:10:17.217169     308 [logdeviced-main] main.cpp:192] drop_root() Running under root, but no --user command line option specified. Will keep running as root
I0917 02:10:17.218412     308 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 39363
I0917 02:10:17.219711     308 [logdeviced-main] Server.cpp:781] initListeners() Initializing a gossip listener.
I0917 02:10:17.221196     308 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 39365
I0917 02:10:17.226781     308 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 39369 (SSL)
I0917 02:10:17.228711     308 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 39373 (SSL)
I0917 02:10:17.230404     308 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 39372 (SSL)
I0917 02:10:17.257620     366 [io-stall:s0] IOTracing.cpp:83] stallDetectionThreadMain() Started IO stall detection thread for shard 0
I0917 02:10:17.258153     367 [io-stall:s1] IOTracing.cpp:83] stallDetectionThreadMain() Started IO stall detection thread for shard 1
I0917 02:10:17.259037     308 [logdeviced-main] RocksDBLogStoreConfig.cpp:162] RocksDBLogStoreConfig() LD manages flushes and all memory budgets on the node. atomic_flush is set to true.
I0917 02:10:17.275163     390 [ld:open-rocks0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:387] Created background thread for deletion scheduler with rate_bytes_per_sec: 100000000
I0917 02:10:17.275784     391 [ld:open-rocks1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:387] Created background thread for deletion scheduler with rate_bytes_per_sec: 100000000
I0917 02:10:17.536701     391 [ld:open-rocks1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000015.log.trash as trash -- OK
I0917 02:10:17.554925     393 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 0after deleting file /data/store/N0:1/db/shard1/000015.log.trash
I0917 02:10:17.583435     390 [ld:open-rocks0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000015.log.trash as trash -- OK
I0917 02:10:17.595496     392 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 0after deleting file /data/store/N0:1/db/shard0/000015.log.trash
I0917 02:10:17.609971     391 [ld:open-rocks1] PartitionedRocksDBStore.cpp:897] open() Shard[1] dirty ranges: []
I0917 02:10:17.615691     391 [ld:open-rocks1] PartitionedRocksDBStore.cpp:1491] convertDataKeyFormat() DataKey format conversion took 0.000s. Read 1 records, 107 bytes. Wrote 0 records, 0 bytes.
I0917 02:10:17.640934     390 [ld:open-rocks0] PartitionedRocksDBStore.cpp:897] open() Shard[0] dirty ranges: []
I0917 02:10:17.645775     390 [ld:open-rocks0] PartitionedRocksDBStore.cpp:1491] convertDataKeyFormat() DataKey format conversion took 0.000s. Read 2 records, 214 bytes. Wrote 0 records, 0 bytes.
I0917 02:10:17.683499     376 [rocks-high-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000021.log.trash as trash -- OK
I0917 02:10:17.690768     391 [ld:open-rocks1] ShardedRocksDBLocalLogStore.cpp:232] operator()() Opened RocksDB instance at /data/store/N0:1/db/shard1 in 408 ms
I0917 02:10:17.691084     393 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 0after deleting file /data/store/N0:1/db/shard1/000021.log.trash
I0917 02:10:17.702434     377 [rocks-high-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000021.log.trash as trash -- OK
I0917 02:10:17.709903     390 [ld:open-rocks0] ShardedRocksDBLocalLogStore.cpp:232] operator()() Opened RocksDB instance at /data/store/N0:1/db/shard0 in 432 ms
I0917 02:10:17.712624     392 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 0after deleting file /data/store/N0:1/db/shard0/000021.log.trash
I0917 02:10:17.724032     308 [logdeviced-main] ShardedRocksDBLocalLogStore.cpp:691] printDiskShardMapping() Disk -> Shard mapping: /data/store/N0:1/db/shard0 -> [0,1]
I0917 02:10:17.725536     308 [logdeviced-main] ShardedRocksDBLocalLogStore.cpp:285] init() Initialized sharded RocksDB instance at /data/store/N0:1/db with 2 shards
E0917 02:10:17.727711    1122 [ld:populateLogS] ThreadID.cpp:26] set() Truncating thread name "ld:populateLogState1"!
E0917 02:10:17.727711    1120 [ld:populateLogS] ThreadID.cpp:26] set() Truncating thread name "ld:populateLogState0"!
I0917 02:10:17.734966     308 [logdeviced-main] Server.cpp:1173] initLogStorageStateMap() Populating log storage state map successful.
E0917 02:10:17.743065     308 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:10:17.745046     308 [logdeviced-main] RecordCacheMonitorThread.cpp:27] RecordCacheMonitorThread() Record cache monitor thread started.
I0917 02:10:17.746401     308 [logdeviced-main] ClusterState.cpp:356] resizeClusterState() Cluster state size updated from 0 to 5
I0917 02:10:17.746095    1160 [ld:cache-evict] RecordCacheMonitorThread.cpp:44] threadMain() Record cache eviction thread started. Size limit is set to 4294967296 bytes.
E0917 02:10:17.749249     308 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:10:17.752889     308 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:10:17.758169     308 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:10:17.762767     308 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:10:17.775290     308 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:10:17.777397     308 [logdeviced-main] Processor.cpp:247] init() Initialized 5 workers of type GENERAL
E0917 02:10:17.787135     308 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:10:17.789443     308 [logdeviced-main] Processor.cpp:247] init() Initialized 1 workers of type FAILURE_DETECTOR
E0917 02:10:17.792455     308 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:10:17.796734     308 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:10:17.802543     308 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:10:17.815216     308 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:10:17.822570    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 1ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:10:17.826617     308 [logdeviced-main] Processor.cpp:247] init() Initialized 4 workers of type BACKGROUND
I0917 02:10:17.844244     308 [logdeviced-main] FailureDetector.cpp:139] FailureDetector() Failure Detector created with instance id: 1631844617844
I0917 02:10:17.852024     308 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N0 changed from UNKNOWN to DEAD
I0917 02:10:17.854682     308 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N0 changed from HEALTHY to UNHEALTHY
I0917 02:10:17.860260     308 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N1 changed from UNKNOWN to DEAD
I0917 02:10:17.862296     308 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N1 changed from HEALTHY to UNHEALTHY
I0917 02:10:17.863429     308 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N2 changed from UNKNOWN to DEAD
I0917 02:10:17.865173     308 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N2 changed from HEALTHY to UNHEALTHY
I0917 02:10:17.865981     308 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N3 changed from UNKNOWN to DEAD
I0917 02:10:17.867958     308 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N3 changed from HEALTHY to UNHEALTHY
I0917 02:10:17.869012     308 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N4 changed from UNKNOWN to DEAD
I0917 02:10:17.870912     308 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N4 changed from HEALTHY to UNHEALTHY
W0917 02:10:17.873857    1178 [ld:srv:WG2] Worker.cpp:1322] processRequest() Request queued for 29 msec: BUFFERED_WRITER_CREATE_SHARD (id: 4294967298), p :HI_PRI
W0917 02:10:17.875974    1173 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 32 msec: BUFFERED_WRITER_CREATE_SHARD (id: 4294967297), p :HI_PRI
W0917 02:10:17.878081    1192 [ld:srv:WG4] Worker.cpp:1322] processRequest() Request queued for 33 msec: BUFFERED_WRITER_CREATE_SHARD (id: 4294967300), p :HI_PRI
W0917 02:10:17.883166    1173 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 28 msec: NODE_STATE_UPDATED (id: 4294967302), p :LO_PRI
W0917 02:10:17.884816    1173 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 30 msec: NODE_STATE_UPDATED (id: 4294967308), p :LO_PRI
I0917 02:10:17.883629     308 [logdeviced-main] FileBasedVersionedConfigStore.cpp:35] FileBasedVersionedConfigStore() FileBasedVersionedConfigStore threads started.
W0917 02:10:17.886878    1173 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 24 msec: NODE_STATE_UPDATED (id: 4294967312), p :LO_PRI
I0917 02:10:17.892093    1219 [ld:srv:WB3] AllSequencers.cpp:981] noteConfigurationChanged() Acquiring lock for sequencer map took 0ms
W0917 02:10:17.892194    1173 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 30 msec: NODE_STATE_UPDATED (id: 4294967318), p :LO_PRI
W0917 02:10:17.893845    1173 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 28 msec: NODE_STATE_UPDATED (id: 4294967322), p :LO_PRI
I0917 02:10:17.893013    1219 [ld:srv:WB3] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 0ms, config update: 0ms, notifications: 1ms
W0917 02:10:17.895411    1173 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 30 msec: NODE_STATE_UPDATED (id: 4294967328), p :LO_PRI
W0917 02:10:17.900741    1173 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 32 msec: NODE_STATE_UPDATED (id: 4294967332), p :LO_PRI
I0917 02:10:17.908097    1219 [ld:srv:WB3] NodesConfigurationManager.cpp:507] onProcessingFinished() Updated local nodes config to version 5...
I0917 02:10:17.909384    1146 [ld:s0:default:0] PartitionedRocksDBStore.cpp:3527] readAllLogSnapshotBlobs() Snapshots column family does not exist
I0917 02:10:17.911587    1156 [ld:s1:default:0] PartitionedRocksDBStore.cpp:3527] readAllLogSnapshotBlobs() Snapshots column family does not exist
I0917 02:10:17.913382    1146 [ld:s0:default:0] RecordCacheRepopulationTask.cpp:103] execute() Repopulated record caches for 0 logs on shard 0, totaling 0 bytes.
I0917 02:10:17.914242    1156 [ld:s1:default:0] RecordCacheRepopulationTask.cpp:103] execute() Repopulated record caches for 0 logs on shard 1, totaling 0 bytes.
I0917 02:10:17.916814     308 [logdeviced-main] Server.cpp:1319] initSequencers() Initializing FileEpochStore
I0917 02:10:17.919062     308 [logdeviced-main] Server.cpp:1398] initSequencerPlacement() using SequencerOptions::LAZY
I0917 02:10:17.920913     308 [logdeviced-main] Server.cpp:1430] initRebuildingCoordinator() Initializing EventLog RSM and RebuildingCoordinator
I0917 02:10:17.922310     308 [logdeviced-main] RsmServerSnapshotStoreFactory.h:25] create() Attempting to create snapshot store (type:1, key:4611686018427387903)
I0917 02:10:17.923025     308 [logdeviced-main] RsmSnapshotStoreFactory.h:31] create() Attempting to create snapshot store (type:1, key:4611686018427387903, is_server:1)
I0917 02:10:17.923854     308 [logdeviced-main] RsmSnapshotStoreFactory.h:58] create() Creating LogBasedRSMSnapshotStore
I0917 02:10:17.925583     308 [logdeviced-main] Server.cpp:1467] initRebuildingCoordinator() Starting RebuildingSupervisor
I0917 02:10:17.927908     308 [logdeviced-main] Server.cpp:1480] initRebuildingCoordinator() Starting RebuildingCoordinator
I0917 02:10:17.928022    1192 [ld:srv:WG4] RebuildingSupervisor.cpp:62] init() Starting Rebuilding Supervisor
I0917 02:10:17.931213    1192 [ld:srv:WG4] RebuildingSupervisor.cpp:222] addForRebuilding() Scheduling rebuilding of N4 to execute in 1200s
I0917 02:10:17.932461    1192 [ld:srv:WG4] RebuildingSupervisor.cpp:222] addForRebuilding() Scheduling rebuilding of N3 to execute in 1200s
I0917 02:10:17.931867    1219 [ld:srv:WB3] RebuildingCoordinator.cpp:309] subscribeToEventLogIfNeeded() RebuildingCoordinator did not start yet because LogsConfig is not fully loaded yet
I0917 02:10:17.931950     308 [logdeviced-main] LogStoreMonitor.cpp:190] start() Logstore monitoring thread started.
I0917 02:10:17.935937     308 [logdeviced-main] UnreleasedRecordDetector.cpp:106] start() Unreleased record detector thread started.
I0917 02:10:17.933454    1192 [ld:srv:WG4] RebuildingSupervisor.cpp:222] addForRebuilding() Scheduling rebuilding of N2 to execute in 1200s
I0917 02:10:17.934657    1219 [ld:srv:WB3] ReplicatedStateMachine-inl.h:667] onBaseSnapshotRetrieved() [eventlog] Base snapshot has version:e0n1, delta_log_read_ptr:LSN_INVALID
I0917 02:10:17.938682     308 [logdeviced-main] RsmServerSnapshotStoreFactory.h:25] create() Attempting to create snapshot store (type:1, key:4611686018427387901)
I0917 02:10:17.945907     308 [logdeviced-main] RsmSnapshotStoreFactory.h:31] create() Attempting to create snapshot store (type:1, key:4611686018427387901, is_server:1)
I0917 02:10:17.940356    1192 [ld:srv:WG4] RebuildingSupervisor.cpp:222] addForRebuilding() Scheduling rebuilding of N1 to execute in 1200s
I0917 02:10:17.944154    1219 [ld:srv:WB3] EventLogStateMachine.cpp:146] gotInitialState() Got base rebuilding set: {}
I0917 02:10:17.948841    1219 [ld:srv:WB3] ReplicatedStateMachine-inl.h:684] getDeltaLogTailLSN() [eventlog] Retrieving tail lsn of delta log...
I0917 02:10:17.946719     308 [logdeviced-main] RsmSnapshotStoreFactory.h:58] create() Creating LogBasedRSMSnapshotStore
W0917 02:10:17.947993    1192 [ld:srv:WG4] RebuildingSupervisor.cpp:705] adjustRebuildingThrottle() Entering throttling mode: #draining=0, #rebuilding=0, #triggers=4, #adjusted cluster size=5, threshold=35%, actual=80%
I0917 02:10:17.956444    1192 [ld:srv:WG4] RebuildingSupervisor.cpp:718] adjustRebuildingThrottle() Entering throttling mode: #draining=0, #rebuilding=0, #triggers=4, #adjusted cluster size=5, threshold=35%, actual=80%
W0917 02:10:17.949902    1219 [ld:srv:WB3] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.015s, source: [Request: START_EVENT_LOG_READER]
E0917 02:10:17.949968    1189 [ld:srv:WG3] HashBasedSequencerLocator.cpp:208] locateSequencer() No available sequencer node for log 4611686018427387903. All sequencer nodes are unavailable.
I0917 02:10:17.955052    1216 [ld:srv:WB1] LogsConfigManager.cpp:38] execute() Starting LogsConfigManager on Worker 1 in pool: BACKGROUND
I0917 02:10:17.961619    1216 [ld:srv:WB1] LogsConfigManager.cpp:165] start() Starting LogsConfig Manager RSM
I0917 02:10:17.959417    1219 [ld:srv:WB3] ReplicatedStateMachine-inl.h:667] onBaseSnapshotRetrieved() [maintenancelog] Base snapshot has version:e0n1, delta_log_read_ptr:LSN_INVALID
E0917 02:10:17.960854    1189 [ld:srv:WG3] SequencerRouter.cpp:56] operator()() Error during sequencer lookup for log 4611686018427387903 (NOSEQUENCER), reporting NOSEQUENCER.
I0917 02:10:17.963763    1216 [ld:srv:WB1] LogsConfigManager.cpp:206] start() Starting LogsConfig Replicated State Machine
I0917 02:10:17.965224    1219 [ld:srv:WB3] ReplicatedStateMachine-inl.h:684] getDeltaLogTailLSN() [maintenancelog] Retrieving tail lsn of delta log...
E0917 02:10:17.967781    1189 [ld:srv:WG3] SequencerRouter.cpp:105] onFailure() Error during sequencer routing for log 4611686018427387903 (NOSEQUENCER), request[type:GET_SEQ_STATE_REQ_TYPE, handler:0x7f14bffa4028].
I0917 02:10:17.968737    1216 [ld:srv:WB1] ReplicatedStateMachine-inl.h:38] startFetchingSnapshot() [logsconfig] Using RSMSnapshotStore...
I0917 02:10:17.974681    1216 [ld:srv:WB1] ReplicatedStateMachine-inl.h:1294] initSnapshotFetchTimer() [logsconfig] Creating timer to fetch snapshots ...
W0917 02:10:17.971992    1219 [ld:srv:WB3] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.013s, source: [Request: START_CLUSTER_MAINTENANCE_STATE_MACHINE]
E0917 02:10:17.972114    1192 [ld:srv:WG4] HashBasedSequencerLocator.cpp:208] locateSequencer() No available sequencer node for log 4611686018427387899. All sequencer nodes are unavailable.
W0917 02:10:17.977405    1192 [ld:srv:WG4] GetSeqStateRequest.cpp:700] finalize() GET_SEQ_STATE for log 4611686018427387899 (rqid:8589934615, ctx:rsm) returned NOSEQUENCER: no sequencer was found for log
W0917 02:10:17.973990    1189 [ld:srv:WG3] GetSeqStateRequest.cpp:512] onSequencerRoutingFailure() Unable to find a sequencer node for log 4611686018427387903, rqid:8589934613, ctx:rsm, err(NOSEQUENCER)
I0917 02:10:17.977083     308 [logdeviced-main] LogDeviceThriftServer.cpp:44] LogDeviceThriftServer() Thrift server LogDevice Admin API Service will listen on port 39366
I0917 02:10:17.981044     308 [logdeviced-main] Server.cpp:1540] createAndAttachMaintenanceManager() Not initializing MaintenanceManager since it is disabled in settings
E0917 02:10:17.978662    1219 [ld:srv:WB3] SyncSequencerRequest.cpp:178] onGotSeqState() Got sequencer state (id:8589934615) for log:4611686018427387899, status:NOSEQUENCER: no sequencer was found for log,sequencer:[invalid NodeID], last_released_lsn:LSN_INVALID, next_lsn:LSN_INVALID
W0917 02:10:17.979787    1189 [ld:srv:WG3] GetSeqStateRequest.cpp:700] finalize() GET_SEQ_STATE for log 4611686018427387903 (rqid:8589934613, ctx:rsm) returned NOSEQUENCER: no sequencer was found for log
I0917 02:10:17.981672     308 [logdeviced-main] Server.cpp:905] initThriftServer() Initializing Thrift Server: s2s-api
I0917 02:10:17.987892     308 [logdeviced-main] LogDeviceThriftServer.cpp:44] LogDeviceThriftServer() Thrift server s2s-api will listen on port 39370
E0917 02:10:17.987096    1219 [ld:srv:WB3] SyncSequencerRequest.cpp:178] onGotSeqState() Got sequencer state (id:8589934613) for log:4611686018427387903, status:NOSEQUENCER: no sequencer was found for log,sequencer:[invalid NodeID], last_released_lsn:LSN_INVALID, next_lsn:LSN_INVALID
I0917 02:10:17.989040     308 [logdeviced-main] Server.cpp:905] initThriftServer() Initializing Thrift Server: c2s-api
I0917 02:10:17.991549     308 [logdeviced-main] LogDeviceThriftServer.cpp:44] LogDeviceThriftServer() Thrift server c2s-api will listen on port 39371
I0917 02:10:17.992369     308 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:10:17.993839     332 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:10:17.995986     308 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:10:17.999894     333 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:10:18.001662    1212 [ld:srv:WF0] FailureDetector.cpp:228] startGetClusterState() Sending GET_CLUSTER_STATE to build initial FD cluster view
I0917 02:10:18.002043     308 [logdeviced-main] SimpleThriftServer.cpp:28] start() Starting a listener thread for Thrift server s2s-api
I0917 02:10:18.007570    1212 [ld:srv:WF0] ProtocolHandler.cpp:127] notifyErrorOnSocket() Socket N3:1(127.0.0.1:39332) hit error AsyncSocketException: connect failed, type = Socket not open, errno = 111 (Connection refused)
I0917 02:10:18.036000     308 [logdeviced-main] SimpleThriftServer.cpp:28] start() Starting a listener thread for Thrift server c2s-api
I0917 02:10:18.060797     308 [logdeviced-main] SimpleThriftServer.cpp:28] start() Starting a listener thread for Thrift server LogDevice Admin API Service
I0917 02:10:18.098077     308 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:10:18.109317     334 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:10:18.112439     308 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:10:18.114943    1212 [ld:srv:WF0] FailureDetector.cpp:267] operator()() Cluster state received with 5 dead nodes (N0,N1,N2,N3,N4) and 0 boycotted nodes ()
I0917 02:10:18.116080    1212 [ld:srv:WF0] FailureDetector.cpp:288] buildInitialState() Wait over (cluster state received)
I0917 02:10:18.115089     332 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:10:18.117421    1212 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N1 changed from DEAD to STARTING
I0917 02:10:18.119613    1212 [ld:srv:WF0] FailureDetector.cpp:316] buildInitialState() N1 transitioned to ALIVE
I0917 02:10:18.121056    1212 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N2 changed from DEAD to STARTING
I0917 02:10:18.119706    1192 [ld:srv:WG4] RebuildingSupervisor.cpp:256] onNodeStateChanged() Cancelling rebuilding trigger for node N1
I0917 02:10:18.119894     308 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:10:18.123595    1212 [ld:srv:WF0] FailureDetector.cpp:316] buildInitialState() N2 transitioned to ALIVE
I0917 02:10:18.124007    1192 [ld:srv:WG4] RebuildingSupervisor.cpp:256] onNodeStateChanged() Cancelling rebuilding trigger for node N2
I0917 02:10:18.124841     332 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:10:18.125835    1212 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N3 changed from DEAD to STARTING
I0917 02:10:18.128053     308 [logdeviced-main] main.cpp:464] main() Listeners initialized
I0917 02:10:18.128785    1212 [ld:srv:WF0] FailureDetector.cpp:316] buildInitialState() N3 transitioned to ALIVE
I0917 02:10:18.129037    1192 [ld:srv:WG4] RebuildingSupervisor.cpp:256] onNodeStateChanged() Cancelling rebuilding trigger for node N3
W0917 02:10:18.131113    1192 [ld:srv:WG4] RebuildingSupervisor.cpp:734] adjustRebuildingThrottle() Exiting throttling mode: #draining=0, #rebuilding=0, #triggers=1, #adjusted cluster size=5, threshold=35%, actual=20%
I0917 02:10:18.130392    1212 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N4 changed from DEAD to STARTING
I0917 02:10:18.132174    1192 [ld:srv:WG4] RebuildingSupervisor.cpp:751] adjustRebuildingThrottle() Not triggering new rebuildings until 2021-09-17 02:30:18.132: It's too soon after exiting throttling mode
I0917 02:10:18.132992    1212 [ld:srv:WF0] FailureDetector.cpp:316] buildInitialState() N4 transitioned to ALIVE
I0917 02:10:18.134190    1212 [ld:srv:WF0] FailureDetector.cpp:334] buildInitialState() N1 transitioned to UNHEALTHY (status)
I0917 02:10:18.133836    1192 [ld:srv:WG4] RebuildingSupervisor.cpp:256] onNodeStateChanged() Cancelling rebuilding trigger for node N4
I0917 02:10:18.135011    1212 [ld:srv:WF0] FailureDetector.cpp:334] buildInitialState() N2 transitioned to UNHEALTHY (status)
I0917 02:10:18.136332    1212 [ld:srv:WF0] FailureDetector.cpp:334] buildInitialState() N3 transitioned to UNHEALTHY (status)
I0917 02:10:18.137302    1212 [ld:srv:WF0] FailureDetector.cpp:334] buildInitialState() N4 transitioned to UNHEALTHY (status)
I0917 02:10:18.138392    1212 [ld:srv:WF0] FailureDetector.cpp:344] buildInitialState() Initializing DomainIsolationChecker
I0917 02:10:18.139136    1212 [ld:srv:WF0] FailureDetector.cpp:915] startSuspectTimer() Starting suspect state timer
I0917 02:10:18.140032    1212 [ld:srv:WF0] FailureDetector.cpp:1285] updateNodeState() N0 transitioned from DEAD to ALIVE, FD State:(gossip: 0, instance-id: 0, failover: 0, starting: 1)
I0917 02:10:18.141344    1212 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N0 changed from DEAD to STARTING
I0917 02:10:18.144805    1212 [ld:srv:WF0] FailureDetector.cpp:1456] broadcastBringupUpdate() Broadcasting suspect-state-finished message.
I0917 02:10:18.146781    1212 [ld:srv:WF0] FailureDetector.cpp:1476] broadcastBringupUpdate() Sending suspect-state-finished message with instance id:1631844617844 to node WF0:N4:1 (UNKNOWN)
I0917 02:10:18.150837    1212 [ld:srv:WF0] FailureDetector.cpp:1476] broadcastBringupUpdate() Sending suspect-state-finished message with instance id:1631844617844 to node WF0:N3:1 (127.0.0.1:39332)
I0917 02:10:18.153945    1212 [ld:srv:WF0] FailureDetector.cpp:1456] broadcastBringupUpdate() Broadcasting bringup message.
I0917 02:10:18.156185    1212 [ld:srv:WF0] FailureDetector.cpp:937] startGossiping() Start Gossiping.
I0917 02:10:18.157558    1212 [ld:srv:WF0] FailureDetector.cpp:925] operator()() Suspect timeout expired
I0917 02:10:18.258594    1212 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N0 changed from UNHEALTHY to UNDEFINED
I0917 02:10:18.462363    1212 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N0 changed from UNDEFINED to HEALTHY
I0917 02:10:18.483963    1163 [ld:srv:WG0] GET_SEQ_STATE_Message.cpp:238] getSequencer() Sequencer for log:4611686018427387900 is absent, state=UNAVAILABLE, cur_epoch=0, attempting to activate it upon receiving GSS (rqid:8589934609, ctx:sync-sequencer, min_epoch:none) from WG0:C4 (127.0.0.1:56266)
I0917 02:10:18.492230    1192 [ld:srv:WG4] CONFIG_FETCH_Message.cpp:70] onReceived() CONFIG_FETCH messages received. E.g., from WG4:C5 (127.0.0.1:56270)
I0917 02:10:18.495048    1163 [ld:srv:WG0] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387900, rqid:8589934609, ctx:sync-sequencer) from WG0:C4 (127.0.0.1:56266), but its data log sequencer's state is ACTIVATING
I0917 02:10:18.496169    1163 [ld:srv:WG0] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 4611686018427387900 epoch 2.
I0917 02:10:18.496991    1163 [ld:srv:WG0] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 4611686018427387900: {[E:1 (at 2021-09-17 02:10:18.488) since:1 (at 2021-09-17 02:10:15.391) R:[node: 2] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]], Until:2}.
I0917 02:10:18.498086    1163 [ld:srv:WG0] AllSequencers.cpp:527] onEpochMetaDataFromEpochStore() Activated a sequencer for log 4611686018427387900 (reason: GET_SEQ_STATE) with epoch 2, metadata: [E:2 (at 2021-09-17 02:10:18.488) since:1 (at 2021-09-17 02:10:15.391) R:[node: 2] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]]. Activation Result: RECOVERY.
I0917 02:10:18.497617    1192 [ld:srv:WG4] Connection.cpp:724] close() Closing socket C5(127.0.0.1:56270). Reason: PEER_CLOSED: connection closed by peer
I0917 02:10:18.500916    1163 [ld:srv:WG0] MetaDataLogTrimmer.cpp:115] schedulePeriodicTrimming() Will run periodic trimming every ~7200000 s for 4611686018427387900
I0917 02:10:18.502748    1163 [ld:srv:WG0] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 4611686018427387900 with next_epoch 2
I0917 02:10:18.501703    1192 [ld:srv:WG4] PeriodicReleases.cpp:416] execute() Start broadcasting releases for log 4611686018427387900.
I0917 02:10:18.505043    1192 [ld:srv:WG4] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 13835058055282163708 epoch 2.
I0917 02:10:18.506217    1192 [ld:srv:WG4] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 13835058055282163708: {[E:1 (at -inf) since:1 (at -inf) R:[node: 3] V:2 flags: params:{target:0 seed:0 signature:0} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]], Until:2}.
I0917 02:10:18.507509    1192 [ld:srv:WG4] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 13835058055282163708 with next_epoch 2
I0917 02:10:18.508976    1163 [ld:srv:WG0] LogRecoveryRequest.cpp:1013] sealLog() Starting recovery of epoch 1 of log 4611686018427387900
I0917 02:10:18.514181    1192 [ld:srv:WG4] LogRecoveryRequest.cpp:1013] sealLog() Starting recovery of epoch 1 of log 13835058055282163708
I0917 02:10:18.523038    1147 [ld:s0:default:1] RecordCache.cpp:257] getEpochRecordCache() RecordCache miss for log 13835058055282163708 epoch 1: cache not found but cannot declare no data. head epoch : 0, last_nonauthoritative_epoch: 1.
I0917 02:10:18.530161    1146 [ld:s0:default:0] RecordCache.cpp:257] getEpochRecordCache() RecordCache miss for log 4611686018427387900 epoch 1: cache not found but cannot declare no data. head epoch : 0, last_nonauthoritative_epoch: 1.
I0917 02:10:18.628694    1173 [ld:srv:WG1] RecordCache.cpp:257] getEpochRecordCache() RecordCache miss for log 13835058055282163708 epoch 1: cache not found but cannot declare no data. head epoch : 0, last_nonauthoritative_epoch: 1.
I0917 02:10:18.631323    1178 [ld:srv:WG2] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:13835058055282163708, rqid:30064771073, ctx:start-message) from WG2:C8 (127.0.0.1:56310), but its data log sequencer's state is Recovery Incomplete
I0917 02:10:18.633408    1192 [ld:srv:WG4] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387900, rqid:30064771075, ctx:start-message) from WG4:C6 (127.0.0.1:56276), but its data log sequencer's state is Recovery Incomplete
E0917 02:10:18.634741    1192 [ld:srv:WG4] EpochRecovery.cpp:394] onDigestComplete() All SEALED messages contained zero last_timestamp for a nonempty epoch (recovery: epoch 1 of log 13835058055282163708). This is unexpected, please investigate. Using current time instead. Log: 13835058055282163708, lng: 1.
I0917 02:10:18.635510    1192 [ld:srv:WG4] EpochRecovery.cpp:566] startMutations() Log 13835058055282163708 epoch 1 final digest before mutation: consensus LNG 1, start esn: 2, 0 entries, first esn: 0, last esn: 0, bridge esn: 2, tail esn: 1, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 2021-09-17 02:10:18.634, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163708 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:13835058055282163708 N:e1n1 T:0 OM:{} F:528], final tail record: [L:13835058055282163708 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:10:18.635546    1163 [ld:srv:WG0] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:13835058055282163708, rqid:34359738369, ctx:start-message) from WG0:C10 (127.0.0.1:56312), but its data log sequencer's state is Recovery Incomplete
I0917 02:10:18.638200    1163 [ld:srv:WG0] EpochRecovery.cpp:566] startMutations() Log 4611686018427387900 epoch 1 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:10:18.638688    1173 [ld:srv:WG1] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:13835058055282163708, rqid:38654705665, ctx:start-message) from WG1:C17 (127.0.0.1:56334), but its data log sequencer's state is Recovery Incomplete
I0917 02:10:18.645545    1163 [ld:srv:WG0] Mutator.cpp:400] finalize() Mutator 4611686018427387900e1n1 completed (OK). Trace: W1{N4:S0,N2:S0};X:N4:S0:K;X:N2:S0:K;Y1:N2:S0:K;Y1:N4:S0:K;done:can_replicate:OK;
I0917 02:10:18.647167    1192 [ld:srv:WG4] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 4611686018427387900 has already been cleaned. No purge needed. last clean epoch: 0, purge to: 0.
I0917 02:10:18.665999    1212 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N1 changed from UNHEALTHY to HEALTHY
I0917 02:10:18.666715    1212 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N2 changed from UNHEALTHY to HEALTHY
I0917 02:10:18.668481    1212 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N3 changed from UNHEALTHY to HEALTHY
I0917 02:10:18.743148    1192 [ld:srv:WG4] PurgeUncleanEpochs.cpp:592] complete() Successfully purged epochs [0, 0] for log 4611686018427387900.
I0917 02:10:18.744552    1192 [ld:srv:WG4] Mutator.cpp:400] finalize() Mutator 13835058055282163708e1n2 completed (OK). Trace: W1{N3:S0,N1:S0,N0:S0};X:N3:S0:K;X:N1:S0:K;X:N0:S0:K;Y1:N3:S0:K;Y1:N1:S0:K;Y1:N0:S0:K;done:can_replicate:OK;
I0917 02:10:18.746257    1173 [ld:srv:WG1] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163708 has already been cleaned. No purge needed. last clean epoch: 0, purge to: 0.
I0917 02:10:18.854404    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:10:18.855153    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.002MB in 2 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, [unpartitioned A:0.001MB]
I0917 02:10:18.859945    1163 [ld:srv:WG0] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 4611686018427387900, next_epoch:2
I0917 02:10:18.939485    1178 [ld:srv:WG2] GET_SEQ_STATE_Message.cpp:238] getSequencer() Sequencer for log:4611686018427387901 is absent, state=UNAVAILABLE, cur_epoch=0, attempting to activate it upon receiving GSS (rqid:81604378628, ctx:rsm, min_epoch:none) from WG2:C20 (127.0.0.1:56348)
I0917 02:10:18.943000    1163 [ld:srv:WG0] GET_SEQ_STATE_Message.cpp:238] getSequencer() Sequencer for log:4611686018427387903 is absent, state=UNAVAILABLE, cur_epoch=0, attempting to activate it upon receiving GSS (rqid:25769803781, ctx:rsm, min_epoch:none) from WG0:C21 (127.0.0.1:56350)
W0917 02:10:18.955805    1178 [ld:srv:WG2] Worker.cpp:1000] onStoppedRunning() skipped at least 3 log entries
I0917 02:10:18.957495    1163 [ld:srv:WG0] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 4611686018427387903 epoch 2.
W0917 02:10:18.957918    1178 [ld:srv:WG2] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.016s, source: [Message sent/received: GET_SEQ_STATE]
I0917 02:10:18.958556    1163 [ld:srv:WG0] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 4611686018427387903: {[E:1 (at 2021-09-17 02:10:18.948) since:1 (at 2021-09-17 02:10:15.661) R:[node: 3] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]], Until:2}.
I0917 02:10:18.959895    1178 [ld:srv:WG2] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 4611686018427387901 epoch 2.
I0917 02:10:18.960254    1163 [ld:srv:WG0] AllSequencers.cpp:527] onEpochMetaDataFromEpochStore() Activated a sequencer for log 4611686018427387903 (reason: GET_SEQ_STATE) with epoch 2, metadata: [E:2 (at 2021-09-17 02:10:18.948) since:1 (at 2021-09-17 02:10:15.661) R:[node: 3] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]]. Activation Result: RECOVERY.
I0917 02:10:18.961825    1178 [ld:srv:WG2] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 4611686018427387901: {[E:1 (at 2021-09-17 02:10:18.944) since:1 (at 2021-09-17 02:10:15.609) R:[node: 2] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]], Until:2}.
I0917 02:10:18.963402    1178 [ld:srv:WG2] AllSequencers.cpp:527] onEpochMetaDataFromEpochStore() Activated a sequencer for log 4611686018427387901 (reason: GET_SEQ_STATE) with epoch 2, metadata: [E:2 (at 2021-09-17 02:10:18.944) since:1 (at 2021-09-17 02:10:15.609) R:[node: 2] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]]. Activation Result: RECOVERY.
I0917 02:10:18.962537    1189 [ld:srv:WG3] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 4611686018427387903 with next_epoch 2
I0917 02:10:18.962651    1163 [ld:srv:WG0] PeriodicReleases.cpp:416] execute() Start broadcasting releases for log 4611686018427387903.
I0917 02:10:18.962711    1173 [ld:srv:WG1] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 13835058055282163711 epoch 2.
W0917 02:10:18.964977    1192 [ld:srv:WG4] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.016s, source: [Message sent/received: CLEANED]
I0917 02:10:18.966372    1178 [ld:srv:WG2] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 4611686018427387901 with next_epoch 2
I0917 02:10:18.969560    1173 [ld:srv:WG1] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 13835058055282163711: {[E:1 (at -inf) since:1 (at -inf) R:[node: 3] V:2 flags: params:{target:0 seed:0 signature:0} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]], Until:2}.
I0917 02:10:18.971236    1189 [ld:srv:WG3] PeriodicReleases.cpp:416] execute() Start broadcasting releases for log 4611686018427387901.
I0917 02:10:18.976651    1189 [ld:srv:WG3] LogRecoveryRequest.cpp:1013] sealLog() Starting recovery of epoch 1 of log 4611686018427387903
I0917 02:10:18.971437    1192 [ld:srv:WG4] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 13835058055282163708, next_epoch:2
I0917 02:10:18.974295    1173 [ld:srv:WG1] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 13835058055282163709 epoch 2.
I0917 02:10:18.976881    1216 [ld:srv:WB1] ReplicatedStateMachine-inl.h:130] getSnapshot() [logsconfig] Fetching snapshot with ver:e0n1, (waiting_for_snapshot_:LSN_INVALID, version_:e0n1). sync_state_:0
I0917 02:10:18.978828    1178 [ld:srv:WG2] LogRecoveryRequest.cpp:1013] sealLog() Starting recovery of epoch 1 of log 4611686018427387901
I0917 02:10:18.985309    1189 [ld:srv:WG3] CHECK_SEAL_onReceived.cpp:48] prepareReplyWithHighestSeal() Soft seal for log:4611686018427387899 is [epoch:0, seq:[invalid NodeID]], but normal seal is absent; src_rqid:30064771078
I0917 02:10:18.986613    1173 [ld:srv:WG1] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 13835058055282163709: {[E:1 (at -inf) since:1 (at -inf) R:[node: 3] V:2 flags: params:{target:0 seed:0 signature:0} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]], Until:2}.
I0917 02:10:18.998695    1173 [ld:srv:WG1] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 13835058055282163711 with next_epoch 2
I0917 02:10:19.000463    1219 [ld:srv:WB3] ReplicatedStateMachine-inl.h:712] onGotDeltaLogTailLSN() [eventlog] Tail lsn of delta log is e2n0
I0917 02:10:19.014904    1219 [ld:srv:WB3] ReplicatedStateMachine-inl.h:712] onGotDeltaLogTailLSN() [maintenancelog] Tail lsn of delta log is e2n0
I0917 02:10:19.019365    1173 [ld:srv:WG1] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 13835058055282163709 with next_epoch 2
I0917 02:10:19.026823    1173 [ld:srv:WG1] LogRecoveryRequest.cpp:1013] sealLog() Starting recovery of epoch 1 of log 13835058055282163711
I0917 02:10:19.028426    1192 [ld:srv:WG4] GET_SEQ_STATE_Message.cpp:567] continueExecution() skipped at least 10 log entries
I0917 02:10:19.029080    1192 [ld:srv:WG4] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:13835058055282163709, rqid:85899345920, ctx:start-message) from WG4:C27 (127.0.0.1:56404), but its data log sequencer's state is Recovery Incomplete
I0917 02:10:19.029275    1173 [ld:srv:WG1] LogRecoveryRequest.cpp:1013] sealLog() Starting recovery of epoch 1 of log 13835058055282163709
I0917 02:10:19.035748    1163 [ld:srv:WG0] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:13835058055282163709, rqid:34359738378, ctx:start-message) from WG0:C15 (127.0.0.1:56328), but its data log sequencer's state is Recovery Incomplete
I0917 02:10:19.041584    1163 [ld:srv:WG0] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:13835058055282163711, rqid:64424509475, ctx:start-message) from WG0:C15 (127.0.0.1:56328), but its data log sequencer's state is Recovery Incomplete
I0917 02:10:19.043738    1189 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:13835058055282163709, rqid:81604378626, ctx:start-message) from WG3:C29 (127.0.0.1:56410), but its data log sequencer's state is Recovery Incomplete
I0917 02:10:19.044018    1216 [ld:srv:WB1] LogBasedRSMSnapshotStore.cpp:298] onGotLastReleased() Returning EMPTY for log:4611686018427387900, last_released_real_lsn:LSN_INVALID
I0917 02:10:19.045627    1216 [ld:srv:WB1] ReplicatedStateMachine-inl.h:83] operator()() [logsconfig] getSnapshot()'s callback. st:EMPTY, snapshot blob size:0, attrs:(base_ver:LSN_INVALID, ts:0)
I0917 02:10:19.044967    1189 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387901, rqid:85899345921, ctx:start-message) from WG3:C22 (127.0.0.1:56354), but its data log sequencer's state is Recovery Incomplete
I0917 02:10:19.046496    1216 [ld:srv:WB1] ReplicatedStateMachine-inl.h:667] onBaseSnapshotRetrieved() [logsconfig] Base snapshot has version:e0n1, delta_log_read_ptr:LSN_INVALID
I0917 02:10:19.049668    1216 [ld:srv:WB1] ReplicatedStateMachine-inl.h:684] getDeltaLogTailLSN() [logsconfig] Retrieving tail lsn of delta log...
I0917 02:10:19.053410    1216 [ld:srv:WB1] ReplicatedStateMachine-inl.h:712] onGotDeltaLogTailLSN() [logsconfig] Tail lsn of delta log is e2n0
I0917 02:10:19.091798    1189 [ld:srv:WG3] RecordCache.cpp:257] getEpochRecordCache() skipped at least 10 log entries
I0917 02:10:19.093385    1189 [ld:srv:WG3] RecordCache.cpp:257] getEpochRecordCache() RecordCache miss for log 4611686018427387899 epoch 1: cache not found but cannot declare no data. head epoch : 0, last_nonauthoritative_epoch: 1.
E0917 02:10:19.102837    1173 [ld:srv:WG1] EpochRecovery.cpp:394] onDigestComplete() All SEALED messages contained zero last_timestamp for a nonempty epoch (recovery: epoch 1 of log 13835058055282163709). This is unexpected, please investigate. Using current time instead. Log: 13835058055282163709, lng: 1.
I0917 02:10:19.104823    1173 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 13835058055282163709 epoch 1 final digest before mutation: consensus LNG 1, start esn: 2, 0 entries, first esn: 0, last esn: 0, bridge esn: 2, tail esn: 1, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 2021-09-17 02:10:19.102, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163709 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:13835058055282163709 N:e1n1 T:0 OM:{} F:528], final tail record: [L:13835058055282163709 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
I0917 02:10:19.103024    1189 [ld:srv:WG3] EpochRecovery.cpp:566] startMutations() Log 4611686018427387903 epoch 1 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387903 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387903 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
I0917 02:10:19.104386    1178 [ld:srv:WG2] EpochRecovery.cpp:566] startMutations() Log 4611686018427387901 epoch 1 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387901 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387901 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
E0917 02:10:19.108764    1173 [ld:srv:WG1] EpochRecovery.cpp:394] onDigestComplete() All SEALED messages contained zero last_timestamp for a nonempty epoch (recovery: epoch 1 of log 13835058055282163711). This is unexpected, please investigate. Using current time instead. Log: 13835058055282163711, lng: 1.
I0917 02:10:19.112445    1163 [ld:srv:WG0] RecordCache.cpp:257] getEpochRecordCache() RecordCache miss for log 4611686018427387903 epoch 1: cache not found but cannot declare no data. head epoch : 0, last_nonauthoritative_epoch: 1.
I0917 02:10:19.113645    1189 [ld:srv:WG3] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 4611686018427387899 has already been cleaned. No purge needed. last clean epoch: 0, purge to: 0.
I0917 02:10:19.119120    1173 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 13835058055282163711 epoch 1 final digest before mutation: consensus LNG 1, start esn: 2, 0 entries, first esn: 0, last esn: 0, bridge esn: 2, tail esn: 1, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 2021-09-17 02:10:19.108, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163711 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:13835058055282163711 N:e1n1 T:0 OM:{} F:528], final tail record: [L:13835058055282163711 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
I0917 02:10:19.121431    1178 [ld:srv:WG2] Mutator.cpp:400] finalize() Mutator 4611686018427387901e1n1 completed (OK). Trace: W1{N2:S1,N1:S1};X:N2:S1:K;X:N1:S1:K;Y1:N2:S1:K;Y1:N1:S1:K;done:can_replicate:OK;
I0917 02:10:19.124406    1163 [ld:srv:WG0] RecordCache.cpp:257] getEpochRecordCache() RecordCache miss for log 13835058055282163709 epoch 1: cache not found but cannot declare no data. head epoch : 0, last_nonauthoritative_epoch: 1.
I0917 02:10:19.126434    1173 [ld:srv:WG1] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 4611686018427387901 has already been cleaned. No purge needed. last clean epoch: 0, purge to: 0.
I0917 02:10:19.131642    1189 [ld:srv:WG3] Mutator.cpp:400] finalize() Mutator 4611686018427387903e1n1 completed (OK). Trace: W1{N0:S1,N4:S1,N1:S1};X:N0:S1:K;X:N4:S1:K;X:N1:S1:K;Y1:N4:S1:K;Y1:N1:S1:K;Y1:N0:S1:K;done:can_replicate:OK;
I0917 02:10:19.135645    1163 [ld:srv:WG0] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 4611686018427387903 has already been cleaned. No purge needed. last clean epoch: 0, purge to: 0.
I0917 02:10:19.220416    1173 [ld:srv:WG1] Mutator.cpp:400] finalize() Mutator 13835058055282163711e1n2 completed (OK). Trace: W1{N0:S1,N3:S1,N4:S1};X:N0:S1:K;X:N3:S1:K;X:N4:S1:K;Y1:N3:S1:K;Y1:N4:S1:K;Y1:N0:S1:K;done:can_replicate:OK;
I0917 02:10:19.222465    1163 [ld:srv:WG0] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163711 has already been cleaned. No purge needed. last clean epoch: 0, purge to: 0.
I0917 02:10:19.224007    1173 [ld:srv:WG1] Mutator.cpp:400] finalize() Mutator 13835058055282163709e1n2 completed (OK). Trace: W1{N3:S1,N2:S1,N0:S1};X:N3:S1:K;X:N2:S1:K;X:N0:S1:K;Y1:N3:S1:K;Y1:N0:S1:K;Y1:N2:S1:K;done:can_replicate:OK;
I0917 02:10:19.226519    1163 [ld:srv:WG0] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163709 has already been cleaned. No purge needed. last clean epoch: 0, purge to: 0.
I0917 02:10:19.331852    1189 [ld:srv:WG3] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 4611686018427387903, next_epoch:2
I0917 02:10:19.333364    1178 [ld:srv:WG2] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 4611686018427387901, next_epoch:2
I0917 02:10:19.337977    1219 [ld:srv:WB3] ClientReadStream.cpp:4571] operator()() Requested epoch metadata for log 4611686018427387903 epoch 2 is not available. Highest epoch known is 1.
I0917 02:10:19.348713    1219 [ld:srv:WB3] ReplicatedStateMachine-inl.h:1052] onReachedDeltaLogTailLSN() [eventlog] Reached tail of delta log. deliver_while_replaying:0, stop_at_tail_:0
I0917 02:10:19.350467    1219 [ld:srv:WB3] EventLogStateMachine.cpp:93] publishRebuildingSet() [eventlog] Published new EventLogRebuildingSet version (1)
I0917 02:10:19.349550    1216 [ld:srv:WB1] ReplicatedStateMachine-inl.h:1052] onReachedDeltaLogTailLSN() [logsconfig] Reached tail of delta log. deliver_while_replaying:0, stop_at_tail_:0
I0917 02:10:19.351747    1219 [ld:srv:WB3] EventLogStateMachine.cpp:341] trimNotSnapshotted() Trimming event log up to lsn e0n1
I0917 02:10:19.353370    1216 [ld:srv:WB1] LogsConfigManager.cpp:193] operator()() Received update from LogsConfigStateMachine, version 1
W0917 02:10:19.354789    1219 [ld:srv:WB3] Worker.cpp:1000] onStoppedRunning() skipped at least 10 log entries
W0917 02:10:19.357414    1219 [ld:srv:WB3] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.017s, source: [Message sent/received: GAP]
I0917 02:10:19.355291    1216 [ld:srv:WB1] AllSequencers.cpp:981] noteConfigurationChanged() Acquiring lock for sequencer map took 0ms
I0917 02:10:19.359667    1216 [ld:srv:WB1] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 0ms, config update: 0ms, notifications: 4ms
I0917 02:10:19.363591    1216 [ld:srv:WB1] LogsConfigManager.cpp:331] updateLogsConfig() Published new LogsConfig (fully loaded? yes) version (1) from LogsConfigManager
I0917 02:10:19.362506    1219 [ld:srv:WB3] RebuildingCoordinator.cpp:1740] publishDirtyShards() Publishing dirty shards.
W0917 02:10:19.367135    1216 [ld:srv:WB1] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.018s, source: [Message sent/received: GAP]
I0917 02:10:19.368989    1219 [ld:srv:WB3] RebuildingCoordinator.cpp:2064] subscribeToEventLog() Subscribed to EventLog
I0917 02:10:19.373084    1219 [ld:srv:WB3] ClientReadStream.cpp:4571] operator()() Requested epoch metadata for log 4611686018427387899 epoch 2 is not available. Highest epoch known is 1.
I0917 02:10:19.373729    1219 [ld:srv:WB3] ReplicatedStateMachine-inl.h:1052] onReachedDeltaLogTailLSN() [maintenancelog] Reached tail of delta log. deliver_while_replaying:0, stop_at_tail_:0
I0917 02:10:19.374308    1219 [ld:srv:WB3] ClusterMaintenanceStateMachine.cpp:112] onUpdate() [maintenancelog] Published new ClusterMaintenanceState version (1)
I0917 02:10:19.380695    1212 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N0 changed from STARTING to FULLY_STARTED
I0917 02:10:19.437861    1173 [ld:srv:WG1] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 13835058055282163711, next_epoch:2
I0917 02:10:19.438681    1173 [ld:srv:WG1] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 13835058055282163709, next_epoch:2
I0917 02:10:19.483505    1212 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N3 changed from STARTING to FULLY_STARTED
I0917 02:10:19.527249    1173 [ld:srv:WG1] GET_SEQ_STATE_Message.cpp:238] getSequencer() Sequencer for log:4611686018427387902 is absent, state=UNAVAILABLE, cur_epoch=0, attempting to activate it upon receiving GSS (rqid:8589934624, ctx:historical-metadata, min_epoch:none) from WG1:C39 (127.0.0.1:56488)
I0917 02:10:19.538928    1173 [ld:srv:WG1] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 4611686018427387902 epoch 2.
I0917 02:10:19.540080    1173 [ld:srv:WG1] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 4611686018427387902: {[E:1 (at 2021-09-17 02:10:19.532) since:1 (at 2021-09-17 02:10:15.636) R:[node: 3] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]], Until:2}.
I0917 02:10:19.542073    1173 [ld:srv:WG1] AllSequencers.cpp:527] onEpochMetaDataFromEpochStore() Activated a sequencer for log 4611686018427387902 (reason: GET_SEQ_STATE) with epoch 2, metadata: [E:2 (at 2021-09-17 02:10:19.532) since:1 (at 2021-09-17 02:10:15.636) R:[node: 3] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]]. Activation Result: RECOVERY.
I0917 02:10:19.542147    1163 [ld:srv:WG0] PeriodicReleases.cpp:416] execute() Start broadcasting releases for log 4611686018427387902.
I0917 02:10:19.543231    1178 [ld:srv:WG2] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 13835058055282163710 epoch 2.
I0917 02:10:19.545446    1178 [ld:srv:WG2] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 13835058055282163710: {[E:1 (at -inf) since:1 (at -inf) R:[node: 3] V:2 flags: params:{target:0 seed:0 signature:0} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]], Until:2}.
I0917 02:10:19.544125    1173 [ld:srv:WG1] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 4611686018427387902 with next_epoch 2
I0917 02:10:19.545941    1178 [ld:srv:WG2] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 13835058055282163710 with next_epoch 2
I0917 02:10:19.557354    1173 [ld:srv:WG1] LogRecoveryRequest.cpp:1013] sealLog() Starting recovery of epoch 1 of log 4611686018427387902
I0917 02:10:19.565049    1178 [ld:srv:WG2] LogRecoveryRequest.cpp:1013] sealLog() Starting recovery of epoch 1 of log 13835058055282163710
I0917 02:10:19.566708    1216 [ld:srv:WB1] ReplicatedStateMachine-inl.h:841] onDeltaRecord() [logsconfig] Applied delta record with lsn=e2n1 ts=2021-09-17 02:10:19.558
I0917 02:10:19.567957    1216 [ld:srv:WB1] AllSequencers.cpp:981] noteConfigurationChanged() Acquiring lock for sequencer map took 0ms
I0917 02:10:19.572111    1216 [ld:srv:WB1] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 0ms, config update: 0ms, notifications: 4ms
I0917 02:10:19.572445    1192 [ld:srv:WG4] Connection.cpp:724] close() Closing socket C40(127.0.0.1:56498). Reason: PEER_CLOSED: connection closed by peer
I0917 02:10:19.574421    1216 [ld:srv:WB1] LogsConfigManager.cpp:331] updateLogsConfig() Published new LogsConfig (fully loaded? yes) version (8589934593) from LogsConfigManager
I0917 02:10:19.587858    1212 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N2 changed from STARTING to FULLY_STARTED
E0917 02:10:19.666371    1178 [ld:srv:WG2] EpochRecovery.cpp:394] onDigestComplete() All SEALED messages contained zero last_timestamp for a nonempty epoch (recovery: epoch 1 of log 13835058055282163710). This is unexpected, please investigate. Using current time instead. Log: 13835058055282163710, lng: 1.
I0917 02:10:19.667502    1178 [ld:srv:WG2] EpochRecovery.cpp:566] startMutations() Log 13835058055282163710 epoch 1 final digest before mutation: consensus LNG 1, start esn: 2, 0 entries, first esn: 0, last esn: 0, bridge esn: 2, tail esn: 1, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 2021-09-17 02:10:19.666, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163710 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:13835058055282163710 N:e1n1 T:0 OM:{} F:528], final tail record: [L:13835058055282163710 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:10:19.668850    1173 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 4611686018427387902 epoch 1 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387902 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387902 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:10:19.674955    1173 [ld:srv:WG1] Mutator.cpp:400] finalize() Mutator 4611686018427387902e1n1 completed (OK). Trace: W1{N4:S0,N0:S0,N1:S0};X:N4:S0:K;X:N0:S0:K;X:N1:S0:K;Y1:N1:S0:K;Y1:N0:S0:K;Y1:N4:S0:K;done:can_replicate:OK;
I0917 02:10:19.675699    1192 [ld:srv:WG4] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 4611686018427387898 has already been cleaned. No purge needed. last clean epoch: 0, purge to: 0.
I0917 02:10:19.775413    1178 [ld:srv:WG2] Mutator.cpp:400] finalize() Mutator 13835058055282163710e1n2 completed (OK). Trace: W1{N4:S0,N3:S0,N2:S0};X:N4:S0:K;X:N3:S0:K;X:N2:S0:K;Y1:N3:S0:K;Y1:N2:S0:K;Y1:N4:S0:K;done:can_replicate:OK;
I0917 02:10:19.779494    1192 [ld:srv:WG4] PurgeUncleanEpochs.cpp:592] complete() skipped at least 6 log entries
I0917 02:10:19.779965    1192 [ld:srv:WG4] PurgeUncleanEpochs.cpp:592] complete() Successfully purged epochs [0, 0] for log 4611686018427387898.
I0917 02:10:19.888452    1173 [ld:srv:WG1] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 4611686018427387902, next_epoch:2
W0917 02:10:19.994674    1178 [ld:srv:WG2] Worker.cpp:1000] onStoppedRunning() skipped at least 3 log entries
W0917 02:10:19.995557    1178 [ld:srv:WG2] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.014s, source: [Message sent/received: CLEANED]
I0917 02:10:19.998017    1178 [ld:srv:WG2] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 13835058055282163710, next_epoch:2
I0917 02:10:20.199887    1212 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N1 changed from STARTING to FULLY_STARTED
I0917 02:10:20.201135    1212 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N4 changed from STARTING to FULLY_STARTED
I0917 02:10:27.852747    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:10:27.853496    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.004MB in 2 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.003MB, [unpartitioned A:0.002MB]
I0917 02:10:28.871913    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:10:28.872578    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.004MB in 2 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.002MB, [unpartitioned A:0.001MB]
I0917 02:10:29.353433    1219 [ld:srv:WB3] ShardAuthoritativeStatusMap.cpp:165] broadcastToAllWorkers() Posting shard status update to workers: , version=e0n1
I0917 02:10:37.873377    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:10:37.874221    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.004MB in 2 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.003MB, [unpartitioned A:0.002MB]
I0917 02:10:38.890649    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:10:38.891350    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.004MB in 2 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.002MB, [unpartitioned A:0.001MB]
I0917 02:10:47.683721     376 [rocks-high-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000024.log.trash as trash -- OK
I0917 02:10:47.690398     393 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 10after deleting file /data/store/N0:1/db/shard1/000024.log.trash
I0917 02:10:47.722793     377 [rocks-high-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000024.log.trash as trash -- OK
I0917 02:10:47.731403     392 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 8after deleting file /data/store/N0:1/db/shard0/000024.log.trash
I0917 02:10:47.731785     371 [rocks-low-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000027.sst.trash as trash -- OK
I0917 02:10:47.745276     371 [rocks-low-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000025.sst.trash as trash -- OK
I0917 02:10:47.747649     393 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 12after deleting file /data/store/N0:1/db/shard1/000027.sst.trash
I0917 02:10:47.754791     393 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 23after deleting file /data/store/N0:1/db/shard1/000025.sst.trash
I0917 02:10:47.764309     371 [rocks-low-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000019.sst.trash as trash -- OK
I0917 02:10:47.771667     374 [rocks-low-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000027.sst.trash as trash -- OK
I0917 02:10:47.771804     393 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 10after deleting file /data/store/N0:1/db/shard1/000019.sst.trash
I0917 02:10:47.774378     371 [rocks-low-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000016.sst.trash as trash -- OK
I0917 02:10:47.777727     392 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 12after deleting file /data/store/N0:1/db/shard0/000027.sst.trash
I0917 02:10:47.780572     393 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 9after deleting file /data/store/N0:1/db/shard1/000016.sst.trash
I0917 02:10:47.780616     374 [rocks-low-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000025.sst.trash as trash -- OK
I0917 02:10:47.783026     371 [rocks-low-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000014.sst.trash as trash -- OK
I0917 02:10:47.784786     374 [rocks-low-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000019.sst.trash as trash -- OK
I0917 02:10:47.786938     393 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 9after deleting file /data/store/N0:1/db/shard1/000014.sst.trash
I0917 02:10:47.786987     392 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 10after deleting file /data/store/N0:1/db/shard0/000025.sst.trash
I0917 02:10:47.790969     392 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 20after deleting file /data/store/N0:1/db/shard0/000019.sst.trash
I0917 02:10:47.793557     374 [rocks-low-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000016.sst.trash as trash -- OK
I0917 02:10:47.796358     392 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 9after deleting file /data/store/N0:1/db/shard0/000016.sst.trash
I0917 02:10:47.799299     374 [rocks-low-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000014.sst.trash as trash -- OK
I0917 02:10:47.801699     392 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 9after deleting file /data/store/N0:1/db/shard0/000014.sst.trash
I0917 02:10:47.895305    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:10:47.896669    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:10:48.910119    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:10:48.911083    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:10:57.912892    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:10:57.914007    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:10:58.928739    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:10:58.929962    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:11:08.077961    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 87 log entries
I0917 02:11:08.352726    1064 [ld:s0:db-bg-fl] debug.cpp:573] log() Slow log() call: it took 0.056s to prepare, 0.218s to write().
I0917 02:11:08.179158    1212 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() skipped at least 1 log entries
W0917 02:11:08.189261    1219 [ld:srv:WB3] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.010s, source: [Request: NODES_CONFIGURATION_MANAGER]
I0917 02:11:08.417686    1219 [ld:srv:WB3] debug.cpp:573] log() Slow log() call: it took 0.000s to prepare, 0.228s to write().
I0917 02:11:08.242656    1239 [ld:watchdog] WatchDogThread.cpp:150] run() Entry into watchdog loop took 5207ms
I0917 02:11:08.390846    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 6ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:11:08.471933    1064 [ld:s0:db-bg-fl] debug.cpp:573] log() Slow log() call: it took 0.000s to prepare, 0.081s to write().
I0917 02:11:08.403172    1212 [ld:srv:WF0] debug.cpp:573] log() Slow log() call: it took 0.000s to prepare, 0.224s to write().
I0917 02:11:08.479010    1212 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N0 changed from HEALTHY to UNHEALTHY
I0917 02:11:08.445954    1239 [ld:watchdog] debug.cpp:573] log() Slow log() call: it took 0.000s to prepare, 0.203s to write().
W0917 02:11:08.481692    1212 [ld:srv:WF0] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.329s, source: [Message sent/received: GET_CLUSTER_STATE_REPLY]
I0917 02:11:08.788197    1212 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N1 changed from HEALTHY to UNHEALTHY
I0917 02:11:08.790953    1212 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N2 changed from HEALTHY to UNHEALTHY
I0917 02:11:08.791965    1212 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N3 changed from HEALTHY to UNHEALTHY
I0917 02:11:08.792920    1212 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N4 changed from HEALTHY to UNHEALTHY
I0917 02:11:09.201200    1212 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N4 changed from UNHEALTHY to HEALTHY
I0917 02:11:09.405257    1212 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N2 changed from UNHEALTHY to HEALTHY
I0917 02:11:09.529106    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:11:09.529962    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:11:10.672369    1163 [ld:srv:WG0] CONFIG_FETCH_Message.cpp:70] onReceived() CONFIG_FETCH messages received. E.g., from WG0:C49 (127.0.0.1:57004)
I0917 02:11:10.684445    1163 [ld:srv:WG0] ProtocolHandler.cpp:127] notifyErrorOnSocket() skipped at least 4 log entries
I0917 02:11:10.685391    1163 [ld:srv:WG0] ProtocolHandler.cpp:127] notifyErrorOnSocket() Socket C49(127.0.0.1:57004) hit error AsyncSocketException: Socket read end of file., type = End of file
I0917 02:11:10.686714    1163 [ld:srv:WG0] Connection.cpp:724] close() Closing socket C49(127.0.0.1:57004). Reason: PEER_CLOSED: connection closed by peer
I0917 02:11:11.949844    1216 [ld:srv:WB1] ReplicatedStateMachine-inl.h:841] onDeltaRecord() [logsconfig] Applied delta record with lsn=e2n2 ts=2021-09-17 02:11:11.944
I0917 02:11:11.950781    1216 [ld:srv:WB1] LogsConfigManager.cpp:193] operator()() skipped at least 1 log entries
I0917 02:11:11.952142    1216 [ld:srv:WB1] LogsConfigManager.cpp:193] operator()() Received update from LogsConfigStateMachine, version 8589934594
I0917 02:11:11.953155    1216 [ld:srv:WB1] AllSequencers.cpp:981] noteConfigurationChanged() Acquiring lock for sequencer map took 0ms
I0917 02:11:11.955292    1216 [ld:srv:WB1] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 0ms, config update: 0ms, notifications: 2ms
I0917 02:11:11.956245    1216 [ld:srv:WB1] LogsConfigManager.cpp:331] updateLogsConfig() Published new LogsConfig (fully loaded? yes) version (8589934594) from LogsConfigManager
I0917 02:11:16.239113    1212 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N1 changed from UNHEALTHY to HEALTHY
I0917 02:11:18.093106    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 84 log entries
I0917 02:11:18.095408    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:11:19.151892    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:11:19.152586    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:11:28.111697    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:11:28.112584    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:11:29.171058    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:11:29.171646    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:11:32.681586    1192 [ld:srv:WG4] CONFIG_FETCH_Message.cpp:70] onReceived() skipped at least 7 log entries
I0917 02:11:32.683416    1192 [ld:srv:WG4] CONFIG_FETCH_Message.cpp:70] onReceived() CONFIG_FETCH messages received. E.g., from WG4:C50 (127.0.0.1:57012)
I0917 02:11:38.131323    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:11:38.131955    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:11:39.220460    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:11:39.221119    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:11:48.149263    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:11:48.150170    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:11:49.238142    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:11:49.239096    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:11:54.884595    1192 [ld:srv:WG4] CONFIG_FETCH_Message.cpp:70] onReceived() skipped at least 6 log entries
I0917 02:11:54.885405    1192 [ld:srv:WG4] CONFIG_FETCH_Message.cpp:70] onReceived() CONFIG_FETCH messages received. E.g., from WG4:C50 (127.0.0.1:57012)
I0917 02:11:58.168293    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:11:58.169705    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:11:59.256051    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:11:59.257019    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:12:08.184802    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:12:08.185670    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:12:09.274360    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:12:09.275318    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:12:16.968645    1192 [ld:srv:WG4] CONFIG_FETCH_Message.cpp:70] onReceived() skipped at least 6 log entries
I0917 02:12:16.969572    1192 [ld:srv:WG4] CONFIG_FETCH_Message.cpp:70] onReceived() CONFIG_FETCH messages received. E.g., from WG4:C50 (127.0.0.1:57012)
I0917 02:12:18.202697    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:12:18.203827    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:12:19.302548    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:12:19.303309    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:12:28.220330    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:12:28.221468    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:12:29.320060    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:12:29.320620    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:12:38.238922    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:12:38.239984    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:12:38.884400    1192 [ld:srv:WG4] CONFIG_FETCH_Message.cpp:70] onReceived() skipped at least 6 log entries
I0917 02:12:38.885584    1192 [ld:srv:WG4] CONFIG_FETCH_Message.cpp:70] onReceived() CONFIG_FETCH messages received. E.g., from WG4:C50 (127.0.0.1:57012)
I0917 02:12:39.337621    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:12:39.338225    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:12:48.255655    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:12:48.256889    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:12:49.354631    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:12:49.355281    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:12:52.854275    1212 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N0 changed from UNHEALTHY to HEALTHY
I0917 02:12:52.957914    1212 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N3 changed from UNHEALTHY to HEALTHY
I0917 02:12:58.274344    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:12:58.274835    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:12:59.375879    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:12:59.376548    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:13:00.936772    1192 [ld:srv:WG4] CONFIG_FETCH_Message.cpp:70] onReceived() skipped at least 6 log entries
I0917 02:13:00.937964    1192 [ld:srv:WG4] CONFIG_FETCH_Message.cpp:70] onReceived() CONFIG_FETCH messages received. E.g., from WG4:C50 (127.0.0.1:57012)
I0917 02:13:08.293798    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:13:08.295026    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:13:09.391905    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:13:09.392694    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:13:18.315117    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:13:18.315872    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:13:19.410865    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:13:19.411966    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:13:22.840398    1192 [ld:srv:WG4] CONFIG_FETCH_Message.cpp:70] onReceived() skipped at least 6 log entries
I0917 02:13:22.842109    1192 [ld:srv:WG4] CONFIG_FETCH_Message.cpp:70] onReceived() CONFIG_FETCH messages received. E.g., from WG4:C50 (127.0.0.1:57012)
I0917 02:13:28.331823    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:13:28.332740    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:13:29.431215    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:13:29.431888    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:13:33.780458     316 [ld:conf] FileConfigSource.cpp:86] checkForUpdates() Change detected in config file /data/store/logdevice.conf, mtime = 1631844813641395829
I0917 02:13:33.782622     316 [ld:conf] TextConfigUpdater.cpp:116] onContents() Main config from file, hash = vj0b5rfpgt
I0917 02:13:33.785366     316 [ld:conf] TextConfigUpdater.cpp:245] compareServerConfig() Comparing new config (version 1) with existing config (version 1)
W0917 02:13:33.786234     316 [ld:conf] TextConfigUpdater.cpp:264] compareServerConfig() Received config with same version (1) but mismatched hash (9egjnxotid != vj0b5rfpgt)
I0917 02:13:33.940707    1212 [ld:srv:WF0] FailureDetector.cpp:682] processFlags() Received starting_state_finished message from N4:1 with instance id:1631844617777, sent_time:1631844813939ms
I0917 02:13:33.945291    1212 [ld:srv:WF0] FailureDetector.cpp:1845] resetVersions() Resetting RSM and NCM Versions for N4
I0917 02:13:33.950736    1212 [ld:srv:WF0] FailureDetector.cpp:682] processFlags() Received starting_state_finished message from N3:1 with instance id:1631844617866, sent_time:1631844813945ms
I0917 02:13:33.954191    1212 [ld:srv:WF0] FailureDetector.cpp:1845] resetVersions() Resetting RSM and NCM Versions for N3
I0917 02:13:33.970381    1212 [ld:srv:WF0] FailureDetector.cpp:682] processFlags() Received starting_state_finished message from N2:1 with instance id:1631844617789, sent_time:1631844813966ms
I0917 02:13:33.972128    1212 [ld:srv:WF0] FailureDetector.cpp:1845] resetVersions() Resetting RSM and NCM Versions for N2
I0917 02:13:33.985628    1212 [ld:srv:WF0] FailureDetector.cpp:682] processFlags() Received starting_state_finished message from N1:1 with instance id:1631844617782, sent_time:1631844813985ms
I0917 02:13:33.985965     316 [ld:conf] Server.cpp:185] operator()() Updating settings from config took 198ms
I0917 02:13:33.987106    1212 [ld:srv:WF0] FailureDetector.cpp:1845] resetVersions() Resetting RSM and NCM Versions for N1
I0917 02:13:33.989569     316 [ld:conf] UpdateableSecurityInfo.cpp:124] onConfigUpdate() PermissionChecker is changed
I0917 02:13:33.991401     316 [ld:conf] AllSequencers.cpp:981] noteConfigurationChanged() Acquiring lock for sequencer map took 0ms
I0917 02:13:33.992767    1212 [ld:srv:WF0] FailureDetector.cpp:1456] broadcastBringupUpdate() Broadcasting starting_state_finished message.
I0917 02:13:33.993797    1212 [ld:srv:WF0] FailureDetector.cpp:1476] broadcastBringupUpdate() skipped at least 6 log entries
I0917 02:13:33.993499     316 [ld:conf] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 201ms, config update: 0ms, notifications: 4ms
I0917 02:13:33.995590     316 [ld:conf] TextConfigUpdater.cpp:306] pushServerConfig() Updated config (version: 1 - hash: vj0b5rfpgt)
I0917 02:13:33.994741    1212 [ld:srv:WF0] FailureDetector.cpp:1476] broadcastBringupUpdate() Sending starting_state_finished message with instance id:1631844617844 to node WF0:N4:1 (127.0.0.1:39321)
I0917 02:13:33.996341     316 [ld:conf] TextConfigUpdater.cpp:397] pushLogsConfig() Ignoring changes in the 'logs' section in the config file because LogsConfigManager is ENABLED
I0917 02:13:33.998294     316 [ld:conf] TextConfigUpdater.cpp:174] update() Config update result: ServerConfig=updated, ZK=skipped, LogsConfig=updated
I0917 02:13:33.997850    1212 [ld:srv:WF0] FailureDetector.cpp:1476] broadcastBringupUpdate() Sending starting_state_finished message with instance id:1631844617844 to node WF0:N3:1 (127.0.0.1:39332)
I0917 02:13:38.351187    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 88 log entries
I0917 02:13:38.351898    1064 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:13:39.448913    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:13:39.449885    1045 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:13:41.348947    1192 [ld:srv:WG4] ProtocolHandler.cpp:127] notifyErrorOnSocket() Socket C51(127.0.0.1:57022) hit error AsyncSocketException: Socket read end of file., type = End of file
I0917 02:13:41.349002    1173 [ld:srv:WG1] Connection.cpp:724] close() Closing socket C52(127.0.0.1:57024). Reason: PEER_CLOSED: connection closed by peer
I0917 02:14:06.757248     760 [logdeviced-main] main.cpp:399] main() Command line arguments: --local-log-store-path /data/store/N0:1/db --num-shards 2 --server-id 0z607uccdt --log-file /data/store/N0:1/log --name Node0 --config-path file:/data/store/logdevice.conf --client-thrift-api-port 48365 --gossip-port 48359 --server-thrift-api-port 48364 --test-mode true --server-to-server-port 48363 --admin-port 48360 --loglevel info --port 48357 --sequencers lazy 
I0917 02:14:06.759020     760 [logdeviced-main] main.cpp:403] main() Plugins loaded: {"Thrift client factory plugin":"built-in","ZookeeperClient factory":"built-in","ConfigSource factory":["built-in"],"PermissionChecker factory":"built-in","Build Info":"built-in","Plugin provider":["Built-in plugin provider","Static plug-in loader","Dynamic plugin loader"]}
I0917 02:14:06.760616     760 [logdeviced-main] main.cpp:405] main() server starting
I0917 02:14:06.763242     760 [logdeviced-main] main.cpp:410] main() version 99.99.99
I0917 02:14:06.766567     760 [logdeviced-main] main.cpp:418] main() asserts off (NDEBUG set)
I0917 02:14:06.772069     760 [logdeviced-main] TextConfigUpdater.cpp:56] fetchFromSource() Requesting main config "/data/store/logdevice.conf" (source: file)
I0917 02:14:06.775046     760 [logdeviced-main] TextConfigUpdater.cpp:116] onContents() Main config from file, hash = hyg3dqvegx
I0917 02:14:07.322431     760 [logdeviced-main] Server.cpp:185] operator()() Updating settings from config took 544ms
I0917 02:14:07.323770     760 [logdeviced-main] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 545ms, config update: 0ms, notifications: 0ms
I0917 02:14:07.324645     760 [logdeviced-main] TextConfigUpdater.cpp:306] pushServerConfig() Updated config (version: 1 - hash: hyg3dqvegx)
I0917 02:14:07.325688     760 [logdeviced-main] TextConfigUpdater.cpp:397] pushLogsConfig() Ignoring changes in the 'logs' section in the config file because LogsConfigManager is ENABLED
I0917 02:14:07.326381     760 [logdeviced-main] TextConfigUpdater.cpp:174] update() Config update result: ServerConfig=updated, ZK=skipped, LogsConfig=updated
I0917 02:14:07.327746     760 [logdeviced-main] FileBasedVersionedConfigStore.cpp:35] FileBasedVersionedConfigStore() FileBasedVersionedConfigStore threads started.
I0917 02:14:07.344004     760 [logdeviced-main] NodesConfigurationInit.cpp:233] operator()() Fetching initial nodes configuration (Attempt 1)...
I0917 02:14:07.351407     768 [ld:file-ncs] NodesConfigurationInit.cpp:290] operator()() Got a NodesConfiguration of version: 5
I0917 02:14:07.352608     768 [ld:file-ncs] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 0ms, config update: 0ms, notifications: 0ms
I0917 02:14:07.369826     760 [logdeviced-main] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 0ms, config update: 0ms, notifications: 0ms
I0917 02:14:07.370979     760 [logdeviced-main] Server.cpp:510] init() My Node ID is N0:1
I0917 02:14:07.371763     760 [logdeviced-main] Server.cpp:517] init() My version is 0
I0917 02:14:07.372540     760 [logdeviced-main] NoopTraceLogger.cpp:22] NoopTraceLogger() NOOP TraceLogger is ON, no trace samples will be published.
I0917 02:14:07.376975     760 [logdeviced-main] FileBasedVersionedConfigStore.cpp:46] stopAndJoin() FileBasedVersionedConfigStore threads stopped.
I0917 02:14:07.378323     760 [logdeviced-main] main.cpp:144] set_fd_limit() Limit on number of file descriptors is 1048576
W0917 02:14:07.379263     760 [logdeviced-main] main.cpp:192] drop_root() Running under root, but no --user command line option specified. Will keep running as root
I0917 02:14:07.383267     760 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 48357
I0917 02:14:07.384986     760 [logdeviced-main] Server.cpp:781] initListeners() Initializing a gossip listener.
I0917 02:14:07.386454     760 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 48359
I0917 02:14:07.394442     760 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 48363 (SSL)
I0917 02:14:07.396222     760 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 48367 (SSL)
I0917 02:14:07.400196     760 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 48366 (SSL)
I0917 02:14:07.421969     760 [logdeviced-main] RocksDBLogStoreConfig.cpp:162] RocksDBLogStoreConfig() LD manages flushes and all memory budgets on the node. atomic_flush is set to true.
I0917 02:14:07.425173     774 [io-stall:s1] IOTracing.cpp:83] stallDetectionThreadMain() Started IO stall detection thread for shard 1
I0917 02:14:07.429516     773 [io-stall:s0] IOTracing.cpp:83] stallDetectionThreadMain() Started IO stall detection thread for shard 0
I0917 02:14:07.437428     782 [ld:open-rocks1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:387] Created background thread for deletion scheduler with rate_bytes_per_sec: 100000000
I0917 02:14:07.444402     781 [ld:open-rocks0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:387] Created background thread for deletion scheduler with rate_bytes_per_sec: 100000000
I0917 02:14:07.749315     782 [ld:open-rocks1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000037.log.trash as trash -- OK
I0917 02:14:07.756034     783 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 0after deleting file /data/store/N0:1/db/shard1/000037.log.trash
I0917 02:14:07.829559     782 [ld:open-rocks1] PartitionedRocksDBStore.cpp:891] open() Partition s1:1000000 is under-replicated: Start:2021-09-17 02:10:14.858, Min:2021-09-17 02:10:09.858, Max: 2021-09-17 02:10:19.858
I0917 02:14:07.834861     782 [ld:open-rocks1] PartitionedRocksDBStore.cpp:897] open() Shard[1] dirty ranges: [A:{{[2021-09-17 02:10:09.858,2021-09-17 02:10:19.858]}}]
I0917 02:14:07.850122     782 [ld:open-rocks1] PartitionedRocksDBStore.cpp:1491] convertDataKeyFormat() DataKey format conversion took 0.001s. Read 6 records, 386 bytes. Wrote 0 records, 0 bytes.
I0917 02:14:08.033581     781 [ld:open-rocks0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000037.log.trash as trash -- OK
I0917 02:14:08.062527     784 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 0after deleting file /data/store/N0:1/db/shard0/000037.log.trash
I0917 02:14:08.073841     780 [rocks-high-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000043.log.trash as trash -- OK
I0917 02:14:08.079683     931 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 1ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:14:08.080078     783 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 1after deleting file /data/store/N0:1/db/shard1/000043.log.trash
I0917 02:14:08.081978     782 [ld:open-rocks1] ShardedRocksDBLocalLogStore.cpp:232] operator()() Opened RocksDB instance at /data/store/N0:1/db/shard1 in 639 ms
I0917 02:14:08.142743     781 [ld:open-rocks0] PartitionedRocksDBStore.cpp:891] open() Partition s0:1000000 is under-replicated: Start:2021-09-17 02:10:14.859, Min:2021-09-17 02:10:09.859, Max: 2021-09-17 02:10:19.859
I0917 02:14:08.152681     781 [ld:open-rocks0] PartitionedRocksDBStore.cpp:897] open() Shard[0] dirty ranges: [A:{{[2021-09-17 02:10:09.859,2021-09-17 02:10:19.859]}}]
I0917 02:14:08.167294     781 [ld:open-rocks0] PartitionedRocksDBStore.cpp:1491] convertDataKeyFormat() DataKey format conversion took 0.000s. Read 5 records, 341 bytes. Wrote 0 records, 0 bytes.
I0917 02:14:08.233431     779 [rocks-high-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000043.log.trash as trash -- OK
I0917 02:14:08.240486     781 [ld:open-rocks0] ShardedRocksDBLocalLogStore.cpp:232] operator()() Opened RocksDB instance at /data/store/N0:1/db/shard0 in 794 ms
I0917 02:14:08.240752     784 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 1after deleting file /data/store/N0:1/db/shard0/000043.log.trash
I0917 02:14:08.254872     760 [logdeviced-main] ShardedRocksDBLocalLogStore.cpp:691] printDiskShardMapping() Disk -> Shard mapping: /data/store/N0:1/db/shard0 -> [0,1]
I0917 02:14:08.257930     760 [logdeviced-main] ShardedRocksDBLocalLogStore.cpp:285] init() Initialized sharded RocksDB instance at /data/store/N0:1/db with 2 shards
E0917 02:14:08.263851    1133 [ld:populateLogS] ThreadID.cpp:26] set() Truncating thread name "ld:populateLogState1"!
E0917 02:14:08.265389    1132 [ld:populateLogS] ThreadID.cpp:26] set() Truncating thread name "ld:populateLogState0"!
I0917 02:14:08.274732     760 [logdeviced-main] Server.cpp:1173] initLogStorageStateMap() Populating log storage state map successful.
E0917 02:14:08.320777     760 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:08.323348     760 [logdeviced-main] RecordCacheMonitorThread.cpp:27] RecordCacheMonitorThread() Record cache monitor thread started.
I0917 02:14:08.324325    1211 [ld:cache-evict] RecordCacheMonitorThread.cpp:44] threadMain() Record cache eviction thread started. Size limit is set to 4294967296 bytes.
I0917 02:14:08.324791     760 [logdeviced-main] ClusterState.cpp:356] resizeClusterState() Cluster state size updated from 0 to 5
E0917 02:14:08.332255     760 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:14:08.354984     760 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:14:08.376029     760 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:14:08.392878     760 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:14:08.413081     760 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:08.415368     760 [logdeviced-main] Processor.cpp:247] init() Initialized 5 workers of type GENERAL
E0917 02:14:08.422342     760 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:08.437714     760 [logdeviced-main] Processor.cpp:247] init() Initialized 1 workers of type FAILURE_DETECTOR
E0917 02:14:08.455591     760 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:14:08.477967     760 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:14:08.498776     760 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:14:08.507959     760 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:08.521375     760 [logdeviced-main] Processor.cpp:247] init() Initialized 4 workers of type BACKGROUND
I0917 02:14:08.527164     760 [logdeviced-main] FailureDetector.cpp:139] FailureDetector() Failure Detector created with instance id: 1631844848527
I0917 02:14:08.533194     760 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N0 changed from UNKNOWN to DEAD
I0917 02:14:08.534205     760 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N0 changed from HEALTHY to UNHEALTHY
I0917 02:14:08.535925     760 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N1 changed from UNKNOWN to DEAD
I0917 02:14:08.538278     760 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N1 changed from HEALTHY to UNHEALTHY
I0917 02:14:08.541674     760 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N2 changed from UNKNOWN to DEAD
I0917 02:14:08.543540     760 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N2 changed from HEALTHY to UNHEALTHY
I0917 02:14:08.559026     760 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N3 changed from UNKNOWN to DEAD
I0917 02:14:08.564246     760 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N3 changed from HEALTHY to UNHEALTHY
I0917 02:14:08.565968     760 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N4 changed from UNKNOWN to DEAD
I0917 02:14:08.567777     760 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N4 changed from HEALTHY to UNHEALTHY
W0917 02:14:08.570917    1217 [ld:srv:WG3] Worker.cpp:1322] processRequest() Request queued for 43 msec: BUFFERED_WRITER_CREATE_SHARD (id: 4294967299), p :HI_PRI
W0917 02:14:08.575264    1216 [ld:srv:WG2] Worker.cpp:1322] processRequest() Request queued for 49 msec: BUFFERED_WRITER_CREATE_SHARD (id: 4294967298), p :HI_PRI
I0917 02:14:08.589673     760 [logdeviced-main] FileBasedVersionedConfigStore.cpp:35] FileBasedVersionedConfigStore() FileBasedVersionedConfigStore threads started.
I0917 02:14:08.594378    1237 [ld:srv:WB3] AllSequencers.cpp:981] noteConfigurationChanged() Acquiring lock for sequencer map took 0ms
I0917 02:14:08.599007    1237 [ld:srv:WB3] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 0ms, config update: 0ms, notifications: 5ms
W0917 02:14:08.607035    1215 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 72 msec: NODE_STATE_UPDATED (id: 4294967302), p :LO_PRI
W0917 02:14:08.614803    1215 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 80 msec: NODE_STATE_UPDATED (id: 4294967308), p :LO_PRI
W0917 02:14:08.616388    1215 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 78 msec: NODE_STATE_UPDATED (id: 4294967312), p :LO_PRI
W0917 02:14:08.623616    1215 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 85 msec: NODE_STATE_UPDATED (id: 4294967318), p :LO_PRI
W0917 02:14:08.624675    1215 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 81 msec: NODE_STATE_UPDATED (id: 4294967322), p :LO_PRI
W0917 02:14:08.625813    1215 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 82 msec: NODE_STATE_UPDATED (id: 4294967328), p :LO_PRI
W0917 02:14:08.626666    1215 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 62 msec: NODE_STATE_UPDATED (id: 4294967332), p :LO_PRI
W0917 02:14:08.627680    1215 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 63 msec: NODE_STATE_UPDATED (id: 4294967338), p :LO_PRI
I0917 02:14:08.629448    1237 [ld:srv:WB3] NodesConfigurationManager.cpp:507] onProcessingFinished() Updated local nodes config to version 5...
I0917 02:14:08.631519    1208 [ld:s1:default:1] PartitionedRocksDBStore.cpp:3527] readAllLogSnapshotBlobs() Snapshots column family does not exist
I0917 02:14:08.638485    1208 [ld:s1:default:1] RecordCacheRepopulationTask.cpp:103] execute() Repopulated record caches for 0 logs on shard 1, totaling 0 bytes.
I0917 02:14:08.634035    1178 [ld:s0:default:0] PartitionedRocksDBStore.cpp:3527] readAllLogSnapshotBlobs() Snapshots column family does not exist
I0917 02:14:08.641571    1178 [ld:s0:default:0] RecordCacheRepopulationTask.cpp:103] execute() Repopulated record caches for 0 logs on shard 0, totaling 0 bytes.
I0917 02:14:08.648770     760 [logdeviced-main] Server.cpp:1319] initSequencers() Initializing FileEpochStore
I0917 02:14:08.652183     760 [logdeviced-main] Server.cpp:1398] initSequencerPlacement() using SequencerOptions::LAZY
I0917 02:14:08.656628     760 [logdeviced-main] Server.cpp:1430] initRebuildingCoordinator() Initializing EventLog RSM and RebuildingCoordinator
I0917 02:14:08.658623     760 [logdeviced-main] RsmServerSnapshotStoreFactory.h:25] create() Attempting to create snapshot store (type:1, key:4611686018427387903)
I0917 02:14:08.661499     760 [logdeviced-main] RsmSnapshotStoreFactory.h:31] create() Attempting to create snapshot store (type:1, key:4611686018427387903, is_server:1)
I0917 02:14:08.666764     760 [logdeviced-main] RsmSnapshotStoreFactory.h:58] create() Creating LogBasedRSMSnapshotStore
I0917 02:14:08.669240     760 [logdeviced-main] Server.cpp:1467] initRebuildingCoordinator() Starting RebuildingSupervisor
I0917 02:14:08.675750    1220 [ld:srv:WG4] RebuildingSupervisor.cpp:62] init() Starting Rebuilding Supervisor
I0917 02:14:08.677028    1220 [ld:srv:WG4] RebuildingSupervisor.cpp:222] addForRebuilding() Scheduling rebuilding of N4 to execute in 1200s
I0917 02:14:08.678027    1220 [ld:srv:WG4] RebuildingSupervisor.cpp:222] addForRebuilding() Scheduling rebuilding of N3 to execute in 1200s
I0917 02:14:08.679006    1220 [ld:srv:WG4] RebuildingSupervisor.cpp:222] addForRebuilding() Scheduling rebuilding of N2 to execute in 1200s
I0917 02:14:08.680940    1220 [ld:srv:WG4] RebuildingSupervisor.cpp:222] addForRebuilding() Scheduling rebuilding of N1 to execute in 1200s
W0917 02:14:08.683631    1220 [ld:srv:WG4] RebuildingSupervisor.cpp:705] adjustRebuildingThrottle() Entering throttling mode: #draining=0, #rebuilding=0, #triggers=4, #adjusted cluster size=5, threshold=35%, actual=80%
I0917 02:14:08.684772    1220 [ld:srv:WG4] RebuildingSupervisor.cpp:718] adjustRebuildingThrottle() Entering throttling mode: #draining=0, #rebuilding=0, #triggers=4, #adjusted cluster size=5, threshold=35%, actual=80%
I0917 02:14:08.685584     760 [logdeviced-main] Server.cpp:1480] initRebuildingCoordinator() Starting RebuildingCoordinator
W0917 02:14:08.686556    1220 [ld:srv:WG4] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.011s, source: [Request: REBUILDING_SUPERVISOR]
I0917 02:14:08.697035    1237 [ld:srv:WB3] RebuildingCoordinator.cpp:309] subscribeToEventLogIfNeeded() RebuildingCoordinator did not start yet because LogsConfig is not fully loaded yet
I0917 02:14:08.724946     760 [logdeviced-main] LogStoreMonitor.cpp:190] start() Logstore monitoring thread started.
I0917 02:14:08.731384     760 [logdeviced-main] UnreleasedRecordDetector.cpp:106] start() Unreleased record detector thread started.
I0917 02:14:08.732729     760 [logdeviced-main] RsmServerSnapshotStoreFactory.h:25] create() Attempting to create snapshot store (type:1, key:4611686018427387901)
I0917 02:14:08.725024    1237 [ld:srv:WB3] ReplicatedStateMachine-inl.h:667] onBaseSnapshotRetrieved() [eventlog] Base snapshot has version:e0n1, delta_log_read_ptr:LSN_INVALID
I0917 02:14:08.739001    1237 [ld:srv:WB3] EventLogStateMachine.cpp:146] gotInitialState() Got base rebuilding set: {}
I0917 02:14:08.740409    1237 [ld:srv:WB3] ReplicatedStateMachine-inl.h:684] getDeltaLogTailLSN() [eventlog] Retrieving tail lsn of delta log...
W0917 02:14:08.741563    1237 [ld:srv:WB3] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.017s, source: [Request: START_EVENT_LOG_READER]
I0917 02:14:08.742510    1237 [ld:srv:WB3] ReplicatedStateMachine-inl.h:667] onBaseSnapshotRetrieved() [maintenancelog] Base snapshot has version:e0n1, delta_log_read_ptr:LSN_INVALID
I0917 02:14:08.736593     760 [logdeviced-main] RsmSnapshotStoreFactory.h:31] create() Attempting to create snapshot store (type:1, key:4611686018427387901, is_server:1)
I0917 02:14:08.746743     760 [logdeviced-main] RsmSnapshotStoreFactory.h:58] create() Creating LogBasedRSMSnapshotStore
E0917 02:14:08.742595    1217 [ld:srv:WG3] HashBasedSequencerLocator.cpp:208] locateSequencer() No available sequencer node for log 4611686018427387903. All sequencer nodes are unavailable.
E0917 02:14:08.749252    1217 [ld:srv:WG3] SequencerRouter.cpp:56] operator()() Error during sequencer lookup for log 4611686018427387903 (NOSEQUENCER), reporting NOSEQUENCER.
I0917 02:14:08.743778    1237 [ld:srv:WB3] ReplicatedStateMachine-inl.h:684] getDeltaLogTailLSN() [maintenancelog] Retrieving tail lsn of delta log...
I0917 02:14:08.748466    1235 [ld:srv:WB1] LogsConfigManager.cpp:38] execute() Starting LogsConfigManager on Worker 1 in pool: BACKGROUND
I0917 02:14:08.756412    1235 [ld:srv:WB1] LogsConfigManager.cpp:165] start() Starting LogsConfig Manager RSM
E0917 02:14:08.750765    1217 [ld:srv:WG3] SequencerRouter.cpp:105] onFailure() Error during sequencer routing for log 4611686018427387903 (NOSEQUENCER), request[type:GET_SEQ_STATE_REQ_TYPE, handler:0x7f3f8bb5b028].
E0917 02:14:08.752995    1220 [ld:srv:WG4] HashBasedSequencerLocator.cpp:208] locateSequencer() No available sequencer node for log 4611686018427387899. All sequencer nodes are unavailable.
I0917 02:14:08.758005    1235 [ld:srv:WB1] LogsConfigManager.cpp:206] start() Starting LogsConfig Replicated State Machine
I0917 02:14:08.774994    1235 [ld:srv:WB1] ReplicatedStateMachine-inl.h:38] startFetchingSnapshot() [logsconfig] Using RSMSnapshotStore...
W0917 02:14:08.759999    1217 [ld:srv:WG3] GetSeqStateRequest.cpp:512] onSequencerRoutingFailure() Unable to find a sequencer node for log 4611686018427387903, rqid:8589934613, ctx:rsm, err(NOSEQUENCER)
W0917 02:14:08.761451    1220 [ld:srv:WG4] GetSeqStateRequest.cpp:700] finalize() GET_SEQ_STATE for log 4611686018427387899 (rqid:8589934615, ctx:rsm) returned NOSEQUENCER: no sequencer was found for log
I0917 02:14:08.777344    1235 [ld:srv:WB1] ReplicatedStateMachine-inl.h:1294] initSnapshotFetchTimer() [logsconfig] Creating timer to fetch snapshots ...
E0917 02:14:08.781895    1237 [ld:srv:WB3] SyncSequencerRequest.cpp:178] onGotSeqState() Got sequencer state (id:8589934613) for log:4611686018427387903, status:NOSEQUENCER: no sequencer was found for log,sequencer:[invalid NodeID], last_released_lsn:LSN_INVALID, next_lsn:LSN_INVALID
E0917 02:14:08.788633    1237 [ld:srv:WB3] SyncSequencerRequest.cpp:178] onGotSeqState() Got sequencer state (id:8589934615) for log:4611686018427387899, status:NOSEQUENCER: no sequencer was found for log,sequencer:[invalid NodeID], last_released_lsn:LSN_INVALID, next_lsn:LSN_INVALID
I0917 02:14:08.810765     760 [logdeviced-main] LogDeviceThriftServer.cpp:44] LogDeviceThriftServer() Thrift server LogDevice Admin API Service will listen on port 48360
I0917 02:14:08.820355     760 [logdeviced-main] Server.cpp:1540] createAndAttachMaintenanceManager() Not initializing MaintenanceManager since it is disabled in settings
I0917 02:14:08.824276     760 [logdeviced-main] Server.cpp:905] initThriftServer() Initializing Thrift Server: s2s-api
I0917 02:14:08.827441     760 [logdeviced-main] LogDeviceThriftServer.cpp:44] LogDeviceThriftServer() Thrift server s2s-api will listen on port 48364
I0917 02:14:08.829158     760 [logdeviced-main] Server.cpp:905] initThriftServer() Initializing Thrift Server: c2s-api
I0917 02:14:08.831418     760 [logdeviced-main] LogDeviceThriftServer.cpp:44] LogDeviceThriftServer() Thrift server c2s-api will listen on port 48365
I0917 02:14:08.835340     760 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:14:08.840200     769 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:14:08.845946     760 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:14:08.849277     770 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:14:08.851836     760 [logdeviced-main] SimpleThriftServer.cpp:28] start() Starting a listener thread for Thrift server s2s-api
I0917 02:14:08.852658    1223 [ld:srv:WF0] FailureDetector.cpp:228] startGetClusterState() Sending GET_CLUSTER_STATE to build initial FD cluster view
I0917 02:14:08.866279    1223 [ld:srv:WF0] ProtocolHandler.cpp:127] notifyErrorOnSocket() Socket N1:1(127.0.0.1:48348) hit error AsyncSocketException: connect failed, type = Socket not open, errno = 111 (Connection refused)
I0917 02:14:08.870451    1223 [ld:srv:WF0] GetClusterStateRequest.cpp:252] onError() Retrieving the state of the cluster failed. sending another wave.
E0917 02:14:08.880069    1223 [ld:srv:WF0] GetClusterStateRequest.cpp:243] onError() Retrieving the state of the cluster failed. giving up.
E0917 02:14:08.882372    1223 [ld:srv:WF0] FailureDetector.cpp:243] operator()() Unable to refresh cluster state: FAILED: request failed
I0917 02:14:08.886406    1223 [ld:srv:WF0] FailureDetector.cpp:288] buildInitialState() Wait over
I0917 02:14:08.887385    1223 [ld:srv:WF0] FailureDetector.cpp:344] buildInitialState() Initializing DomainIsolationChecker
I0917 02:14:08.889452    1223 [ld:srv:WF0] FailureDetector.cpp:915] startSuspectTimer() Starting suspect state timer
I0917 02:14:08.891527    1223 [ld:srv:WF0] FailureDetector.cpp:1285] updateNodeState() N0 transitioned from DEAD to ALIVE, FD State:(gossip: 0, instance-id: 0, failover: 0, starting: 1)
I0917 02:14:08.892935    1223 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N0 changed from DEAD to STARTING
I0917 02:14:08.895104    1223 [ld:srv:WF0] FailureDetector.cpp:1456] broadcastBringupUpdate() Broadcasting suspect-state-finished message.
I0917 02:14:08.898031     760 [logdeviced-main] SimpleThriftServer.cpp:28] start() Starting a listener thread for Thrift server c2s-api
I0917 02:14:08.898775    1223 [ld:srv:WF0] FailureDetector.cpp:1476] broadcastBringupUpdate() Sending suspect-state-finished message with instance id:1631844848527 to node WF0:N4:1 (127.0.0.1:48315)
I0917 02:14:08.910549    1223 [ld:srv:WF0] FailureDetector.cpp:1476] broadcastBringupUpdate() Sending suspect-state-finished message with instance id:1631844848527 to node WF0:N3:1 (127.0.0.1:48326)
I0917 02:14:08.919772     760 [logdeviced-main] SimpleThriftServer.cpp:28] start() Starting a listener thread for Thrift server LogDevice Admin API Service
I0917 02:14:08.919989    1223 [ld:srv:WF0] FailureDetector.cpp:1456] broadcastBringupUpdate() Broadcasting bringup message.
I0917 02:14:08.929794    1223 [ld:srv:WF0] FailureDetector.cpp:937] startGossiping() Start Gossiping.
I0917 02:14:08.940966    1223 [ld:srv:WF0] FailureDetector.cpp:925] operator()() Suspect timeout expired
I0917 02:14:08.965387     760 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:14:08.971362     771 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:14:08.984470     760 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:14:08.990585     769 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:14:08.992601     760 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:14:08.993775     769 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:14:08.995614     760 [logdeviced-main] main.cpp:464] main() Listeners initialized
I0917 02:14:09.055698    1223 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N0 changed from UNHEALTHY to UNDEFINED
I0917 02:14:09.063015    1223 [ld:srv:WF0] FailureDetector.cpp:1541] onGossipMessageSent() Could not send gossip to WF0:N1:1 (127.0.0.1:48348): CONNFAILED: connection failed. Trying another node.
I0917 02:14:09.091625    1223 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N0 changed from UNDEFINED to HEALTHY
I0917 02:14:09.108206    1223 [ld:srv:WF0] ProtocolHandler.cpp:127] notifyErrorOnSocket() skipped at least 10 log entries
I0917 02:14:09.112789    1223 [ld:srv:WF0] ProtocolHandler.cpp:127] notifyErrorOnSocket() Socket N3:1(127.0.0.1:48326) hit error AsyncSocketException: connect failed, type = Socket not open, errno = 111 (Connection refused)
W0917 02:14:09.127790    1223 [ld:srv:WF0] FailureDetector.cpp:1529] onGossipMessageSent() Could not send gossip to WF0:N2:1 (127.0.0.1:48337): CONNFAILED: connection failed. Consecutively failed to send 5 gossips.
W0917 02:14:09.256271    1223 [ld:srv:WF0] FailureDetector.cpp:496] gossip() Unable to find a node to send a gossip message to
I0917 02:14:09.283052    1212 [ld:srv:WG0] GET_SEQ_STATE_Message.cpp:238] getSequencer() Sequencer for log:4611686018427387903 is absent, state=UNAVAILABLE, cur_epoch=0, attempting to activate it upon receiving GSS (rqid:8589934607, ctx:rsm, min_epoch:none) from WG0:C2 (127.0.0.1:41904)
I0917 02:14:09.288436     931 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:14:09.294207     931 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:14:09.315477    1212 [ld:srv:WG0] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387903, rqid:8589934607, ctx:rsm) from WG0:C2 (127.0.0.1:41904), but its data log sequencer's state is ACTIVATING
I0917 02:14:09.320073    1212 [ld:srv:WG0] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 4611686018427387903 epoch 4.
I0917 02:14:09.321815    1212 [ld:srv:WG0] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 4611686018427387903: {[E:1 (at 2021-09-17 02:14:09.296) since:1 (at 2021-09-17 02:10:15.661) R:[node: 3] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]], Until:4}.
I0917 02:14:09.323895    1212 [ld:srv:WG0] AllSequencers.cpp:527] onEpochMetaDataFromEpochStore() Activated a sequencer for log 4611686018427387903 (reason: GET_SEQ_STATE) with epoch 4, metadata: [E:4 (at 2021-09-17 02:14:09.296) since:1 (at 2021-09-17 02:10:15.661) R:[node: 3] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]]. Activation Result: RECOVERY.
I0917 02:14:09.328758    1212 [ld:srv:WG0] PeriodicReleases.cpp:416] execute() Start broadcasting releases for log 4611686018427387903.
I0917 02:14:09.335554    1212 [ld:srv:WG0] MetaDataLogTrimmer.cpp:115] schedulePeriodicTrimming() Will run periodic trimming every ~7200000 s for 4611686018427387903
I0917 02:14:09.329512    1215 [ld:srv:WG1] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 13835058055282163711 epoch 4.
I0917 02:14:09.345323    1215 [ld:srv:WG1] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 13835058055282163711: {[E:1 (at -inf) since:1 (at -inf) R:[node: 3] V:2 flags: params:{target:0 seed:0 signature:0} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]], Until:4}.
I0917 02:14:09.330753    1217 [ld:srv:WG3] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 4611686018427387903 with next_epoch 4
I0917 02:14:09.349746    1215 [ld:srv:WG1] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 13835058055282163711 with next_epoch 4
I0917 02:14:09.359955    1217 [ld:srv:WG3] LogRecoveryRequest.cpp:1018] sealLog() Starting recovery of epochs [2, 3] of log 4611686018427387903
W0917 02:14:09.366808    1215 [ld:srv:WG1] Worker.cpp:1000] onStoppedRunning() skipped at least 10 log entries
W0917 02:14:09.368623    1215 [ld:srv:WG1] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.017s, source: [Request: LOG_RECOVERY]
I0917 02:14:09.373503    1215 [ld:srv:WG1] LogRecoveryRequest.cpp:1018] sealLog() Starting recovery of epochs [2, 3] of log 13835058055282163711
I0917 02:14:09.388234    1223 [ld:srv:WF0] FailureDetector.cpp:682] processFlags() Received suspect-state-finished message from N2:1 with instance id:1631844848952, sent_time:1631844849358ms
I0917 02:14:09.390742    1223 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N2 changed from DEAD to STARTING
I0917 02:14:09.392927    1220 [ld:srv:WG4] RebuildingSupervisor.cpp:256] onNodeStateChanged() Cancelling rebuilding trigger for node N2
I0917 02:14:09.392927    1223 [ld:srv:WF0] FailureDetector.cpp:717] processFlags() N2 transitioned to STARTING as a result of receiving suspect-state-finished message, FD State:(gossip: 0, instance-id: 1631844848952, failover: 0, starting: 1)
I0917 02:14:09.397067    1223 [ld:srv:WF0] FailureDetector.cpp:682] processFlags() Received bringup message from N2:1 with instance id:1631844848952, sent_time:1631844849374ms
I0917 02:14:09.400730    1223 [ld:srv:WF0] FailureDetector.cpp:1845] resetVersions() Resetting RSM and NCM Versions for N2
I0917 02:14:09.403000    1223 [ld:srv:WF0] FailureDetector.cpp:682] processFlags() Received suspect-state-finished message from N4:1 with instance id:1631844848982, sent_time:1631844849378ms
I0917 02:14:09.405717    1223 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N4 changed from DEAD to STARTING
I0917 02:14:09.409987    1223 [ld:srv:WF0] FailureDetector.cpp:717] processFlags() N4 transitioned to STARTING as a result of receiving suspect-state-finished message, FD State:(gossip: 0, instance-id: 1631844848982, failover: 0, starting: 1)
I0917 02:14:09.409953    1220 [ld:srv:WG4] RebuildingSupervisor.cpp:256] onNodeStateChanged() Cancelling rebuilding trigger for node N4
I0917 02:14:09.412478    1223 [ld:srv:WF0] FailureDetector.cpp:682] processFlags() Received bringup message from N4:1 with instance id:1631844848982, sent_time:1631844849383ms
I0917 02:14:09.416303    1223 [ld:srv:WF0] FailureDetector.cpp:1845] resetVersions() Resetting RSM and NCM Versions for N4
I0917 02:14:09.485681    1223 [ld:srv:WF0] FailureDetector.cpp:1541] onGossipMessageSent() skipped at least 10 log entries
I0917 02:14:09.486965    1223 [ld:srv:WF0] FailureDetector.cpp:1541] onGossipMessageSent() Could not send gossip to WF0:N3:1 (127.0.0.1:48326): CONNFAILED: connection failed. Trying another node.
I0917 02:14:09.607878    1217 [ld:srv:WG3] EpochRecovery.cpp:1502] onDigestMayHaveBecomeComplete() Completing an authoritative partial digest for epoch 2 of log 4611686018427387903 because grace period with timeout of 100ms expired. State: {s:N1:S1,N3:S1 m:N0:S1,N2:S1,N4:S1 DIGEST}
I0917 02:14:09.609483    1217 [ld:srv:WG3] EpochRecovery.cpp:566] startMutations() Log 4611686018427387903 epoch 2 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_INCOMPLETE, mutation set: {N0:S1, N2:S1, N4:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387903 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387903 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {s:N1:S1,N3:S1 m:N0:S1,N2:S1,N4:S1 MUTATION}.
I0917 02:14:09.608289    1215 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 13835058055282163711 epoch 2 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_INCOMPLETE, mutation set: {N0:S1, N2:S1, N4:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163711 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163711 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {s:N1:S1,N3:S1 m:N0:S1,N2:S1,N4:S1 MUTATION}.
W0917 02:14:09.618538    1215 [ld:srv:WG1] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.010s, source: [Message sent/received: GAP]
I0917 02:14:09.623577    1220 [ld:srv:WG4] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163711 has already been cleaned. No purge needed. last clean epoch: 1, purge to: 1.
I0917 02:14:09.662729    1215 [ld:srv:WG1] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387903, rqid:55834574848, ctx:store-message) from WG1:C7 (127.0.0.1:41962), but its data log sequencer's state is Recovery Incomplete
I0917 02:14:09.666884    1217 [ld:srv:WG3] Mutator.cpp:400] finalize() Mutator 4611686018427387903e2n1 completed (OK). Trace: W1{N2:S1,N4:S1,N0:S1};X:N2:S1:K;X:N4:S1:K;X:N0:S1:K;Y1:N4:S1:K;Y1:N2:S1:K;Y1:N0:S1:K;done:all:OK;
I0917 02:14:09.668091    1217 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387903, rqid:55834574848, ctx:store-message) from WG3:C6 (127.0.0.1:41964), but its data log sequencer's state is Recovery Incomplete
I0917 02:14:09.703066    1220 [ld:srv:WG4] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 4611686018427387903 has already been cleaned. No purge needed. last clean epoch: 1, purge to: 1.
I0917 02:14:09.707717    1220 [ld:srv:WG4] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387903, rqid:34359738369, ctx:store-message) from WG4:C4 (127.0.0.1:41914), but its data log sequencer's state is Recovery Incomplete
I0917 02:14:09.754815    1220 [ld:srv:WG4] PurgeUncleanEpochs.cpp:592] complete() Successfully purged epochs [1, 1] for log 13835058055282163711.
I0917 02:14:09.785856    1235 [ld:srv:WB1] ReplicatedStateMachine-inl.h:130] getSnapshot() [logsconfig] Fetching snapshot with ver:e0n1, (waiting_for_snapshot_:LSN_INVALID, version_:e0n1). sync_state_:0
I0917 02:14:09.806281    1212 [ld:srv:WG0] GET_SEQ_STATE_Message.cpp:238] getSequencer() Sequencer for log:4611686018427387900 is absent, state=UNAVAILABLE, cur_epoch=0, attempting to activate it upon receiving GSS (rqid:85899345921, ctx:sync-sequencer, min_epoch:none) from WG0:C10 (127.0.0.1:41984)
I0917 02:14:09.845719    1212 [ld:srv:WG0] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387900, rqid:85899345921, ctx:sync-sequencer) from WG0:C10 (127.0.0.1:41984), but its data log sequencer's state is ACTIVATING
W0917 02:14:09.849339    1212 [ld:srv:WG0] Worker.cpp:1000] onStoppedRunning() skipped at least 1 log entries
W0917 02:14:09.854829    1212 [ld:srv:WG0] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.050s, source: [Message sent/received: GET_SEQ_STATE]
I0917 02:14:09.856778    1212 [ld:srv:WG0] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 4611686018427387900 epoch 4.
I0917 02:14:09.857722    1212 [ld:srv:WG0] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 4611686018427387900: {[E:1 (at 2021-09-17 02:14:09.826) since:1 (at 2021-09-17 02:10:15.391) R:[node: 2] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]], Until:4}.
I0917 02:14:09.861271    1216 [ld:srv:WG2] PeriodicReleases.cpp:416] execute() Start broadcasting releases for log 4611686018427387900.
I0917 02:14:09.861553    1212 [ld:srv:WG0] AllSequencers.cpp:527] onEpochMetaDataFromEpochStore() Activated a sequencer for log 4611686018427387900 (reason: GET_SEQ_STATE) with epoch 4, metadata: [E:4 (at 2021-09-17 02:14:09.826) since:1 (at 2021-09-17 02:10:15.391) R:[node: 2] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]]. Activation Result: RECOVERY.
I0917 02:14:09.866572    1220 [ld:srv:WG4] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 13835058055282163708 epoch 4.
I0917 02:14:09.867512    1220 [ld:srv:WG4] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 13835058055282163708: {[E:1 (at -inf) since:1 (at -inf) R:[node: 3] V:2 flags: params:{target:0 seed:0 signature:0} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]], Until:4}.
I0917 02:14:09.868437    1220 [ld:srv:WG4] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 13835058055282163708 with next_epoch 4
I0917 02:14:09.877362    1220 [ld:srv:WG4] LogRecoveryRequest.cpp:1018] sealLog() Starting recovery of epochs [2, 3] of log 13835058055282163708
W0917 02:14:09.879349    1212 [ld:srv:WG0] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.013s, source: [Request: GET_TRIM_POINT]
I0917 02:14:09.889426    1212 [ld:srv:WG0] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 4611686018427387900 with next_epoch 4
I0917 02:14:09.892758    1237 [ld:srv:WB3] ReplicatedStateMachine-inl.h:712] onGotDeltaLogTailLSN() [eventlog] Tail lsn of delta log is e4n0
I0917 02:14:09.902799    1212 [ld:srv:WG0] LogRecoveryRequest.cpp:1018] sealLog() Starting recovery of epochs [2, 3] of log 4611686018427387900
I0917 02:14:09.957451    1223 [ld:srv:WF0] FailureDetector.cpp:682] processFlags() Received suspect-state-finished message from N3:1 with instance id:1631844849516, sent_time:1631844849916ms
I0917 02:14:09.959029    1223 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N3 changed from DEAD to STARTING
I0917 02:14:09.960269    1223 [ld:srv:WF0] FailureDetector.cpp:717] processFlags() N3 transitioned to STARTING as a result of receiving suspect-state-finished message, FD State:(gossip: 0, instance-id: 1631844849516, failover: 0, starting: 1)
I0917 02:14:09.960763    1220 [ld:srv:WG4] RebuildingSupervisor.cpp:256] onNodeStateChanged() Cancelling rebuilding trigger for node N3
W0917 02:14:09.962197    1220 [ld:srv:WG4] RebuildingSupervisor.cpp:734] adjustRebuildingThrottle() Exiting throttling mode: #draining=0, #rebuilding=0, #triggers=1, #adjusted cluster size=5, threshold=35%, actual=20%
I0917 02:14:09.963034    1220 [ld:srv:WG4] RebuildingSupervisor.cpp:751] adjustRebuildingThrottle() Not triggering new rebuildings until 2021-09-17 02:34:09.962: It's too soon after exiting throttling mode
I0917 02:14:09.964802    1223 [ld:srv:WF0] FailureDetector.cpp:1845] resetVersions() Resetting RSM and NCM Versions for N3
I0917 02:14:10.000935    1217 [ld:srv:WG3] EpochRecovery.cpp:1502] onDigestMayHaveBecomeComplete() Completing an authoritative partial digest for epoch 3 of log 4611686018427387903 because grace period with timeout of 100ms expired. State: {s:N1:S1,N3:S1 m:N0:S1,N2:S1,N4:S1 DIGEST}
I0917 02:14:10.003386    1217 [ld:srv:WG3] EpochRecovery.cpp:566] startMutations() Log 4611686018427387903 epoch 3 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_INCOMPLETE, mutation set: {N0:S1, N2:S1, N4:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387903 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387903 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {s:N1:S1,N3:S1 m:N0:S1,N2:S1,N4:S1 MUTATION}.
I0917 02:14:10.030291    1217 [ld:srv:WG3] Mutator.cpp:400] finalize() Mutator 4611686018427387903e3n1 completed (OK). Trace: W1{N2:S1,N0:S1,N4:S1};X:N2:S1:K;X:N0:S1:K;X:N4:S1:K;Y1:N4:S1:K;Y1:N2:S1:K;Y1:N0:S1:K;done:all:OK;
I0917 02:14:10.034922    1220 [ld:srv:WG4] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 4611686018427387903 has already been cleaned. No purge needed. last clean epoch: 2, purge to: 2.
I0917 02:14:10.049403    1215 [ld:srv:WG1] EpochRecovery.cpp:1502] onDigestMayHaveBecomeComplete() Completing an authoritative partial digest for epoch 3 of log 13835058055282163711 because grace period with timeout of 100ms expired. State: {s:N1:S1 m:N0:S1,N2:S1,N3:S1,N4:S1 DIGEST}
I0917 02:14:10.057305    1215 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 13835058055282163711 epoch 3 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_INCOMPLETE, mutation set: {N0:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163711 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163711 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {s:N1:S1 m:N0:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
I0917 02:14:10.060533    1220 [ld:srv:WG4] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163711 has already been cleaned. No purge needed. last clean epoch: 2, purge to: 2.
I0917 02:14:10.106761    1223 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N2 changed from UNHEALTHY to HEALTHY
I0917 02:14:10.108009    1223 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N3 changed from UNHEALTHY to UNDEFINED
I0917 02:14:10.108830    1223 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N4 changed from UNHEALTHY to HEALTHY
I0917 02:14:10.110360    1223 [ld:srv:WF0] FailureDetector.cpp:1541] onGossipMessageSent() Could not send gossip to WF0:N1:1 (127.0.0.1:48348): CONNFAILED: connection failed. Trying another node.
I0917 02:14:10.136606    1212 [ld:srv:WG0] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163707 has already been cleaned. No purge needed. last clean epoch: 1, purge to: 1.
I0917 02:14:10.148817    1212 [ld:srv:WG0] EpochRecovery.cpp:1502] onDigestMayHaveBecomeComplete() Completing an authoritative partial digest for epoch 2 of log 4611686018427387900 because grace period with timeout of 100ms expired. State: {s:N1:S0 m:N0:S0,N2:S0,N3:S0,N4:S0 DIGEST}
I0917 02:14:10.148898    1220 [ld:srv:WG4] EpochRecovery.cpp:566] startMutations() Log 13835058055282163708 epoch 2 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_INCOMPLETE, mutation set: {N0:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163708 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163708 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {s:N1:S0 m:N0:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:14:10.150444    1212 [ld:srv:WG0] EpochRecovery.cpp:566] startMutations() Log 4611686018427387900 epoch 2 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_INCOMPLETE, mutation set: {N0:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {s:N1:S0 m:N0:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:14:10.152389    1216 [ld:srv:WG2] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163708 has already been cleaned. No purge needed. last clean epoch: 1, purge to: 1.
I0917 02:14:10.165575    1212 [ld:srv:WG0] Mutator.cpp:400] finalize() Mutator 4611686018427387900e2n1 completed (OK). Trace: W1{N3:S0,N0:S0};X:N3:S0:K;X:N0:S0:K;Y1:N3:S0:K;Y1:N0:S0:K;done:can_replicate:OK;
I0917 02:14:10.168240    1212 [ld:srv:WG0] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 4611686018427387900 has already been cleaned. No purge needed. last clean epoch: 1, purge to: 1.
I0917 02:14:10.214508    1215 [ld:srv:WG1] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 4611686018427387899 has already been cleaned. No purge needed. last clean epoch: 1, purge to: 1.
I0917 02:14:10.216469    1223 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N3 changed from UNDEFINED to HEALTHY
I0917 02:14:10.283282    1217 [ld:srv:WG3] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 4611686018427387903, next_epoch:4
I0917 02:14:10.316002    1237 [ld:srv:WB3] ClientReadStream.cpp:4571] operator()() Requested epoch metadata for log 4611686018427387903 epoch 2 is not available. Highest epoch known is 1.
W0917 02:14:10.318460    1237 [ld:srv:WB3] ClientReadStream.cpp:2476] updateCurrentMetaData() Existing epoch metadata has changed for log 4611686018427387903 on epoch 1, check metadata log!
I0917 02:14:10.325162    1237 [ld:srv:WB3] ClientReadStream.cpp:650] sendStart() Cannot send START message to N1:S1 for log 13835058055282163711: DISABLED: server is marked down
I0917 02:14:10.340596    1220 [ld:srv:WG4] RecoveryNode.cpp:401] onDisconnect() Lost connection to N4:S0 while waiting for reply in state CLEANING during recovery of epoch 2 of log 13835058055282163708: CONNFAILED: connection failed
I0917 02:14:10.340886    1212 [ld:srv:WG0] CheckSealRequest.cpp:128] onSent() Sending a check-seal message to N4:S1 failed with err:PEER_CLOSED (log:4611686018427387903, rqid:68719476768, gss-rqid:8589934607)
I0917 02:14:10.363966    1212 [ld:srv:WG0] RecoveryNode.cpp:401] onDisconnect() Lost connection to N4:S0 while waiting for reply in state CLEANING during recovery of epoch 2 of log 4611686018427387900: PEER_CLOSED: connection closed by peer
I0917 02:14:10.341333    1215 [ld:srv:WG1] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 13835058055282163711, next_epoch:4
I0917 02:14:10.341515    1237 [ld:srv:WB3] ClientReadStream.cpp:4571] operator()() Requested epoch metadata for log 4611686018427387903 epoch 3 is not available. Highest epoch known is 1.
I0917 02:14:10.378912    1220 [ld:srv:WG4] RecoveryNode.cpp:401] onDisconnect() Lost connection to N3:S0 while waiting for reply in state CLEANING during recovery of epoch 2 of log 13835058055282163708: PEER_CLOSED: connection closed by peer
I0917 02:14:10.404487    1237 [ld:srv:WB3] debug.cpp:573] log() Slow log() call: it took 0.000s to prepare, 0.063s to write().
I0917 02:14:16.684403    2576 [logdeviced-main] main.cpp:399] main() Command line arguments: --local-log-store-path /data/store/N0:1/db --num-shards 2 --server-id 1hcw9tam4b --log-file /data/store/N0:1/log --name Node0 --config-path file:/data/store/logdevice.conf --client-thrift-api-port 41097 --gossip-port 41091 --server-thrift-api-port 41096 --test-mode true --server-to-server-port 41095 --admin-port 41092 --loglevel info --port 41089 --sequencers lazy 
I0917 02:14:16.686237    2576 [logdeviced-main] main.cpp:403] main() Plugins loaded: {"Thrift client factory plugin":"built-in","ZookeeperClient factory":"built-in","ConfigSource factory":["built-in"],"PermissionChecker factory":"built-in","Build Info":"built-in","Plugin provider":["Built-in plugin provider","Static plug-in loader","Dynamic plugin loader"]}
I0917 02:14:16.691224    2576 [logdeviced-main] main.cpp:405] main() server starting
I0917 02:14:16.695653    2576 [logdeviced-main] main.cpp:410] main() version 99.99.99
I0917 02:14:16.697831    2576 [logdeviced-main] main.cpp:418] main() asserts off (NDEBUG set)
I0917 02:14:16.709968    2576 [logdeviced-main] TextConfigUpdater.cpp:56] fetchFromSource() Requesting main config "/data/store/logdevice.conf" (source: file)
I0917 02:14:16.715187    2576 [logdeviced-main] TextConfigUpdater.cpp:116] onContents() Main config from file, hash = 99e4033bvf
I0917 02:14:17.598573    2576 [logdeviced-main] Server.cpp:185] operator()() Updating settings from config took 881ms
I0917 02:14:17.599952    2576 [logdeviced-main] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 882ms, config update: 0ms, notifications: 0ms
I0917 02:14:17.601511    2576 [logdeviced-main] TextConfigUpdater.cpp:306] pushServerConfig() Updated config (version: 1 - hash: 99e4033bvf)
I0917 02:14:17.602895    2576 [logdeviced-main] TextConfigUpdater.cpp:397] pushLogsConfig() Ignoring changes in the 'logs' section in the config file because LogsConfigManager is ENABLED
I0917 02:14:17.605007    2576 [logdeviced-main] TextConfigUpdater.cpp:174] update() Config update result: ServerConfig=updated, ZK=skipped, LogsConfig=updated
I0917 02:14:17.606919    2576 [logdeviced-main] FileBasedVersionedConfigStore.cpp:35] FileBasedVersionedConfigStore() FileBasedVersionedConfigStore threads started.
I0917 02:14:17.615583    2576 [logdeviced-main] NodesConfigurationInit.cpp:233] operator()() Fetching initial nodes configuration (Attempt 1)...
I0917 02:14:17.627393    2616 [ld:file-ncs] NodesConfigurationInit.cpp:290] operator()() Got a NodesConfiguration of version: 5
I0917 02:14:17.630091    2616 [ld:file-ncs] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 0ms, config update: 0ms, notifications: 0ms
I0917 02:14:17.636410    2576 [logdeviced-main] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 0ms, config update: 0ms, notifications: 0ms
I0917 02:14:17.639245    2576 [logdeviced-main] Server.cpp:510] init() My Node ID is N0:1
I0917 02:14:17.641333    2576 [logdeviced-main] Server.cpp:517] init() My version is 0
I0917 02:14:17.643629    2576 [logdeviced-main] NoopTraceLogger.cpp:22] NoopTraceLogger() NOOP TraceLogger is ON, no trace samples will be published.
I0917 02:14:17.648178    2576 [logdeviced-main] FileBasedVersionedConfigStore.cpp:46] stopAndJoin() FileBasedVersionedConfigStore threads stopped.
I0917 02:14:17.652471    2576 [logdeviced-main] main.cpp:144] set_fd_limit() Limit on number of file descriptors is 1048576
W0917 02:14:17.668970    2576 [logdeviced-main] main.cpp:192] drop_root() Running under root, but no --user command line option specified. Will keep running as root
I0917 02:14:17.695856    2576 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 41089
I0917 02:14:17.700337    2576 [logdeviced-main] Server.cpp:781] initListeners() Initializing a gossip listener.
I0917 02:14:17.704123    2576 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 41091
I0917 02:14:17.713030    2576 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 41095 (SSL)
I0917 02:14:17.715411    2576 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 41099 (SSL)
I0917 02:14:17.719659    2576 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 41098 (SSL)
I0917 02:14:17.776965    2576 [logdeviced-main] RocksDBLogStoreConfig.cpp:162] RocksDBLogStoreConfig() LD manages flushes and all memory budgets on the node. atomic_flush is set to true.
I0917 02:14:17.778808    2712 [io-stall:s1] IOTracing.cpp:83] stallDetectionThreadMain() Started IO stall detection thread for shard 1
I0917 02:14:17.782539    2711 [io-stall:s0] IOTracing.cpp:83] stallDetectionThreadMain() Started IO stall detection thread for shard 0
I0917 02:14:17.805960    2719 [ld:open-rocks0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:387] Created background thread for deletion scheduler with rate_bytes_per_sec: 100000000
I0917 02:14:17.811336    2720 [ld:open-rocks1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:387] Created background thread for deletion scheduler with rate_bytes_per_sec: 100000000
I0917 02:14:18.505850    2720 [ld:open-rocks1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000057.log.trash as trash -- OK
I0917 02:14:18.525355    2724 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 0after deleting file /data/store/N0:1/db/shard1/000057.log.trash
I0917 02:14:18.659498    2720 [ld:open-rocks1] PartitionedRocksDBStore.cpp:891] open() Partition s1:1000000 is under-replicated: Start:2021-09-17 02:10:14.858, Min:2021-09-17 02:10:04.858, Max: 2021-09-17 02:10:24.858
I0917 02:14:18.676042    2720 [ld:open-rocks1] PartitionedRocksDBStore.cpp:897] open() Shard[1] dirty ranges: [A:{{[2021-09-17 02:10:04.858,2021-09-17 02:10:24.858]}}]
I0917 02:14:18.704693    2719 [ld:open-rocks0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000057.log.trash as trash -- OK
I0917 02:14:18.718691    2720 [ld:open-rocks1] PartitionedRocksDBStore.cpp:1491] convertDataKeyFormat() DataKey format conversion took 0.012s. Read 9 records, 579 bytes. Wrote 0 records, 0 bytes.
I0917 02:14:18.721052    2725 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 0after deleting file /data/store/N0:1/db/shard0/000057.log.trash
I0917 02:14:18.880883    2719 [ld:open-rocks0] PartitionedRocksDBStore.cpp:891] open() Partition s0:1000000 is under-replicated: Start:2021-09-17 02:10:14.859, Min:2021-09-17 02:10:04.859, Max: 2021-09-17 02:10:24.859
I0917 02:14:18.883243    2719 [ld:open-rocks0] PartitionedRocksDBStore.cpp:897] open() Shard[0] dirty ranges: [A:{{[2021-09-17 02:10:04.859,2021-09-17 02:10:24.859]}}]
I0917 02:14:18.897371    2717 [rocks-high-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000063.log.trash as trash -- OK
I0917 02:14:18.900663    2719 [ld:open-rocks0] PartitionedRocksDBStore.cpp:1491] convertDataKeyFormat() DataKey format conversion took 0.001s. Read 6 records, 382 bytes. Wrote 0 records, 0 bytes.
I0917 02:14:18.933987    2720 [ld:open-rocks1] ShardedRocksDBLocalLogStore.cpp:232] operator()() Opened RocksDB instance at /data/store/N0:1/db/shard1 in 1118 ms
I0917 02:14:18.938826    2934 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 8ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:14:18.939594    2724 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 1after deleting file /data/store/N0:1/db/shard1/000063.log.trash
I0917 02:14:19.046241    2715 [rocks-low-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000067.sst.trash as trash -- OK
I0917 02:14:19.065846    2724 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 10after deleting file /data/store/N0:1/db/shard1/000067.sst.trash
I0917 02:14:19.089557    2715 [rocks-low-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000061.sst.trash as trash -- OK
I0917 02:14:19.101466    2724 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 10after deleting file /data/store/N0:1/db/shard1/000061.sst.trash
I0917 02:14:19.125985    2715 [rocks-low-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000058.sst.trash as trash -- OK
I0917 02:14:19.155611    2724 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 9after deleting file /data/store/N0:1/db/shard1/000058.sst.trash
I0917 02:14:19.157707    2718 [rocks-high-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000063.log.trash as trash -- OK
I0917 02:14:19.174564    2715 [rocks-low-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000056.sst.trash as trash -- OK
I0917 02:14:19.182868    2719 [ld:open-rocks0] ShardedRocksDBLocalLogStore.cpp:232] operator()() Opened RocksDB instance at /data/store/N0:1/db/shard0 in 1370 ms
I0917 02:14:19.186595    2724 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 10after deleting file /data/store/N0:1/db/shard1/000056.sst.trash
I0917 02:14:19.184330    2725 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 1after deleting file /data/store/N0:1/db/shard0/000063.log.trash
I0917 02:14:19.189453    2714 [rocks-low-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000067.sst.trash as trash -- OK
I0917 02:14:19.193205    2715 [rocks-low-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000049.sst.trash as trash -- OK
I0917 02:14:19.212826    2576 [logdeviced-main] ShardedRocksDBLocalLogStore.cpp:691] printDiskShardMapping() Disk -> Shard mapping: /data/store/N0:1/db/shard0 -> [0,1]
I0917 02:14:19.225676    2576 [logdeviced-main] ShardedRocksDBLocalLogStore.cpp:285] init() Initialized sharded RocksDB instance at /data/store/N0:1/db with 2 shards
E0917 02:14:19.230793    3129 [ld:populateLogS] ThreadID.cpp:26] set() Truncating thread name "ld:populateLogState1"!
I0917 02:14:19.242205    2715 [rocks-low-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000047.sst.trash as trash -- OK
E0917 02:14:19.232415    3128 [ld:populateLogS] ThreadID.cpp:26] set() Truncating thread name "ld:populateLogState0"!
I0917 02:14:19.240701    2714 [rocks-low-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000061.sst.trash as trash -- OK
I0917 02:14:19.246370    2725 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 10after deleting file /data/store/N0:1/db/shard0/000067.sst.trash
I0917 02:14:19.254381    2724 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 10after deleting file /data/store/N0:1/db/shard1/000049.sst.trash
I0917 02:14:19.264476    2576 [logdeviced-main] Server.cpp:1173] initLogStorageStateMap() Populating log storage state map successful.
I0917 02:14:19.278632    2725 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 10after deleting file /data/store/N0:1/db/shard0/000061.sst.trash
I0917 02:14:19.344664    2725 [not-set] debug.cpp:573] log() Slow log() call: it took 0.000s to prepare, 0.066s to write().
E0917 02:14:19.315683    2576 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:19.345976    2715 [rocks-low-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000041.sst.trash as trash -- OK
I0917 02:14:19.347480    2714 [rocks-low-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000058.sst.trash as trash -- OK
I0917 02:14:19.358381    2724 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 20after deleting file /data/store/N0:1/db/shard1/000047.sst.trash
I0917 02:14:19.361027    2576 [logdeviced-main] RecordCacheMonitorThread.cpp:27] RecordCacheMonitorThread() Record cache monitor thread started.
I0917 02:14:19.397270    2576 [logdeviced-main] ClusterState.cpp:356] resizeClusterState() Cluster state size updated from 0 to 5
E0917 02:14:19.406567    2576 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:19.373016    3187 [ld:cache-evict] RecordCacheMonitorThread.cpp:44] threadMain() Record cache eviction thread started. Size limit is set to 4294967296 bytes.
E0917 02:14:19.426700    2576 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:19.393857    2724 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 31after deleting file /data/store/N0:1/db/shard1/000041.sst.trash
I0917 02:14:19.447422    2724 [not-set] debug.cpp:573] log() Slow log() call: it took 0.000s to prepare, 0.053s to write().
I0917 02:14:19.398808    2725 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 9after deleting file /data/store/N0:1/db/shard0/000058.sst.trash
I0917 02:14:19.463773    2725 [not-set] debug.cpp:573] log() Slow log() call: it took 0.000s to prepare, 0.065s to write().
I0917 02:14:19.408102    2715 [rocks-low-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000038.sst.trash as trash -- OK
I0917 02:14:19.472028    2715 [rocks-low-0] debug.cpp:573] log() Slow log() call: it took 0.000s to prepare, 0.064s to write().
I0917 02:14:19.415105    2714 [rocks-low-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000056.sst.trash as trash -- OK
I0917 02:14:19.482792    2714 [rocks-low-1] debug.cpp:573] log() Slow log() call: it took 0.000s to prepare, 0.067s to write().
I0917 02:14:19.426783    3187 [ld:cache-evict] debug.cpp:573] log() Slow log() call: it took 0.000s to prepare, 0.054s to write().
I0917 02:14:19.514758    2715 [rocks-low-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000036.sst.trash as trash -- OK
E0917 02:14:19.456728    2576 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:19.491067    2724 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 9after deleting file /data/store/N0:1/db/shard1/000038.sst.trash
I0917 02:14:19.528076    2725 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 10after deleting file /data/store/N0:1/db/shard0/000056.sst.trash
I0917 02:14:19.515611    2714 [rocks-low-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000049.sst.trash as trash -- OK
I0917 02:14:19.523838    2576 [logdeviced-main] debug.cpp:573] log() Slow log() call: it took 0.000s to prepare, 0.067s to write().
I0917 02:14:19.541638    2724 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 10after deleting file /data/store/N0:1/db/shard1/000036.sst.trash
I0917 02:14:19.549256    2715 [rocks-low-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000029.sst.trash as trash -- OK
I0917 02:14:19.559287    2714 [rocks-low-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000047.sst.trash as trash -- OK
I0917 02:14:19.591866    2725 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 9after deleting file /data/store/N0:1/db/shard0/000049.sst.trash
E0917 02:14:19.571650    2576 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:19.571901    2724 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 13after deleting file /data/store/N0:1/db/shard1/000029.sst.trash
I0917 02:14:19.614897    2714 [rocks-low-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000041.sst.trash as trash -- OK
I0917 02:14:19.628443    2725 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 20after deleting file /data/store/N0:1/db/shard0/000047.sst.trash
E0917 02:14:19.630330    2576 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:19.654886    2576 [logdeviced-main] Processor.cpp:247] init() Initialized 5 workers of type GENERAL
I0917 02:14:19.669662    2714 [rocks-low-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000038.sst.trash as trash -- OK
I0917 02:14:19.689007    2725 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 30after deleting file /data/store/N0:1/db/shard0/000041.sst.trash
E0917 02:14:19.698307    2576 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:19.706400    2714 [rocks-low-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000036.sst.trash as trash -- OK
I0917 02:14:19.710168    2576 [logdeviced-main] Processor.cpp:247] init() Initialized 1 workers of type FAILURE_DETECTOR
I0917 02:14:19.714461    2725 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 40after deleting file /data/store/N0:1/db/shard0/000038.sst.trash
I0917 02:14:19.734483    2725 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 50after deleting file /data/store/N0:1/db/shard0/000036.sst.trash
I0917 02:14:19.742524    2714 [rocks-low-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000029.sst.trash as trash -- OK
E0917 02:14:19.735527    2576 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:19.756389    2725 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 12after deleting file /data/store/N0:1/db/shard0/000029.sst.trash
E0917 02:14:19.760468    2576 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:14:19.820493    2576 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:14:19.858813    2576 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:19.876566    2576 [logdeviced-main] Processor.cpp:247] init() Initialized 4 workers of type BACKGROUND
I0917 02:14:19.887878    2576 [logdeviced-main] FailureDetector.cpp:139] FailureDetector() Failure Detector created with instance id: 1631844859887
I0917 02:14:19.891866    2576 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N0 changed from UNKNOWN to DEAD
I0917 02:14:19.895715    2576 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N0 changed from HEALTHY to UNHEALTHY
I0917 02:14:19.912939    2576 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N1 changed from UNKNOWN to DEAD
I0917 02:14:19.919125    2576 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N1 changed from HEALTHY to UNHEALTHY
I0917 02:14:19.925866    2576 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N2 changed from UNKNOWN to DEAD
I0917 02:14:19.931847    2576 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N2 changed from HEALTHY to UNHEALTHY
I0917 02:14:19.938987    2576 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N3 changed from UNKNOWN to DEAD
I0917 02:14:19.964334    2576 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N3 changed from HEALTHY to UNHEALTHY
I0917 02:14:19.971666    2576 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N4 changed from UNKNOWN to DEAD
I0917 02:14:19.983828    2576 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N4 changed from HEALTHY to UNHEALTHY
W0917 02:14:19.994343    3220 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 106 msec: BUFFERED_WRITER_CREATE_SHARD (id: 4294967297), p :HI_PRI
W0917 02:14:20.018426    3251 [ld:srv:WG4] Worker.cpp:1322] processRequest() Request queued for 130 msec: BUFFERED_WRITER_CREATE_SHARD (id: 4294967300), p :HI_PRI
W0917 02:14:20.044387    3219 [ld:srv:WG0] Worker.cpp:1322] processRequest() Request queued for 149 msec: NODE_STATE_UPDATED (id: 4294967301), p :LO_PRI
W0917 02:14:20.058535    3219 [ld:srv:WG0] Worker.cpp:1322] processRequest() Request queued for 163 msec: NODE_STATE_UPDATED (id: 4294967306), p :LO_PRI
W0917 02:14:20.063842    3219 [ld:srv:WG0] Worker.cpp:1322] processRequest() Request queued for 168 msec: NODE_STATE_UPDATED (id: 4294967307), p :LO_PRI
W0917 02:14:20.069357    3219 [ld:srv:WG0] Worker.cpp:1322] processRequest() Request queued for 150 msec: NODE_STATE_UPDATED (id: 4294967311), p :LO_PRI
W0917 02:14:20.073962    3219 [ld:srv:WG0] Worker.cpp:1322] processRequest() Request queued for 154 msec: NODE_STATE_UPDATED (id: 4294967316), p :LO_PRI
W0917 02:14:20.083846    3219 [ld:srv:WG0] Worker.cpp:1322] processRequest() Request queued for 164 msec: NODE_STATE_UPDATED (id: 4294967317), p :LO_PRI
W0917 02:14:20.087771    3219 [ld:srv:WG0] Worker.cpp:1322] processRequest() Request queued for 156 msec: NODE_STATE_UPDATED (id: 4294967321), p :LO_PRI
I0917 02:14:20.088308    2576 [logdeviced-main] FileBasedVersionedConfigStore.cpp:35] FileBasedVersionedConfigStore() FileBasedVersionedConfigStore threads started.
W0917 02:14:20.098609    3219 [ld:srv:WG0] Worker.cpp:1322] processRequest() Request queued for 166 msec: NODE_STATE_UPDATED (id: 4294967326), p :LO_PRI
I0917 02:14:20.106145    3274 [ld:srv:WB0] AllSequencers.cpp:981] noteConfigurationChanged() Acquiring lock for sequencer map took 0ms
I0917 02:14:20.135001    3274 [ld:srv:WB0] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 0ms, config update: 0ms, notifications: 30ms
I0917 02:14:20.122951    3002 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
W0917 02:14:20.139721    3274 [ld:srv:WB0] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.036s, source: [Request: NODES_CONFIGURATION_MANAGER]
I0917 02:14:20.149375    3002 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:14:20.155459    3274 [ld:srv:WB0] NodesConfigurationManager.cpp:507] onProcessingFinished() Updated local nodes config to version 5...
I0917 02:14:20.158162    3168 [ld:s0:default:1] PartitionedRocksDBStore.cpp:3527] readAllLogSnapshotBlobs() Snapshots column family does not exist
I0917 02:14:20.169978    3168 [ld:s0:default:1] RecordCacheRepopulationTask.cpp:103] execute() Repopulated record caches for 0 logs on shard 0, totaling 0 bytes.
I0917 02:14:20.169813    3178 [ld:s1:default:1] PartitionedRocksDBStore.cpp:3527] readAllLogSnapshotBlobs() Snapshots column family does not exist
I0917 02:14:20.191286    3178 [ld:s1:default:1] RecordCacheRepopulationTask.cpp:103] execute() Repopulated record caches for 0 logs on shard 1, totaling 0 bytes.
I0917 02:14:20.198491    2576 [logdeviced-main] Server.cpp:1319] initSequencers() Initializing FileEpochStore
I0917 02:14:20.201422    2576 [logdeviced-main] Server.cpp:1398] initSequencerPlacement() using SequencerOptions::LAZY
I0917 02:14:20.206574    2576 [logdeviced-main] Server.cpp:1430] initRebuildingCoordinator() Initializing EventLog RSM and RebuildingCoordinator
I0917 02:14:20.224447    2576 [logdeviced-main] RsmServerSnapshotStoreFactory.h:25] create() Attempting to create snapshot store (type:1, key:4611686018427387903)
I0917 02:14:20.228912    2576 [logdeviced-main] RsmSnapshotStoreFactory.h:31] create() Attempting to create snapshot store (type:1, key:4611686018427387903, is_server:1)
I0917 02:14:20.242784    2576 [logdeviced-main] RsmSnapshotStoreFactory.h:58] create() Creating LogBasedRSMSnapshotStore
I0917 02:14:20.246483    2576 [logdeviced-main] Server.cpp:1467] initRebuildingCoordinator() Starting RebuildingSupervisor
I0917 02:14:20.248855    2576 [logdeviced-main] Server.cpp:1480] initRebuildingCoordinator() Starting RebuildingCoordinator
I0917 02:14:20.248868    3251 [ld:srv:WG4] RebuildingSupervisor.cpp:62] init() Starting Rebuilding Supervisor
I0917 02:14:20.257568    3251 [ld:srv:WG4] RebuildingSupervisor.cpp:222] addForRebuilding() Scheduling rebuilding of N4 to execute in 1200s
I0917 02:14:20.272006    3333 [ld:srv:WB3] RebuildingCoordinator.cpp:309] subscribeToEventLogIfNeeded() RebuildingCoordinator did not start yet because LogsConfig is not fully loaded yet
I0917 02:14:20.281303    3333 [ld:srv:WB3] ReplicatedStateMachine-inl.h:667] onBaseSnapshotRetrieved() [eventlog] Base snapshot has version:e0n1, delta_log_read_ptr:LSN_INVALID
I0917 02:14:20.272124    2576 [logdeviced-main] LogStoreMonitor.cpp:190] start() Logstore monitoring thread started.
I0917 02:14:20.286863    2576 [logdeviced-main] UnreleasedRecordDetector.cpp:106] start() Unreleased record detector thread started.
I0917 02:14:20.274754    3251 [ld:srv:WG4] RebuildingSupervisor.cpp:222] addForRebuilding() Scheduling rebuilding of N3 to execute in 1200s
I0917 02:14:20.283605    3333 [ld:srv:WB3] EventLogStateMachine.cpp:146] gotInitialState() Got base rebuilding set: {}
I0917 02:14:20.303884    3333 [ld:srv:WB3] ReplicatedStateMachine-inl.h:684] getDeltaLogTailLSN() [eventlog] Retrieving tail lsn of delta log...
I0917 02:14:20.290242    2576 [logdeviced-main] RsmServerSnapshotStoreFactory.h:25] create() Attempting to create snapshot store (type:1, key:4611686018427387901)
I0917 02:14:20.320753    2576 [logdeviced-main] RsmSnapshotStoreFactory.h:31] create() Attempting to create snapshot store (type:1, key:4611686018427387901, is_server:1)
I0917 02:14:20.296325    3251 [ld:srv:WG4] RebuildingSupervisor.cpp:222] addForRebuilding() Scheduling rebuilding of N2 to execute in 1200s
I0917 02:14:20.341237    3251 [ld:srv:WG4] RebuildingSupervisor.cpp:222] addForRebuilding() Scheduling rebuilding of N1 to execute in 1200s
W0917 02:14:20.309361    3333 [ld:srv:WB3] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.028s, source: [Request: START_EVENT_LOG_READER]
E0917 02:14:20.309578    3243 [ld:srv:WG3] HashBasedSequencerLocator.cpp:208] locateSequencer() No available sequencer node for log 4611686018427387903. All sequencer nodes are unavailable.
I0917 02:14:20.365247    3243 [ld:srv:WG3] debug.cpp:573] log() Slow log() call: it took 0.000s to prepare, 0.056s to write().
I0917 02:14:20.329248    2576 [logdeviced-main] RsmSnapshotStoreFactory.h:58] create() Creating LogBasedRSMSnapshotStore
I0917 02:14:20.373011    3279 [ld:srv:WB1] LogsConfigManager.cpp:38] execute() Starting LogsConfigManager on Worker 1 in pool: BACKGROUND
W0917 02:14:20.352959    3251 [ld:srv:WG4] RebuildingSupervisor.cpp:705] adjustRebuildingThrottle() Entering throttling mode: #draining=0, #rebuilding=0, #triggers=4, #adjusted cluster size=5, threshold=35%, actual=80%
I0917 02:14:20.381067    3251 [ld:srv:WG4] RebuildingSupervisor.cpp:718] adjustRebuildingThrottle() Entering throttling mode: #draining=0, #rebuilding=0, #triggers=4, #adjusted cluster size=5, threshold=35%, actual=80%
I0917 02:14:20.358768    3333 [ld:srv:WB3] ReplicatedStateMachine-inl.h:667] onBaseSnapshotRetrieved() [maintenancelog] Base snapshot has version:e0n1, delta_log_read_ptr:LSN_INVALID
I0917 02:14:20.405826    3333 [ld:srv:WB3] ReplicatedStateMachine-inl.h:684] getDeltaLogTailLSN() [maintenancelog] Retrieving tail lsn of delta log...
E0917 02:14:20.368841    3243 [ld:srv:WG3] SequencerRouter.cpp:56] operator()() Error during sequencer lookup for log 4611686018427387903 (NOSEQUENCER), reporting NOSEQUENCER.
I0917 02:14:20.377336    3279 [ld:srv:WB1] LogsConfigManager.cpp:165] start() Starting LogsConfig Manager RSM
I0917 02:14:20.425475    3279 [ld:srv:WB1] LogsConfigManager.cpp:206] start() Starting LogsConfig Replicated State Machine
E0917 02:14:20.413845    3251 [ld:srv:WG4] HashBasedSequencerLocator.cpp:208] locateSequencer() No available sequencer node for log 4611686018427387899. All sequencer nodes are unavailable.
W0917 02:14:20.440517    3251 [ld:srv:WG4] GetSeqStateRequest.cpp:512] onSequencerRoutingFailure() Unable to find a sequencer node for log 4611686018427387899, rqid:25769803779, ctx:rsm, err(NOSEQUENCER)
E0917 02:14:20.418354    3243 [ld:srv:WG3] SequencerRouter.cpp:105] onFailure() Error during sequencer routing for log 4611686018427387903 (NOSEQUENCER), request[type:GET_SEQ_STATE_REQ_TYPE, handler:0x7f05361ab028].
W0917 02:14:20.447361    3243 [ld:srv:WG3] GetSeqStateRequest.cpp:512] onSequencerRoutingFailure() Unable to find a sequencer node for log 4611686018427387903, rqid:25769803777, ctx:rsm, err(NOSEQUENCER)
I0917 02:14:20.429234    3279 [ld:srv:WB1] ReplicatedStateMachine-inl.h:38] startFetchingSnapshot() [logsconfig] Using RSMSnapshotStore...
W0917 02:14:20.442993    3251 [ld:srv:WG4] GetSeqStateRequest.cpp:700] finalize() GET_SEQ_STATE for log 4611686018427387899 (rqid:25769803779, ctx:rsm) returned NOSEQUENCER: no sequencer was found for log
E0917 02:14:20.452650    3333 [ld:srv:WB3] SyncSequencerRequest.cpp:178] onGotSeqState() Got sequencer state (id:25769803777) for log:4611686018427387903, status:NOSEQUENCER: no sequencer was found for log,sequencer:[invalid NodeID], last_released_lsn:LSN_INVALID, next_lsn:LSN_INVALID
I0917 02:14:20.457413    3279 [ld:srv:WB1] ReplicatedStateMachine-inl.h:1294] initSnapshotFetchTimer() [logsconfig] Creating timer to fetch snapshots ...
E0917 02:14:20.471114    3333 [ld:srv:WB3] SyncSequencerRequest.cpp:178] onGotSeqState() Got sequencer state (id:25769803779) for log:4611686018427387899, status:NOSEQUENCER: no sequencer was found for log,sequencer:[invalid NodeID], last_released_lsn:LSN_INVALID, next_lsn:LSN_INVALID
I0917 02:14:20.478094    2576 [logdeviced-main] LogDeviceThriftServer.cpp:44] LogDeviceThriftServer() Thrift server LogDevice Admin API Service will listen on port 41092
I0917 02:14:20.497896    2576 [logdeviced-main] Server.cpp:1540] createAndAttachMaintenanceManager() Not initializing MaintenanceManager since it is disabled in settings
I0917 02:14:20.502431    2576 [logdeviced-main] Server.cpp:905] initThriftServer() Initializing Thrift Server: s2s-api
I0917 02:14:20.519371    2576 [logdeviced-main] LogDeviceThriftServer.cpp:44] LogDeviceThriftServer() Thrift server s2s-api will listen on port 41096
I0917 02:14:20.526605    2576 [logdeviced-main] Server.cpp:905] initThriftServer() Initializing Thrift Server: c2s-api
I0917 02:14:20.530414    2576 [logdeviced-main] LogDeviceThriftServer.cpp:44] LogDeviceThriftServer() Thrift server c2s-api will listen on port 41097
I0917 02:14:20.542301    2576 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:14:20.549928    2663 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:14:20.561762    2576 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:14:20.573590    2688 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:14:20.580198    2576 [logdeviced-main] SimpleThriftServer.cpp:28] start() Starting a listener thread for Thrift server s2s-api
I0917 02:14:20.584879    3260 [ld:srv:WF0] FailureDetector.cpp:228] startGetClusterState() Sending GET_CLUSTER_STATE to build initial FD cluster view
I0917 02:14:20.598631    3260 [ld:srv:WF0] ProtocolHandler.cpp:127] notifyErrorOnSocket() Socket N2:1(127.0.0.1:41069) hit error AsyncSocketException: connect failed, type = Socket not open, errno = 111 (Connection refused)
I0917 02:14:20.723287    3260 [ld:srv:WF0] FailureDetector.cpp:267] operator()() Cluster state received with 5 dead nodes (N0,N1,N2,N3,N4) and 0 boycotted nodes ()
I0917 02:14:20.731054    3260 [ld:srv:WF0] FailureDetector.cpp:288] buildInitialState() Wait over (cluster state received)
I0917 02:14:20.749256    3260 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N1 changed from DEAD to STARTING
I0917 02:14:20.733046    2576 [logdeviced-main] SimpleThriftServer.cpp:28] start() Starting a listener thread for Thrift server c2s-api
I0917 02:14:20.755407    3260 [ld:srv:WF0] FailureDetector.cpp:316] buildInitialState() N1 transitioned to ALIVE
I0917 02:14:20.755697    3251 [ld:srv:WG4] RebuildingSupervisor.cpp:256] onNodeStateChanged() Cancelling rebuilding trigger for node N1
I0917 02:14:20.765680    3260 [ld:srv:WF0] FailureDetector.cpp:316] buildInitialState() N2 transitioned to DEAD
I0917 02:14:20.774146    3260 [ld:srv:WF0] FailureDetector.cpp:316] buildInitialState() N3 transitioned to DEAD
I0917 02:14:20.777604    3260 [ld:srv:WF0] FailureDetector.cpp:316] buildInitialState() N4 transitioned to DEAD
I0917 02:14:20.781738    3260 [ld:srv:WF0] FailureDetector.cpp:334] buildInitialState() N1 transitioned to UNHEALTHY (status)
I0917 02:14:20.784692    3260 [ld:srv:WF0] FailureDetector.cpp:334] buildInitialState() N2 transitioned to UNHEALTHY (status)
I0917 02:14:20.790959    3260 [ld:srv:WF0] FailureDetector.cpp:334] buildInitialState() N3 transitioned to UNHEALTHY (status)
I0917 02:14:20.794982    3260 [ld:srv:WF0] FailureDetector.cpp:334] buildInitialState() N4 transitioned to UNHEALTHY (status)
I0917 02:14:20.798351    3260 [ld:srv:WF0] FailureDetector.cpp:344] buildInitialState() Initializing DomainIsolationChecker
I0917 02:14:20.834226    3260 [ld:srv:WF0] FailureDetector.cpp:915] startSuspectTimer() Starting suspect state timer
I0917 02:14:20.844583    3260 [ld:srv:WF0] FailureDetector.cpp:1285] updateNodeState() N0 transitioned from DEAD to ALIVE, FD State:(gossip: 0, instance-id: 0, failover: 0, starting: 1)
I0917 02:14:20.848940    3260 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N0 changed from DEAD to STARTING
I0917 02:14:20.860090    3260 [ld:srv:WF0] FailureDetector.cpp:1456] broadcastBringupUpdate() Broadcasting suspect-state-finished message.
I0917 02:14:20.868045    3260 [ld:srv:WF0] FailureDetector.cpp:1476] broadcastBringupUpdate() Sending suspect-state-finished message with instance id:1631844859887 to node WF0:N4:1 (UNKNOWN)
I0917 02:14:20.850465    3167 [ld:s0:default:0] RecordCache.cpp:257] getEpochRecordCache() RecordCache miss for log 4611686018427387900 epoch 2: cache not found but cannot declare no data. head epoch : 0, last_nonauthoritative_epoch: 2.
I0917 02:14:20.954867    3260 [ld:srv:WF0] debug.cpp:573] log() Slow log() call: it took 0.000s to prepare, 0.087s to write().
I0917 02:14:20.968481    3167 [ld:s0:default:0] debug.cpp:573] log() Slow log() call: it took 0.000s to prepare, 0.118s to write().
I0917 02:14:20.986869    3260 [ld:srv:WF0] FailureDetector.cpp:1476] broadcastBringupUpdate() Sending suspect-state-finished message with instance id:1631844859887 to node WF0:N3:1 (UNKNOWN)
I0917 02:14:21.024743    3260 [ld:srv:WF0] FailureDetector.cpp:1456] broadcastBringupUpdate() Broadcasting bringup message.
I0917 02:14:21.036586    3260 [ld:srv:WF0] FailureDetector.cpp:937] startGossiping() Start Gossiping.
I0917 02:14:21.051521    3260 [ld:srv:WF0] FailureDetector.cpp:925] operator()() Suspect timeout expired
W0917 02:14:21.062748    3260 [ld:srv:WF0] Worker.cpp:1000] onStoppedRunning() skipped at least 10 log entries
W0917 02:14:21.079500    3260 [ld:srv:WF0] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.011s, source: [Message sent/received: GET_CLUSTER_STATE_REPLY]
I0917 02:14:21.066918    2576 [logdeviced-main] SimpleThriftServer.cpp:28] start() Starting a listener thread for Thrift server LogDevice Admin API Service
I0917 02:14:21.155986    3260 [ld:srv:WF0] FailureDetector.cpp:1541] onGossipMessageSent() Could not send gossip to WF0:N2:1 (127.0.0.1:41069): CONNFAILED: connection failed. Trying another node.
W0917 02:14:21.193499    3260 [ld:srv:WF0] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.038s, source: [Message sent/received: GOSSIP]
I0917 02:14:21.204169    2576 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:14:21.209815    2689 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:14:21.217063    2576 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:14:21.220981    2663 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
W0917 02:14:21.222285    3260 [ld:srv:WF0] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.020s, source: [Message sent/received: GOSSIP]
I0917 02:14:21.232704    2576 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:14:21.242009    2663 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:14:21.259925    2576 [logdeviced-main] main.cpp:464] main() Listeners initialized
I0917 02:14:21.486991    3279 [ld:srv:WB1] ReplicatedStateMachine-inl.h:130] getSnapshot() [logsconfig] Fetching snapshot with ver:e0n1, (waiting_for_snapshot_:LSN_INVALID, version_:e0n1). sync_state_:0
I0917 02:14:21.497921    3260 [ld:srv:WF0] FailureDetector.cpp:682] processFlags() Received suspect-state-finished message from N4:1 with instance id:1631844860532, sent_time:1631844861480ms
I0917 02:14:21.502711    3260 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N4 changed from DEAD to STARTING
I0917 02:14:21.506816    3260 [ld:srv:WF0] FailureDetector.cpp:717] processFlags() N4 transitioned to STARTING as a result of receiving suspect-state-finished message, FD State:(gossip: 0, instance-id: 1631844860532, failover: 0, starting: 1)
I0917 02:14:21.511021    3260 [ld:srv:WF0] FailureDetector.cpp:682] processFlags() Received bringup message from N4:1 with instance id:1631844860532, sent_time:1631844861486ms
I0917 02:14:21.514005    3260 [ld:srv:WF0] FailureDetector.cpp:1845] resetVersions() Resetting RSM and NCM Versions for N4
I0917 02:14:21.502048    3219 [ld:srv:WG0] GET_SEQ_STATE_Message.cpp:238] getSequencer() Sequencer for log:4611686018427387900 is absent, state=UNAVAILABLE, cur_epoch=0, attempting to activate it upon receiving GSS (rqid:68719476737, ctx:sync-sequencer, min_epoch:none) from WG0:C10 (127.0.0.1:46636)
W0917 02:14:21.507325    3251 [ld:srv:WG4] GET_SEQ_STATE_Message.cpp:365] getSequencer() sequencer activation for log:4611686018427387903 from WG4:C9 (127.0.0.1:46634), failed with err ISOLATED
I0917 02:14:21.560879    3251 [ld:srv:WG4] RebuildingSupervisor.cpp:256] onNodeStateChanged() Cancelling rebuilding trigger for node N4
I0917 02:14:21.566423    3251 [ld:srv:WG4] RebuildingSupervisor.cpp:718] adjustRebuildingThrottle() skipped at least 1 log entries
I0917 02:14:21.568898    3251 [ld:srv:WG4] RebuildingSupervisor.cpp:718] adjustRebuildingThrottle() Entering throttling mode: #draining=0, #rebuilding=0, #triggers=2, #adjusted cluster size=5, threshold=35%, actual=40%
W0917 02:14:21.582543    3243 [ld:srv:WG3] GetSeqStateRequest.cpp:700] finalize() GET_SEQ_STATE for log 4611686018427387903 (rqid:25769803780, ctx:rsm) returned FAILED: request failed
E0917 02:14:21.588786    3333 [ld:srv:WB3] SyncSequencerRequest.cpp:178] onGotSeqState() Got sequencer state (id:25769803780) for log:4611686018427387903, status:FAILED: request failed,sequencer:N0:1, last_released_lsn:LSN_INVALID, next_lsn:LSN_INVALID
I0917 02:14:21.596409    3333 [ld:srv:WB3] ReplicatedStateMachine-inl.h:712] onGotDeltaLogTailLSN() [maintenancelog] Tail lsn of delta log is e6n0
I0917 02:14:21.616832    3219 [ld:srv:WG0] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387900, rqid:68719476737, ctx:sync-sequencer) from WG0:C10 (127.0.0.1:46636), but its data log sequencer's state is ACTIVATING
I0917 02:14:21.628427    3219 [ld:srv:WG0] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 4611686018427387900 epoch 7.
I0917 02:14:21.632006    3219 [ld:srv:WG0] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 4611686018427387900: {[E:1 (at 2021-09-17 02:14:21.564) since:1 (at 2021-09-17 02:10:15.391) R:[node: 2] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]], Until:7}.
I0917 02:14:21.635537    3219 [ld:srv:WG0] AllSequencers.cpp:527] onEpochMetaDataFromEpochStore() Activated a sequencer for log 4611686018427387900 (reason: GET_SEQ_STATE) with epoch 7, metadata: [E:7 (at 2021-09-17 02:14:21.564) since:1 (at 2021-09-17 02:10:15.391) R:[node: 2] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]]. Activation Result: RECOVERY.
I0917 02:14:21.635753    3222 [ld:srv:WG2] PeriodicReleases.cpp:416] execute() Start broadcasting releases for log 4611686018427387900.
I0917 02:14:21.644784    3219 [ld:srv:WG0] MetaDataLogTrimmer.cpp:115] schedulePeriodicTrimming() Will run periodic trimming every ~7200000 s for 4611686018427387900
I0917 02:14:21.648967    3219 [ld:srv:WG0] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 4611686018427387900 with next_epoch 7
I0917 02:14:21.731628    3219 [ld:srv:WG0] debug.cpp:573] log() Slow log() call: it took 0.000s to prepare, 0.083s to write().
I0917 02:14:21.679893    3251 [ld:srv:WG4] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 13835058055282163708 epoch 7.
I0917 02:14:21.743673    3251 [ld:srv:WG4] debug.cpp:573] log() Slow log() call: it took 0.000s to prepare, 0.064s to write().
I0917 02:14:21.731504    3222 [ld:srv:WG2] GET_SEQ_STATE_Message.cpp:238] getSequencer() Sequencer for log:4611686018427387903 is absent, state=UNAVAILABLE, cur_epoch=0, attempting to activate it upon receiving GSS (rqid:55834574864, ctx:store-message, min_epoch:none) from WG2:C6 (127.0.0.1:46532)
W0917 02:14:21.750894    3243 [ld:srv:WG3] Worker.cpp:1000] onStoppedRunning() skipped at least 10 log entries
W0917 02:14:21.753498    3243 [ld:srv:WG3] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.056s, source: [Request: MISC]
I0917 02:14:21.762518    3251 [ld:srv:WG4] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 13835058055282163708: {[E:1 (at -inf) since:1 (at -inf) R:[node: 3] V:2 flags: params:{target:0 seed:0 signature:0} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]], Until:7}.
W0917 02:14:21.779839    3251 [ld:srv:WG4] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.100s, source: [Request: START_METADATA_LOG_RECOVERY]
I0917 02:14:21.768857    3219 [ld:srv:WG0] LogRecoveryRequest.cpp:1018] sealLog() Starting recovery of epochs [2, 6] of log 4611686018427387900
I0917 02:14:21.787136    3219 [ld:srv:WG0] ProtocolHandler.cpp:127] notifyErrorOnSocket() skipped at least 10 log entries
I0917 02:14:21.786719    3222 [ld:srv:WG2] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387903, rqid:55834574864, ctx:store-message) from WG2:C6 (127.0.0.1:46532), but its data log sequencer's state is ACTIVATING
I0917 02:14:21.795662    3222 [ld:srv:WG2] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 4611686018427387903 epoch 7.
I0917 02:14:21.787541    3251 [ld:srv:WG4] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 13835058055282163708 with next_epoch 7
I0917 02:14:21.789085    3219 [ld:srv:WG0] ProtocolHandler.cpp:127] notifyErrorOnSocket() Socket N3:1(127.0.0.1:41056) hit error AsyncSocketException: connect failed, type = Socket not open, errno = 111 (Connection refused)
I0917 02:14:21.804348    3222 [ld:srv:WG2] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 4611686018427387903: {[E:1 (at 2021-09-17 02:14:21.753) since:1 (at 2021-09-17 02:10:15.661) R:[node: 3] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]], Until:7}.
I0917 02:14:21.819455    3222 [ld:srv:WG2] AllSequencers.cpp:527] onEpochMetaDataFromEpochStore() Activated a sequencer for log 4611686018427387903 (reason: GET_SEQ_STATE) with epoch 7, metadata: [E:7 (at 2021-09-17 02:14:21.753) since:1 (at 2021-09-17 02:10:15.661) R:[node: 3] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]]. Activation Result: RECOVERY.
I0917 02:14:21.827054    3220 [ld:srv:WG1] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 13835058055282163711 epoch 7.
I0917 02:14:21.829556    3220 [ld:srv:WG1] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 13835058055282163711: {[E:1 (at -inf) since:1 (at -inf) R:[node: 3] V:2 flags: params:{target:0 seed:0 signature:0} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]], Until:7}.
I0917 02:14:21.831506    3220 [ld:srv:WG1] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 13835058055282163711 with next_epoch 7
I0917 02:14:21.840741    3243 [ld:srv:WG3] PeriodicReleases.cpp:416] execute() Start broadcasting releases for log 4611686018427387903.
I0917 02:14:21.857466    3243 [ld:srv:WG3] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 4611686018427387903 with next_epoch 7
I0917 02:14:21.842315    3251 [ld:srv:WG4] LogRecoveryRequest.cpp:1018] sealLog() Starting recovery of epochs [2, 6] of log 13835058055282163708
I0917 02:14:21.861725    3167 [ld:s0:default:0] RecordCache.cpp:257] getEpochRecordCache() RecordCache miss for log 4611686018427387900 epoch 2: cache not found but cannot declare no data. head epoch : 0, last_nonauthoritative_epoch: 2.
I0917 02:14:21.862753    3260 [ld:srv:WF0] FailureDetector.cpp:682] processFlags() Received suspect-state-finished message from N2:1 with instance id:1631844860573, sent_time:1631844861832ms
I0917 02:14:21.884027    3260 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N2 changed from DEAD to STARTING
I0917 02:14:21.863860    3220 [ld:srv:WG1] LogRecoveryRequest.cpp:1018] sealLog() Starting recovery of epochs [4, 6] of log 13835058055282163711
I0917 02:14:21.891349    3251 [ld:srv:WG4] RebuildingSupervisor.cpp:256] onNodeStateChanged() Cancelling rebuilding trigger for node N2
I0917 02:14:21.888390    3260 [ld:srv:WF0] FailureDetector.cpp:717] processFlags() N2 transitioned to STARTING as a result of receiving suspect-state-finished message, FD State:(gossip: 0, instance-id: 1631844860573, failover: 0, starting: 1)
I0917 02:14:21.906744    3260 [ld:srv:WF0] FailureDetector.cpp:682] processFlags() Received bringup message from N2:1 with instance id:1631844860573, sent_time:1631844861837ms
I0917 02:14:21.898539    3333 [ld:srv:WB3] ClientReadStream.cpp:650] sendStart() Cannot send START message to N3:S1 for log 4611686018427387899: DISABLED: server is marked down
W0917 02:14:21.904454    3251 [ld:srv:WG4] RebuildingSupervisor.cpp:734] adjustRebuildingThrottle() Exiting throttling mode: #draining=0, #rebuilding=0, #triggers=1, #adjusted cluster size=5, threshold=35%, actual=20%
I0917 02:14:21.905876    3222 [ld:srv:WG2] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 4611686018427387903 has already been cleaned. No purge needed. last clean epoch: 3, purge to: 3.
I0917 02:14:21.906546    3243 [ld:srv:WG3] LogRecoveryRequest.cpp:1018] sealLog() Starting recovery of epochs [4, 6] of log 4611686018427387903
I0917 02:14:21.908888    3260 [ld:srv:WF0] FailureDetector.cpp:1845] resetVersions() Resetting RSM and NCM Versions for N2
I0917 02:14:21.913609    3251 [ld:srv:WG4] RebuildingSupervisor.cpp:751] adjustRebuildingThrottle() Not triggering new rebuildings until 2021-09-17 02:34:21.913: It's too soon after exiting throttling mode
I0917 02:14:21.965035    3222 [ld:srv:WG2] CheckSealRequest.cpp:128] onSent() Sending a check-seal message to N4:S1 failed with err:CONNFAILED (log:4611686018427387903, rqid:81604378639, gss-rqid:25769803781)
I0917 02:14:21.974098    3243 [ld:srv:WG3] CheckSealRequest.cpp:128] onSent() Sending a check-seal message to N4:S1 failed with err:CONNFAILED (log:4611686018427387903, rqid:30064771075, gss-rqid:38654705665)
I0917 02:14:22.021752    3222 [ld:srv:WG2] PurgeUncleanEpochs.cpp:592] complete() Successfully purged epochs [3, 3] for log 4611686018427387903.
I0917 02:14:22.112052    3251 [ld:srv:WG4] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163707 has already been cleaned. No purge needed. last clean epoch: 1, purge to: 1.
I0917 02:14:22.155682    3220 [ld:srv:WG1] EpochRecovery.cpp:1502] onDigestMayHaveBecomeComplete() Completing an authoritative partial digest for epoch 4 of log 13835058055282163711 because grace period with timeout of 100ms expired. State: {s:N3:S1,N4:S1 m:N0:S1,N1:S1,N2:S1 DIGEST}
I0917 02:14:22.157917    3220 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 13835058055282163711 epoch 4 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_INCOMPLETE, mutation set: {N0:S1, N1:S1, N2:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163711 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163711 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {s:N3:S1,N4:S1 m:N0:S1,N1:S1,N2:S1 MUTATION}.
I0917 02:14:22.173413    3260 [ld:srv:WF0] FailureDetector.cpp:1541] onGossipMessageSent() skipped at least 3 log entries
I0917 02:14:22.184737    3260 [ld:srv:WF0] FailureDetector.cpp:1541] onGossipMessageSent() Could not send gossip to WF0:N3:1 (127.0.0.1:41058): CONNFAILED: connection failed. Trying another node.
W0917 02:14:22.193691    3260 [ld:srv:WF0] Worker.cpp:1000] onStoppedRunning() skipped at least 17 log entries
W0917 02:14:22.195504    3260 [ld:srv:WF0] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.020s, source: [Message sent/received: GOSSIP]
W0917 02:14:22.260513    3243 [ld:srv:WG3] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.012s, source: [Request: MISC]
I0917 02:14:22.304038    3243 [ld:srv:WG3] EpochRecovery.cpp:1502] onDigestMayHaveBecomeComplete() Completing an authoritative partial digest for epoch 4 of log 4611686018427387903 because grace period with timeout of 100ms expired. State: {s:N3:S1,N4:S1 m:N0:S1,N1:S1,N2:S1 DIGEST}
I0917 02:14:22.306827    3243 [ld:srv:WG3] EpochRecovery.cpp:566] startMutations() Log 4611686018427387903 epoch 4 final digest before mutation: consensus LNG 0, start esn: 1, 1 entries, first esn: 1, last esn: 1, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_INCOMPLETE, mutation set: {N0:S1, N1:S1, N2:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387903 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387903 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {s:N3:S1,N4:S1 m:N0:S1,N1:S1,N2:S1 MUTATION}.
I0917 02:14:22.309563    3243 [ld:srv:WG3] CheckSealRequest.cpp:235] onTimeout() Timed out on replies for log:4611686018427387903 for CheckSealRequest(rqid:30064771075, gss-rqid:38654705665), expected_from:[N0:S1 N1:S1 N4:S1 ], recvd_from:[N0:S1 N1:S1 N4:S1 ], replies_successful=2
W0917 02:14:22.311362    3243 [ld:srv:WG3] SocketSender.cpp:348] sendMessageImpl() Unable to send a message of type GET_SEQ_STATE_REPLY to WG3:C12 (127.0.0.1:46676): error NOTCONN
E0917 02:14:22.312910    3243 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:959] sendReply() Failed to send GET_SEQ_STATE_REPLY to WG3:C12 (127.0.0.1:46676): NOTCONN: not connected to destination. (log:4611686018427387903, rqid:38654705665, status=AGAIN, last_released_lsn:LSN_INVALID, next_lsn:LSN_INVALID, ctx:store-message)
I0917 02:14:22.315771    3251 [ld:srv:WG4] EpochRecovery.cpp:1502] onDigestMayHaveBecomeComplete() Completing an authoritative partial digest for epoch 2 of log 13835058055282163708 because grace period with timeout of 100ms expired. State: {s:N3:S0,N4:S0 m:N0:S0,N1:S0,N2:S0 DIGEST}
I0917 02:14:22.318016    3251 [ld:srv:WG4] EpochRecovery.cpp:566] startMutations() Log 13835058055282163708 epoch 2 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_INCOMPLETE, mutation set: {N0:S0, N1:S0, N2:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163708 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163708 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {s:N3:S0,N4:S0 m:N0:S0,N1:S0,N2:S0 MUTATION}.
I0917 02:14:22.361003    3243 [ld:srv:WG3] Mutator.cpp:400] finalize() Mutator 4611686018427387903e4n1 completed (OK). Trace: W1{N0:S1,N1:S1,N2:S1};X:N0:S1:K;X:N1:S1:K;X:N2:S1:K;Y1:N1:S1:K;Y1:N0:S1:K;Y1:N2:S1:K;done:all:OK;
I0917 02:14:22.366637    3243 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:13835058055282163711, rqid:30064771105, ctx:start-message) from WG3:C4 (127.0.0.1:46556), but its data log sequencer's state is Recovery Incomplete
I0917 02:14:22.423549    3243 [ld:srv:WG3] CheckSealRequest.cpp:235] onTimeout() Timed out on replies for log:4611686018427387900 for CheckSealRequest(rqid:30064771077, gss-rqid:42949672961), expected_from:[N2:S0 N4:S0 ], recvd_from:[N2:S0 N4:S0 ], replies_successful=1
I0917 02:14:22.495364    3251 [ld:srv:WG4] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163707 has already been cleaned. No purge needed. last clean epoch: 2, purge to: 2.
I0917 02:14:22.600775    3251 [ld:srv:WG4] EpochRecovery.cpp:1502] onDigestMayHaveBecomeComplete() Completing an authoritative partial digest for epoch 3 of log 13835058055282163708 because grace period with timeout of 100ms expired. State: {s:N3:S0,N4:S0 m:N0:S0,N1:S0,N2:S0 DIGEST}
I0917 02:14:22.602662    3251 [ld:srv:WG4] EpochRecovery.cpp:566] startMutations() Log 13835058055282163708 epoch 3 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_INCOMPLETE, mutation set: {N0:S0, N1:S0, N2:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163708 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163708 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {s:N3:S0,N4:S0 m:N0:S0,N1:S0,N2:S0 MUTATION}.
I0917 02:14:22.605963    3220 [ld:srv:WG1] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163708 has already been cleaned. No purge needed. last clean epoch: 2, purge to: 2.
I0917 02:14:22.846253    3251 [ld:srv:WG4] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163707 has already been cleaned. No purge needed. last clean epoch: 3, purge to: 3.
I0917 02:14:22.943215    3251 [ld:srv:WG4] EpochRecovery.cpp:1502] onDigestMayHaveBecomeComplete() Completing an authoritative partial digest for epoch 4 of log 13835058055282163708 because grace period with timeout of 100ms expired. State: {s:N3:S0,N4:S0 m:N0:S0,N1:S0,N2:S0 DIGEST}
I0917 02:14:22.945131    3251 [ld:srv:WG4] EpochRecovery.cpp:566] startMutations() Log 13835058055282163708 epoch 4 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_INCOMPLETE, mutation set: {N0:S0, N1:S0, N2:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163708 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163708 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {s:N3:S0,N4:S0 m:N0:S0,N1:S0,N2:S0 MUTATION}.
I0917 02:14:22.953251    3220 [ld:srv:WG1] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163708 has already been cleaned. No purge needed. last clean epoch: 3, purge to: 3.
I0917 02:14:23.024914    3251 [ld:srv:WG4] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387903, rqid:81604378624, ctx:store-message) from WG4:C9 (127.0.0.1:46634), but its data log sequencer's state is Recovery Incomplete
I0917 02:14:23.061602    3220 [ld:srv:WG1] PurgeUncleanEpochs.cpp:592] complete() skipped at least 4 log entries
I0917 02:14:23.063846    3220 [ld:srv:WG1] PurgeUncleanEpochs.cpp:592] complete() Successfully purged epochs [3, 3] for log 13835058055282163708.
I0917 02:14:23.110274    3222 [ld:srv:WG2] CheckSealRequest.cpp:128] onSent() skipped at least 10 log entries
I0917 02:14:23.111835    3222 [ld:srv:WG2] CheckSealRequest.cpp:128] onSent() Sending a check-seal message to N4:S0 failed with err:CONNFAILED (log:4611686018427387900, rqid:81604378645, gss-rqid:17179869194)
W0917 02:14:23.198714    3251 [ld:srv:WG4] Worker.cpp:1000] onStoppedRunning() skipped at least 2 log entries
W0917 02:14:23.229226    3251 [ld:srv:WG4] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.025s, source: [Message sent/received: CLEANED]
I0917 02:14:23.235621    3251 [ld:srv:WG4] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163707 has already been cleaned. No purge needed. last clean epoch: 4, purge to: 4.
I0917 02:14:23.246053    3260 [ld:srv:WF0] FailureDetector.cpp:1541] onGossipMessageSent() skipped at least 2 log entries
I0917 02:14:23.266455    3260 [ld:srv:WF0] FailureDetector.cpp:1541] onGossipMessageSent() Could not send gossip to WF0:N4:1 (127.0.0.1:41047): CONNFAILED: connection failed. Trying another node.
W0917 02:14:23.256275    3251 [ld:srv:WG4] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.020s, source: [Message sent/received: CLEAN]
I0917 02:14:23.286967    3251 [ld:srv:WG4] CheckSealRequest.cpp:128] onSent() Sending a check-seal message to N4:S0 failed with err:CONNFAILED (log:4611686018427387900, rqid:34359738385, gss-rqid:68719476737)
I0917 02:14:23.392137    3251 [ld:srv:WG4] EpochRecovery.cpp:1502] onDigestMayHaveBecomeComplete() Completing an authoritative partial digest for epoch 5 of log 13835058055282163708 because grace period with timeout of 100ms expired. State: {s:N3:S0,N4:S0 m:N0:S0,N1:S0,N2:S0 DIGEST}
I0917 02:14:23.395130    3251 [ld:srv:WG4] EpochRecovery.cpp:566] startMutations() Log 13835058055282163708 epoch 5 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_INCOMPLETE, mutation set: {N0:S0, N1:S0, N2:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163708 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163708 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {s:N3:S0,N4:S0 m:N0:S0,N1:S0,N2:S0 MUTATION}.
I0917 02:14:23.399126    3220 [ld:srv:WG1] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163708 has already been cleaned. No purge needed. last clean epoch: 4, purge to: 4.
I0917 02:14:23.461936    3243 [ld:srv:WG3] EpochRecovery.cpp:1502] onDigestMayHaveBecomeComplete() Completing an authoritative partial digest for epoch 5 of log 4611686018427387903 because grace period with timeout of 100ms expired. State: {s:N3:S1,N4:S1 m:N0:S1,N1:S1,N2:S1 DIGEST}
I0917 02:14:23.463911    3243 [ld:srv:WG3] EpochRecovery.cpp:566] startMutations() Log 4611686018427387903 epoch 5 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_INCOMPLETE, mutation set: {N0:S1, N1:S1, N2:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387903 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387903 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {s:N3:S1,N4:S1 m:N0:S1,N1:S1,N2:S1 MUTATION}.
I0917 02:14:23.492999    3243 [ld:srv:WG3] Mutator.cpp:400] finalize() Mutator 4611686018427387903e5n1 completed (OK). Trace: W1{N2:S1,N1:S1,N0:S1};X:N2:S1:K;X:N1:S1:K;X:N0:S1:K;Y1:N2:S1:K;Y1:N0:S1:K;Y1:N1:S1:K;done:all:OK;
I0917 02:14:23.497348    3251 [ld:srv:WG4] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 4611686018427387903 has already been cleaned. No purge needed. last clean epoch: 4, purge to: 4.
I0917 02:14:23.510512    3243 [ld:srv:WG3] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163706 has already been cleaned. No purge needed. last clean epoch: 1, purge to: 1.
I0917 02:14:23.598073    3333 [ld:srv:WB3] ReplicatedStateMachine-inl.h:712] onGotDeltaLogTailLSN() [eventlog] Tail lsn of delta log is e7n0
I0917 02:14:23.602907    3333 [ld:srv:WB3] ClientReadStream.cpp:650] sendStart() skipped at least 1 log entries
I0917 02:14:23.604961    3333 [ld:srv:WB3] ClientReadStream.cpp:650] sendStart() Cannot send START message to N3:S1 for log 13835058055282163711: DISABLED: server is marked down
I0917 02:14:23.641614    3222 [ld:srv:WG2] GetSeqStateRequest.cpp:531] onReplyTimeout() Timed out waiting for GET_SEQ_STATE_REPLY for GetSeqStateRequest (rqid:34359738370, ctx:start-message) for log:13835058055282163707 from node N1:1
I0917 02:14:23.645273    3222 [ld:srv:WG2] SequencerRouter.cpp:321] onDeadNode() Node N1:1 unavailable for log:13835058055282163707, status=TIMEDOUT. request[type:GET_SEQ_STATE_REQ_TYPE, handler_:0x7f05361ab568]
I0917 02:14:23.650261    3222 [ld:srv:WG2] GetSeqStateRequest.cpp:385] onSent() Failed to send GET_SEQ_STATE message to N4:1 for log:13835058055282163707. Reason: CONNFAILED: connection failed
I0917 02:14:23.651521    3222 [ld:srv:WG2] SequencerRouter.cpp:321] onDeadNode() Node N4:1 unavailable for log:13835058055282163707, status=CONNFAILED. request[type:GET_SEQ_STATE_REQ_TYPE, handler_:0x7f05361ab568]
I0917 02:14:23.656801    3222 [ld:srv:WG2] SequencerRouter.cpp:149] onRedirected() Got redirected from N2:1 to N1:1 for log:13835058055282163707, status:REDIRECTED, request[type:GET_SEQ_STATE_REQ_TYPE, handler:0x7f05361ab568]
I0917 02:14:23.734368    3251 [ld:srv:WG4] EpochRecovery.cpp:1502] onDigestMayHaveBecomeComplete() Completing an authoritative partial digest for epoch 6 of log 13835058055282163708 because grace period with timeout of 100ms expired. State: {s:N3:S0,N4:S0 m:N0:S0,N1:S0,N2:S0 DIGEST}
I0917 02:14:23.736548    3251 [ld:srv:WG4] EpochRecovery.cpp:566] startMutations() Log 13835058055282163708 epoch 6 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_INCOMPLETE, mutation set: {N0:S0, N1:S0, N2:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163708 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163708 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {s:N3:S0,N4:S0 m:N0:S0,N1:S0,N2:S0 MUTATION}.
I0917 02:14:23.738504    3251 [ld:srv:WG4] CheckSealRequest.cpp:235] onTimeout() skipped at least 1 log entries
I0917 02:14:23.744366    3251 [ld:srv:WG4] CheckSealRequest.cpp:235] onTimeout() Timed out on replies for log:4611686018427387900 for CheckSealRequest(rqid:34359738385, gss-rqid:68719476737), expected_from:[N2:S0 N4:S0 ], recvd_from:[N2:S0 N4:S0 ], replies_successful=1
I0917 02:14:23.834327    3243 [ld:srv:WG3] EpochRecovery.cpp:1502] onDigestMayHaveBecomeComplete() Completing an authoritative partial digest for epoch 6 of log 4611686018427387903 because grace period with timeout of 100ms expired. State: {s:N3:S1,N4:S1 m:N0:S1,N1:S1,N2:S1 DIGEST}
I0917 02:14:23.837384    3243 [ld:srv:WG3] EpochRecovery.cpp:566] startMutations() Log 4611686018427387903 epoch 6 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_INCOMPLETE, mutation set: {N0:S1, N1:S1, N2:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387903 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387903 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {s:N3:S1,N4:S1 m:N0:S1,N1:S1,N2:S1 MUTATION}.
I0917 02:14:23.855523    3243 [ld:srv:WG3] Mutator.cpp:400] finalize() Mutator 4611686018427387903e6n1 completed (OK). Trace: W1{N0:S1,N1:S1,N2:S1};X:N0:S1:K;X:N1:S1:K;X:N2:S1:K;Y1:N0:S1:K;Y1:N1:S1:K;Y1:N2:S1:K;done:all:OK;
I0917 02:14:23.938557    3333 [ld:srv:WB3] ClientReadStream.cpp:650] sendStart() skipped at least 10 log entries
I0917 02:14:23.940039    3333 [ld:srv:WB3] ClientReadStream.cpp:650] sendStart() Cannot send START message to N3:S1 for log 4611686018427387903: DISABLED: server is marked down
I0917 02:14:24.006467    3251 [ld:srv:WG4] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 13835058055282163708, next_epoch:7
I0917 02:14:24.146455    3243 [ld:srv:WG3] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 4611686018427387903, next_epoch:7
I0917 02:14:24.191933    3333 [ld:srv:WB3] ClientReadStream.cpp:4571] operator()() Requested epoch metadata for log 4611686018427387903 epoch 2 is not available. Highest epoch known is 1.
I0917 02:14:24.206843    3333 [ld:srv:WB3] ClientReadStream.cpp:4571] operator()() Requested epoch metadata for log 4611686018427387903 epoch 3 is not available. Highest epoch known is 1.
I0917 02:14:24.230098    3333 [ld:srv:WB3] ClientReadStream.cpp:4571] operator()() Requested epoch metadata for log 4611686018427387903 epoch 5 is not available. Highest epoch known is 1.
I0917 02:14:24.256858    3333 [ld:srv:WB3] ClientReadStream.cpp:4571] operator()() Requested epoch metadata for log 4611686018427387903 epoch 6 is not available. Highest epoch known is 1.
I0917 02:14:24.267085    3333 [ld:srv:WB3] ClientReadStream.cpp:4571] operator()() Requested epoch metadata for log 4611686018427387903 epoch 7 is not available. Highest epoch known is 1.
I0917 02:14:24.269398    3333 [ld:srv:WB3] ReplicatedStateMachine-inl.h:1052] onReachedDeltaLogTailLSN() [eventlog] Reached tail of delta log. deliver_while_replaying:0, stop_at_tail_:0
I0917 02:14:24.270993    3333 [ld:srv:WB3] EventLogStateMachine.cpp:93] publishRebuildingSet() [eventlog] Published new EventLogRebuildingSet version (1)
I0917 02:14:24.274480    3333 [ld:srv:WB3] EventLogStateMachine.cpp:341] trimNotSnapshotted() Trimming event log up to lsn e0n1
W0917 02:14:24.278677    3333 [ld:srv:WB3] Worker.cpp:1000] onStoppedRunning() skipped at least 10 log entries
W0917 02:14:24.280012    3333 [ld:srv:WB3] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.012s, source: [Message sent/received: GAP]
I0917 02:14:24.341464    3220 [ld:srv:WG1] GetSeqStateRequest.cpp:531] onReplyTimeout() Timed out waiting for GET_SEQ_STATE_REPLY for GetSeqStateRequest (rqid:94489280515, ctx:start-message) for log:13835058055282163711 from node N0:1
I0917 02:14:24.344435    3220 [ld:srv:WG1] SequencerRouter.cpp:321] onDeadNode() Node N0:1 unavailable for log:13835058055282163711, status=TIMEDOUT. request[type:GET_SEQ_STATE_REQ_TYPE, handler_:0x7f053c7fbd28]
I0917 02:14:24.355730    3260 [ld:srv:WF0] FailureDetector.cpp:1541] onGossipMessageSent() skipped at least 6 log entries
I0917 02:14:24.358268    3260 [ld:srv:WF0] FailureDetector.cpp:1541] onGossipMessageSent() Could not send gossip to WF0:N4:1 (127.0.0.1:41047): CONNFAILED: connection failed. Trying another node.
W0917 02:14:24.366747    3220 [ld:srv:WG1] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.025s, source: [Request: GET_SEQ_STATE]
I0917 02:14:24.372632    3220 [ld:srv:WG1] SequencerRouter.cpp:149] onRedirected() Got redirected from N1:1 to N0:1 for log:13835058055282163711, status:REDIRECTED, request[type:GET_SEQ_STATE_REQ_TYPE, handler:0x7f053c7fbd28]
I0917 02:14:24.388069    3220 [ld:srv:WG1] MetaDataLogWriter.cpp:308] runAppender() MetaDataLogWriter cannnot accept a new metadata append due to an existing write / recovery in progress. Failing the append with E::NOBUF. Some info: logid: 4611686018427387903, state: 1, current_epoch: 7, last_writer_epoch: 0, recovery_only: true metadata_last_released: LSN_INVALID.
I0917 02:14:24.393047    3220 [ld:srv:WG1] MetaDataLogWriter.cpp:308] runAppender() MetaDataLogWriter cannnot accept a new metadata append due to an existing write / recovery in progress. Failing the append with E::NOBUF. Some info: logid: 4611686018427387903, state: 1, current_epoch: 7, last_writer_epoch: 0, recovery_only: true metadata_last_released: LSN_INVALID.
I0917 02:14:24.406972    3243 [ld:srv:WG3] PurgeUncleanEpochs.cpp:592] complete() skipped at least 7 log entries
I0917 02:14:24.409801    3243 [ld:srv:WG3] PurgeUncleanEpochs.cpp:592] complete() Successfully purged epochs [3, 3] for log 13835058055282163706.
I0917 02:14:24.531063    3333 [ld:srv:WB3] ProtocolHandler.cpp:127] notifyErrorOnSocket() skipped at least 100 log entries
I0917 02:14:24.532335    3333 [ld:srv:WB3] ProtocolHandler.cpp:127] notifyErrorOnSocket() Socket N3:1(127.0.0.1:41056) hit error AsyncSocketException: connect failed, type = Socket not open, errno = 111 (Connection refused)
I0917 02:14:24.904861    3333 [ld:srv:WB3] ClientReadStream.cpp:650] sendStart() skipped at least 16 log entries
I0917 02:14:24.906264    3333 [ld:srv:WB3] ClientReadStream.cpp:650] sendStart() Cannot send START message to N3:S1 for log 4611686018427387903: DISABLED: server is marked down
I0917 02:14:25.227467    3260 [ld:srv:WF0] FailureDetector.cpp:1285] updateNodeState() N4 transitioned from ALIVE to DEAD, FD State:(gossip: 31, instance-id: 1631844860532, failover: 0, starting: 1)
I0917 02:14:25.229335    3260 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N4 changed from STARTING to DEAD
I0917 02:14:25.232804    3251 [ld:srv:WG4] RebuildingSupervisor.cpp:222] addForRebuilding() Scheduling rebuilding of N4 to execute in 1200s
W0917 02:14:25.234901    3251 [ld:srv:WG4] RebuildingSupervisor.cpp:705] adjustRebuildingThrottle() Entering throttling mode: #draining=0, #rebuilding=0, #triggers=2, #adjusted cluster size=5, threshold=35%, actual=40%
I0917 02:14:25.236580    3251 [ld:srv:WG4] RebuildingSupervisor.cpp:718] adjustRebuildingThrottle() Entering throttling mode: #draining=0, #rebuilding=0, #triggers=2, #adjusted cluster size=5, threshold=35%, actual=40%
E0917 02:14:25.265372    3243 [ld:srv:WG3] WeightedCopySetSelector.cpp:1241] augment() Failed to select 3 nodes for rebuilding for log 13835058055282163711 because too many whole NODEs are unavailable. Copyset: []. Useful existing copies: 0. Nodeset: {N0:S1,N1:S1,N2:S1,N3:S1,N4:S1}. Unavailable nodes: {N3:S1,N0:S1,N4:S1}. Secondary replication: 3. NODE weights: [0.000(2.000-2.000), 2.000, 0.000(2.000-2.000), 2.000, 0.000(2.000-2.000)]
E0917 02:14:25.267858    3243 [ld:srv:WG3] NodeSetAccessor.cpp:224] pickWaveFromCopySet() Failed to pick a copyset for log 13835058055282163711, given exisiting nodes .
I0917 02:14:25.297538    3220 [ld:srv:WG1] RecoveryNode.cpp:62] digestingReadStream() RecoveryNode N2:S1 of epoch 4 of log 13835058055282163711 is not expecting a digest read stream 37. It is in state CLEANING.
I0917 02:14:25.300898    3220 [ld:srv:WG1] RecoveryNode.cpp:62] digestingReadStream() RecoveryNode N1:S1 of epoch 4 of log 13835058055282163711 is not expecting a digest read stream 37. It is in state CLEANING.
I0917 02:14:25.303095    3220 [ld:srv:WG1] RecoveryNode.cpp:62] digestingReadStream() RecoveryNode N0:S1 of epoch 4 of log 13835058055282163711 is not expecting a digest read stream 37. It is in state CLEANING.
I0917 02:14:25.450940    3260 [ld:srv:WF0] FailureDetector.cpp:1541] onGossipMessageSent() skipped at least 3 log entries
I0917 02:14:25.453304    3260 [ld:srv:WF0] FailureDetector.cpp:1541] onGossipMessageSent() Could not send gossip to WF0:N4:1 (127.0.0.1:41047): CONNFAILED: connection failed. Trying another node.
I0917 02:14:25.659987    3222 [ld:srv:WG2] GetSeqStateRequest.cpp:531] onReplyTimeout() Timed out waiting for GET_SEQ_STATE_REPLY for GetSeqStateRequest (rqid:34359738370, ctx:start-message) for log:13835058055282163707 from node N1:1
I0917 02:14:25.661839    3222 [ld:srv:WG2] SequencerRouter.cpp:321] onDeadNode() Node N1:1 unavailable for log:13835058055282163707, status=TIMEDOUT. request[type:GET_SEQ_STATE_REQ_TYPE, handler_:0x7f05361ab568]
E0917 02:14:25.663841    3222 [ld:srv:WG2] SequencerRouter.cpp:105] onFailure() Error during sequencer routing for log 13835058055282163707 (TIMEDOUT), request[type:GET_SEQ_STATE_REQ_TYPE, handler:0x7f05361ab568].
W0917 02:14:25.665630    3222 [ld:srv:WG2] GetSeqStateRequest.cpp:512] onSequencerRoutingFailure() Unable to find a sequencer node for log 13835058055282163707, rqid:34359738370, ctx:start-message, err(TIMEDOUT)
W0917 02:14:25.667551    3222 [ld:srv:WG2] GetSeqStateRequest.cpp:700] finalize() GET_SEQ_STATE for log 13835058055282163707 (rqid:34359738370, ctx:start-message) returned TIMEDOUT: request or connection timed out
I0917 02:14:26.170325    3220 [ld:srv:WG1] RecoveryNode.cpp:401] onDisconnect() Lost connection to N2:S1 while waiting for reply in state CLEANING during recovery of epoch 4 of log 13835058055282163711: PEER_CLOSED: connection closed by peer
I0917 02:14:26.189417    3333 [ld:srv:WB3] ClientReadStream.cpp:650] sendStart() skipped at least 2 log entries
I0917 02:14:26.190256    3333 [ld:srv:WB3] ClientReadStream.cpp:650] sendStart() Cannot send START message to N3:S1 for log 4611686018427387903: DISABLED: server is marked down
I0917 02:14:26.205933    3220 [ld:srv:WG1] RecoveryNode.cpp:401] onDisconnect() Lost connection to N1:S1 while waiting for reply in state CLEANING during recovery of epoch 4 of log 13835058055282163711: PEER_CLOSED: connection closed by peer
I0917 02:14:30.207940    4395 [logdeviced-main] main.cpp:399] main() Command line arguments: --local-log-store-path /data/store/N0:1/db --num-shards 2 --server-id yzy65ih3gu --log-file /data/store/N0:1/log --name Node0 --config-path file:/data/store/logdevice.conf --client-thrift-api-port 48460 --gossip-port 48454 --server-thrift-api-port 48459 --test-mode true --server-to-server-port 48458 --admin-port 48455 --loglevel info --port 48452 --sequencers lazy 
I0917 02:14:30.208919    4395 [logdeviced-main] main.cpp:403] main() Plugins loaded: {"Thrift client factory plugin":"built-in","ZookeeperClient factory":"built-in","ConfigSource factory":["built-in"],"PermissionChecker factory":"built-in","Build Info":"built-in","Plugin provider":["Built-in plugin provider","Static plug-in loader","Dynamic plugin loader"]}
I0917 02:14:30.209936    4395 [logdeviced-main] main.cpp:405] main() server starting
I0917 02:14:30.210979    4395 [logdeviced-main] main.cpp:410] main() version 99.99.99
I0917 02:14:30.211727    4395 [logdeviced-main] main.cpp:418] main() asserts off (NDEBUG set)
I0917 02:14:30.213537    4395 [logdeviced-main] TextConfigUpdater.cpp:56] fetchFromSource() Requesting main config "/data/store/logdevice.conf" (source: file)
I0917 02:14:30.215792    4395 [logdeviced-main] TextConfigUpdater.cpp:116] onContents() Main config from file, hash = ndhqu97h59
I0917 02:14:30.736263    4395 [logdeviced-main] Server.cpp:185] operator()() Updating settings from config took 518ms
I0917 02:14:30.737392    4395 [logdeviced-main] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 519ms, config update: 0ms, notifications: 0ms
I0917 02:14:30.738971    4395 [logdeviced-main] TextConfigUpdater.cpp:306] pushServerConfig() Updated config (version: 1 - hash: ndhqu97h59)
I0917 02:14:30.740777    4395 [logdeviced-main] TextConfigUpdater.cpp:397] pushLogsConfig() Ignoring changes in the 'logs' section in the config file because LogsConfigManager is ENABLED
I0917 02:14:30.741813    4395 [logdeviced-main] TextConfigUpdater.cpp:174] update() Config update result: ServerConfig=updated, ZK=skipped, LogsConfig=updated
I0917 02:14:30.742935    4395 [logdeviced-main] FileBasedVersionedConfigStore.cpp:35] FileBasedVersionedConfigStore() FileBasedVersionedConfigStore threads started.
I0917 02:14:30.748897    4395 [logdeviced-main] NodesConfigurationInit.cpp:233] operator()() Fetching initial nodes configuration (Attempt 1)...
I0917 02:14:30.754370    4518 [ld:file-ncs] NodesConfigurationInit.cpp:290] operator()() Got a NodesConfiguration of version: 5
I0917 02:14:30.755315    4518 [ld:file-ncs] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 0ms, config update: 0ms, notifications: 0ms
I0917 02:14:30.756893    4395 [logdeviced-main] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 0ms, config update: 0ms, notifications: 0ms
I0917 02:14:30.757718    4395 [logdeviced-main] Server.cpp:510] init() My Node ID is N0:1
I0917 02:14:30.759523    4395 [logdeviced-main] Server.cpp:517] init() My version is 0
I0917 02:14:30.762436    4395 [logdeviced-main] NoopTraceLogger.cpp:22] NoopTraceLogger() NOOP TraceLogger is ON, no trace samples will be published.
I0917 02:14:30.767191    4395 [logdeviced-main] FileBasedVersionedConfigStore.cpp:46] stopAndJoin() FileBasedVersionedConfigStore threads stopped.
I0917 02:14:30.768700    4395 [logdeviced-main] main.cpp:144] set_fd_limit() Limit on number of file descriptors is 1048576
W0917 02:14:30.769993    4395 [logdeviced-main] main.cpp:192] drop_root() Running under root, but no --user command line option specified. Will keep running as root
I0917 02:14:30.774394    4395 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 48452
I0917 02:14:30.776400    4395 [logdeviced-main] Server.cpp:781] initListeners() Initializing a gossip listener.
I0917 02:14:30.778874    4395 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 48454
I0917 02:14:30.780876    4395 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 48458 (SSL)
I0917 02:14:30.782084    4395 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 48462 (SSL)
I0917 02:14:30.783285    4395 [logdeviced-main] Listener.cpp:31] Listener() Created Listener: port 48461 (SSL)
I0917 02:14:30.810418    4395 [logdeviced-main] RocksDBLogStoreConfig.cpp:162] RocksDBLogStoreConfig() LD manages flushes and all memory budgets on the node. atomic_flush is set to true.
I0917 02:14:30.811557    4621 [io-stall:s1] IOTracing.cpp:83] stallDetectionThreadMain() Started IO stall detection thread for shard 1
I0917 02:14:30.813224    4620 [io-stall:s0] IOTracing.cpp:83] stallDetectionThreadMain() Started IO stall detection thread for shard 0
I0917 02:14:30.825879    4648 [ld:open-rocks0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:387] Created background thread for deletion scheduler with rate_bytes_per_sec: 100000000
I0917 02:14:30.828900    4649 [ld:open-rocks1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:387] Created background thread for deletion scheduler with rate_bytes_per_sec: 100000000
I0917 02:14:31.232702    4648 [ld:open-rocks0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000077.log.trash as trash -- OK
I0917 02:14:31.247740    4651 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 0after deleting file /data/store/N0:1/db/shard0/000077.log.trash
I0917 02:14:31.259377    4649 [ld:open-rocks1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000078.log.trash as trash -- OK
I0917 02:14:31.267805    4650 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 0after deleting file /data/store/N0:1/db/shard1/000078.log.trash
I0917 02:14:31.302401    4648 [ld:open-rocks0] PartitionedRocksDBStore.cpp:891] open() Partition s0:1000000 is under-replicated: Start:2021-09-17 02:10:14.859, Min:2021-09-17 02:09:59.859, Max: 2021-09-17 02:10:29.859
I0917 02:14:31.304472    4648 [ld:open-rocks0] PartitionedRocksDBStore.cpp:897] open() Shard[0] dirty ranges: [A:{{[2021-09-17 02:09:59.859,2021-09-17 02:10:29.859]}}]
I0917 02:14:31.314369    4649 [ld:open-rocks1] PartitionedRocksDBStore.cpp:891] open() Partition s1:1000000 is under-replicated: Start:2021-09-17 02:10:14.858, Min:2021-09-17 02:09:59.858, Max: 2021-09-17 02:10:29.858
I0917 02:14:31.315760    4648 [ld:open-rocks0] PartitionedRocksDBStore.cpp:1491] convertDataKeyFormat() DataKey format conversion took 0.001s. Read 6 records, 382 bytes. Wrote 0 records, 0 bytes.
I0917 02:14:31.317442    4649 [ld:open-rocks1] PartitionedRocksDBStore.cpp:897] open() Shard[1] dirty ranges: [A:{{[2021-09-17 02:09:59.858,2021-09-17 02:10:29.858]}}]
I0917 02:14:31.356682    4649 [ld:open-rocks1] PartitionedRocksDBStore.cpp:1491] convertDataKeyFormat() DataKey format conversion took 0.001s. Read 12 records, 716 bytes. Wrote 0 records, 0 bytes.
I0917 02:14:31.407413    4627 [rocks-high-0] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard0/000083.log.trash as trash -- OK
I0917 02:14:31.410319    4628 [rocks-high-1] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:84] Mark file: /data/store/N0:1/db/shard1/000084.log.trash as trash -- OK
I0917 02:14:31.426018    4650 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 1after deleting file /data/store/N0:1/db/shard1/000084.log.trash
I0917 02:14:31.426422    4651 [not-set] RocksDBLogger.cpp:86] Logv() [file/delete_scheduler.cc:262] Rate limiting is enabled with penalty 1after deleting file /data/store/N0:1/db/shard0/000083.log.trash
I0917 02:14:31.428916    4648 [ld:open-rocks0] ShardedRocksDBLocalLogStore.cpp:232] operator()() Opened RocksDB instance at /data/store/N0:1/db/shard0 in 601 ms
I0917 02:14:31.430204    4649 [ld:open-rocks1] ShardedRocksDBLocalLogStore.cpp:232] operator()() Opened RocksDB instance at /data/store/N0:1/db/shard1 in 598 ms
I0917 02:14:31.452018    4395 [logdeviced-main] ShardedRocksDBLocalLogStore.cpp:691] printDiskShardMapping() Disk -> Shard mapping: /data/store/N0:1/db/shard0 -> [0,1]
I0917 02:14:31.453671    4395 [logdeviced-main] ShardedRocksDBLocalLogStore.cpp:285] init() Initialized sharded RocksDB instance at /data/store/N0:1/db with 2 shards
E0917 02:14:31.456432    5141 [ld:populateLogS] ThreadID.cpp:26] set() Truncating thread name "ld:populateLogState0"!
E0917 02:14:31.456455    5142 [ld:populateLogS] ThreadID.cpp:26] set() Truncating thread name "ld:populateLogState1"!
I0917 02:14:31.467241    4395 [logdeviced-main] Server.cpp:1173] initLogStorageStateMap() Populating log storage state map successful.
E0917 02:14:31.474714    4395 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:31.481688    4395 [logdeviced-main] RecordCacheMonitorThread.cpp:27] RecordCacheMonitorThread() Record cache monitor thread started.
I0917 02:14:31.483014    5197 [ld:cache-evict] RecordCacheMonitorThread.cpp:44] threadMain() Record cache eviction thread started. Size limit is set to 4294967296 bytes.
I0917 02:14:31.484301    4395 [logdeviced-main] ClusterState.cpp:356] resizeClusterState() Cluster state size updated from 0 to 5
E0917 02:14:31.490250    4395 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:14:31.504414    4395 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:14:31.510232    4395 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:14:31.517163    4395 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:31.523666    5028 [ld:s0:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 2ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
E0917 02:14:31.524546    4395 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:31.529879    4395 [logdeviced-main] Processor.cpp:247] init() Initialized 5 workers of type GENERAL
E0917 02:14:31.536434    4395 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:31.543073    4395 [logdeviced-main] Processor.cpp:247] init() Initialized 1 workers of type FAILURE_DETECTOR
E0917 02:14:31.548571    4395 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:14:31.554966    4395 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:14:31.568446    4395 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
E0917 02:14:31.578663    4395 [logdeviced-main] FileConfigSource.cpp:129] stat_mtime() stat() on config file "" failed. errno=2 (No such file or directory)
I0917 02:14:31.581554    4395 [logdeviced-main] Processor.cpp:247] init() Initialized 4 workers of type BACKGROUND
I0917 02:14:31.595898    4395 [logdeviced-main] FailureDetector.cpp:139] FailureDetector() Failure Detector created with instance id: 1631844871595
I0917 02:14:31.598433    4395 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N0 changed from UNKNOWN to DEAD
I0917 02:14:31.610754    4395 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N0 changed from HEALTHY to UNHEALTHY
I0917 02:14:31.614202    4395 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N1 changed from UNKNOWN to DEAD
I0917 02:14:31.616370    4395 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N1 changed from HEALTHY to UNHEALTHY
I0917 02:14:31.620934    4395 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N2 changed from UNKNOWN to DEAD
I0917 02:14:31.622362    4395 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N2 changed from HEALTHY to UNHEALTHY
I0917 02:14:31.626523    4395 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N3 changed from UNKNOWN to DEAD
I0917 02:14:31.628464    4395 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N3 changed from HEALTHY to UNHEALTHY
I0917 02:14:31.629975    4395 [logdeviced-main] ClusterState.cpp:165] setNodeState() State of N4 changed from UNKNOWN to DEAD
I0917 02:14:31.636735    4395 [logdeviced-main] ClusterState.cpp:184] setNodeStatus() Status of N4 changed from HEALTHY to UNHEALTHY
W0917 02:14:31.640767    5224 [ld:srv:WG2] Worker.cpp:1322] processRequest() Request queued for 44 msec: BUFFERED_WRITER_CREATE_SHARD (id: 4294967298), p :HI_PRI
W0917 02:14:31.642788    5225 [ld:srv:WG3] Worker.cpp:1322] processRequest() Request queued for 47 msec: BUFFERED_WRITER_CREATE_SHARD (id: 4294967299), p :HI_PRI
W0917 02:14:31.645776    5203 [ld:srv:WG1] Worker.cpp:1322] processRequest() Request queued for 50 msec: BUFFERED_WRITER_CREATE_SHARD (id: 4294967297), p :HI_PRI
W0917 02:14:31.648160    5200 [ld:srv:WG0] Worker.cpp:1322] processRequest() Request queued for 52 msec: BUFFERED_WRITER_CREATE_SHARD (id: 4294967296), p :HI_PRI
W0917 02:14:31.654430    5224 [ld:srv:WG2] Worker.cpp:1322] processRequest() Request queued for 32 msec: NODE_STATE_UPDATED (id: 4294967323), p :LO_PRI
W0917 02:14:31.657974    5224 [ld:srv:WG2] Worker.cpp:1322] processRequest() Request queued for 35 msec: NODE_STATE_UPDATED (id: 4294967329), p :LO_PRI
W0917 02:14:31.661460    5224 [ld:srv:WG2] Worker.cpp:1322] processRequest() Request queued for 33 msec: NODE_STATE_UPDATED (id: 4294967333), p :LO_PRI
W0917 02:14:31.663889    5224 [ld:srv:WG2] Worker.cpp:1322] processRequest() Request queued for 35 msec: NODE_STATE_UPDATED (id: 4294967339), p :LO_PRI
W0917 02:14:31.665279    5224 [ld:srv:WG2] Worker.cpp:1322] processRequest() Request queued for 28 msec: NODE_STATE_UPDATED (id: 4294967343), p :LO_PRI
W0917 02:14:31.669539    5224 [ld:srv:WG2] Worker.cpp:1322] processRequest() Request queued for 32 msec: NODE_STATE_UPDATED (id: 4294967349), p :LO_PRI
I0917 02:14:31.680927    4395 [logdeviced-main] FileBasedVersionedConfigStore.cpp:35] FileBasedVersionedConfigStore() FileBasedVersionedConfigStore threads started.
I0917 02:14:31.689474    5254 [ld:srv:WB3] AllSequencers.cpp:981] noteConfigurationChanged() Acquiring lock for sequencer map took 0ms
I0917 02:14:31.693994    5254 [ld:srv:WB3] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 0ms, config update: 0ms, notifications: 6ms
W0917 02:14:31.698670    5254 [ld:srv:WB3] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.013s, source: [Request: NODES_CONFIGURATION_MANAGER]
I0917 02:14:31.702951    5254 [ld:srv:WB3] NodesConfigurationManager.cpp:507] onProcessingFinished() Updated local nodes config to version 5...
I0917 02:14:31.705043    5179 [ld:s0:default:1] PartitionedRocksDBStore.cpp:3527] readAllLogSnapshotBlobs() Snapshots column family does not exist
I0917 02:14:31.706870    5179 [ld:s0:default:1] RecordCacheRepopulationTask.cpp:103] execute() Repopulated record caches for 0 logs on shard 0, totaling 0 bytes.
I0917 02:14:31.706818    5186 [ld:s1:default:0] PartitionedRocksDBStore.cpp:3527] readAllLogSnapshotBlobs() Snapshots column family does not exist
I0917 02:14:31.709219    5186 [ld:s1:default:0] RecordCacheRepopulationTask.cpp:103] execute() Repopulated record caches for 0 logs on shard 1, totaling 0 bytes.
I0917 02:14:31.715617    4395 [logdeviced-main] Server.cpp:1319] initSequencers() Initializing FileEpochStore
I0917 02:14:31.718369    4395 [logdeviced-main] Server.cpp:1398] initSequencerPlacement() using SequencerOptions::LAZY
I0917 02:14:31.719324    4395 [logdeviced-main] Server.cpp:1430] initRebuildingCoordinator() Initializing EventLog RSM and RebuildingCoordinator
I0917 02:14:31.720667    4395 [logdeviced-main] RsmServerSnapshotStoreFactory.h:25] create() Attempting to create snapshot store (type:1, key:4611686018427387903)
I0917 02:14:31.722211    4395 [logdeviced-main] RsmSnapshotStoreFactory.h:31] create() Attempting to create snapshot store (type:1, key:4611686018427387903, is_server:1)
I0917 02:14:31.724038    4395 [logdeviced-main] RsmSnapshotStoreFactory.h:58] create() Creating LogBasedRSMSnapshotStore
I0917 02:14:31.726388    4395 [logdeviced-main] Server.cpp:1467] initRebuildingCoordinator() Starting RebuildingSupervisor
I0917 02:14:31.727490    4395 [logdeviced-main] Server.cpp:1480] initRebuildingCoordinator() Starting RebuildingCoordinator
I0917 02:14:31.727650    5228 [ld:srv:WG4] RebuildingSupervisor.cpp:62] init() Starting Rebuilding Supervisor
I0917 02:14:31.730323    5228 [ld:srv:WG4] RebuildingSupervisor.cpp:222] addForRebuilding() Scheduling rebuilding of N4 to execute in 1200s
I0917 02:14:31.731925    5228 [ld:srv:WG4] RebuildingSupervisor.cpp:222] addForRebuilding() Scheduling rebuilding of N3 to execute in 1200s
I0917 02:14:31.732474    4395 [logdeviced-main] LogStoreMonitor.cpp:190] start() Logstore monitoring thread started.
I0917 02:14:31.741328    4395 [logdeviced-main] UnreleasedRecordDetector.cpp:106] start() Unreleased record detector thread started.
I0917 02:14:31.732536    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:309] subscribeToEventLogIfNeeded() RebuildingCoordinator did not start yet because LogsConfig is not fully loaded yet
W0917 02:14:31.744046    5254 [ld:srv:WB3] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.012s, source: [Request: REBUILDING_COORDINATOR_INIT_REQUEST]
I0917 02:14:31.739234    5228 [ld:srv:WG4] RebuildingSupervisor.cpp:222] addForRebuilding() Scheduling rebuilding of N2 to execute in 1200s
I0917 02:14:31.743148    4395 [logdeviced-main] RsmServerSnapshotStoreFactory.h:25] create() Attempting to create snapshot store (type:1, key:4611686018427387901)
I0917 02:14:31.747987    4395 [logdeviced-main] RsmSnapshotStoreFactory.h:31] create() Attempting to create snapshot store (type:1, key:4611686018427387901, is_server:1)
I0917 02:14:31.746178    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:667] onBaseSnapshotRetrieved() [eventlog] Base snapshot has version:e0n1, delta_log_read_ptr:LSN_INVALID
I0917 02:14:31.754045    5254 [ld:srv:WB3] EventLogStateMachine.cpp:146] gotInitialState() Got base rebuilding set: {}
I0917 02:14:31.746824    5228 [ld:srv:WG4] RebuildingSupervisor.cpp:222] addForRebuilding() Scheduling rebuilding of N1 to execute in 1200s
W0917 02:14:31.758984    5228 [ld:srv:WG4] RebuildingSupervisor.cpp:705] adjustRebuildingThrottle() Entering throttling mode: #draining=0, #rebuilding=0, #triggers=4, #adjusted cluster size=5, threshold=35%, actual=80%
I0917 02:14:31.749206    4395 [logdeviced-main] RsmSnapshotStoreFactory.h:58] create() Creating LogBasedRSMSnapshotStore
I0917 02:14:31.756756    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:684] getDeltaLogTailLSN() [eventlog] Retrieving tail lsn of delta log...
I0917 02:14:31.759789    5228 [ld:srv:WG4] RebuildingSupervisor.cpp:718] adjustRebuildingThrottle() Entering throttling mode: #draining=0, #rebuilding=0, #triggers=4, #adjusted cluster size=5, threshold=35%, actual=80%
I0917 02:14:31.761403    5242 [ld:srv:WB1] LogsConfigManager.cpp:38] execute() Starting LogsConfigManager on Worker 1 in pool: BACKGROUND
I0917 02:14:31.764903    5242 [ld:srv:WB1] LogsConfigManager.cpp:165] start() Starting LogsConfig Manager RSM
I0917 02:14:31.762176    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:667] onBaseSnapshotRetrieved() [maintenancelog] Base snapshot has version:e0n1, delta_log_read_ptr:LSN_INVALID
I0917 02:14:31.767261    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:684] getDeltaLogTailLSN() [maintenancelog] Retrieving tail lsn of delta log...
E0917 02:14:31.762355    5225 [ld:srv:WG3] HashBasedSequencerLocator.cpp:208] locateSequencer() No available sequencer node for log 4611686018427387903. All sequencer nodes are unavailable.
I0917 02:14:31.766292    5242 [ld:srv:WB1] LogsConfigManager.cpp:206] start() Starting LogsConfig Replicated State Machine
I0917 02:14:31.769909    5242 [ld:srv:WB1] ReplicatedStateMachine-inl.h:38] startFetchingSnapshot() [logsconfig] Using RSMSnapshotStore...
E0917 02:14:31.768519    5228 [ld:srv:WG4] SequencerRouter.cpp:56] operator()() Error during sequencer lookup for log 4611686018427387899 (NOSEQUENCER), reporting NOSEQUENCER.
E0917 02:14:31.769168    5225 [ld:srv:WG3] SequencerRouter.cpp:105] onFailure() Error during sequencer routing for log 4611686018427387903 (NOSEQUENCER), request[type:GET_SEQ_STATE_REQ_TYPE, handler:0x7f8b40da6028].
I0917 02:14:31.771832    5242 [ld:srv:WB1] ReplicatedStateMachine-inl.h:1294] initSnapshotFetchTimer() [logsconfig] Creating timer to fetch snapshots ...
W0917 02:14:31.773054    5228 [ld:srv:WG4] GetSeqStateRequest.cpp:512] onSequencerRoutingFailure() Unable to find a sequencer node for log 4611686018427387899, rqid:8589934616, ctx:rsm, err(NOSEQUENCER)
I0917 02:14:31.777322    4395 [logdeviced-main] LogDeviceThriftServer.cpp:44] LogDeviceThriftServer() Thrift server LogDevice Admin API Service will listen on port 48455
W0917 02:14:31.774662    5225 [ld:srv:WG3] GetSeqStateRequest.cpp:700] finalize() GET_SEQ_STATE for log 4611686018427387903 (rqid:8589934614, ctx:rsm) returned NOSEQUENCER: no sequencer was found for log
E0917 02:14:31.777865    5254 [ld:srv:WB3] SyncSequencerRequest.cpp:178] onGotSeqState() Got sequencer state (id:8589934616) for log:4611686018427387899, status:NOSEQUENCER: no sequencer was found for log,sequencer:[invalid NodeID], last_released_lsn:LSN_INVALID, next_lsn:LSN_INVALID
E0917 02:14:31.785304    5254 [ld:srv:WB3] SyncSequencerRequest.cpp:178] onGotSeqState() Got sequencer state (id:8589934614) for log:4611686018427387903, status:NOSEQUENCER: no sequencer was found for log,sequencer:[invalid NodeID], last_released_lsn:LSN_INVALID, next_lsn:LSN_INVALID
I0917 02:14:31.778403    4395 [logdeviced-main] Server.cpp:1540] createAndAttachMaintenanceManager() Not initializing MaintenanceManager since it is disabled in settings
I0917 02:14:31.787989    4395 [logdeviced-main] Server.cpp:905] initThriftServer() Initializing Thrift Server: s2s-api
I0917 02:14:31.790155    4395 [logdeviced-main] LogDeviceThriftServer.cpp:44] LogDeviceThriftServer() Thrift server s2s-api will listen on port 48459
I0917 02:14:31.791217    4395 [logdeviced-main] Server.cpp:905] initThriftServer() Initializing Thrift Server: c2s-api
I0917 02:14:31.792556    4395 [logdeviced-main] LogDeviceThriftServer.cpp:44] LogDeviceThriftServer() Thrift server c2s-api will listen on port 48460
I0917 02:14:31.793749    4395 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:14:31.795028    4566 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:14:31.797014    4395 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:14:31.799563    4578 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:14:31.802280    4395 [logdeviced-main] SimpleThriftServer.cpp:28] start() Starting a listener thread for Thrift server s2s-api
I0917 02:14:31.802364    5235 [ld:srv:WF0] FailureDetector.cpp:228] startGetClusterState() Sending GET_CLUSTER_STATE to build initial FD cluster view
I0917 02:14:31.805725    5235 [ld:srv:WF0] ProtocolHandler.cpp:127] notifyErrorOnSocket() Socket N3:1(127.0.0.1:48421) hit error AsyncSocketException: connect failed, type = Socket not open, errno = 111 (Connection refused)
I0917 02:14:31.809686    5235 [ld:srv:WF0] FailureDetector.cpp:267] operator()() Cluster state received with 5 dead nodes (N0,N1,N2,N3,N4) and 0 boycotted nodes ()
I0917 02:14:31.810822    5235 [ld:srv:WF0] FailureDetector.cpp:288] buildInitialState() Wait over (cluster state received)
I0917 02:14:31.811974    5235 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N1 changed from DEAD to STARTING
I0917 02:14:31.813684    5228 [ld:srv:WG4] RebuildingSupervisor.cpp:256] onNodeStateChanged() Cancelling rebuilding trigger for node N1
I0917 02:14:31.814098    5235 [ld:srv:WF0] FailureDetector.cpp:316] buildInitialState() N1 transitioned to ALIVE
I0917 02:14:31.816183    5235 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N2 changed from DEAD to STARTING
I0917 02:14:31.817389    5228 [ld:srv:WG4] RebuildingSupervisor.cpp:256] onNodeStateChanged() Cancelling rebuilding trigger for node N2
I0917 02:14:31.817972    5235 [ld:srv:WF0] FailureDetector.cpp:316] buildInitialState() N2 transitioned to ALIVE
I0917 02:14:31.818931    5235 [ld:srv:WF0] FailureDetector.cpp:316] buildInitialState() N3 transitioned to DEAD
I0917 02:14:31.819935    5235 [ld:srv:WF0] FailureDetector.cpp:316] buildInitialState() N4 transitioned to DEAD
I0917 02:14:31.821131    5235 [ld:srv:WF0] FailureDetector.cpp:334] buildInitialState() N1 transitioned to UNHEALTHY (status)
I0917 02:14:31.821912    5235 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N2 changed from UNHEALTHY to UNDEFINED
I0917 02:14:31.822772    5235 [ld:srv:WF0] FailureDetector.cpp:334] buildInitialState() N2 transitioned to UNDEFINED (status)
I0917 02:14:31.823484    5235 [ld:srv:WF0] FailureDetector.cpp:334] buildInitialState() N3 transitioned to UNHEALTHY (status)
I0917 02:14:31.824314    5235 [ld:srv:WF0] FailureDetector.cpp:334] buildInitialState() N4 transitioned to UNHEALTHY (status)
I0917 02:14:31.824956    5235 [ld:srv:WF0] FailureDetector.cpp:344] buildInitialState() Initializing DomainIsolationChecker
I0917 02:14:31.825724    5235 [ld:srv:WF0] FailureDetector.cpp:915] startSuspectTimer() Starting suspect state timer
I0917 02:14:31.827319    5235 [ld:srv:WF0] FailureDetector.cpp:1285] updateNodeState() N0 transitioned from DEAD to ALIVE, FD State:(gossip: 0, instance-id: 0, failover: 0, starting: 1)
I0917 02:14:31.832468    5235 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N0 changed from DEAD to STARTING
I0917 02:14:31.834304    5235 [ld:srv:WF0] FailureDetector.cpp:1456] broadcastBringupUpdate() Broadcasting suspect-state-finished message.
I0917 02:14:31.835085    5235 [ld:srv:WF0] FailureDetector.cpp:1476] broadcastBringupUpdate() Sending suspect-state-finished message with instance id:1631844871595 to node WF0:N4:1 (UNKNOWN)
I0917 02:14:31.835835    4395 [logdeviced-main] SimpleThriftServer.cpp:28] start() Starting a listener thread for Thrift server c2s-api
I0917 02:14:31.836443    5235 [ld:srv:WF0] FailureDetector.cpp:1476] broadcastBringupUpdate() Sending suspect-state-finished message with instance id:1631844871595 to node WF0:N3:1 (127.0.0.1:48421)
I0917 02:14:31.842844    5235 [ld:srv:WF0] FailureDetector.cpp:1456] broadcastBringupUpdate() Broadcasting bringup message.
I0917 02:14:31.844745    5235 [ld:srv:WF0] FailureDetector.cpp:937] startGossiping() Start Gossiping.
I0917 02:14:31.853969    5235 [ld:srv:WF0] FailureDetector.cpp:925] operator()() Suspect timeout expired
I0917 02:14:31.894668    4395 [logdeviced-main] SimpleThriftServer.cpp:28] start() Starting a listener thread for Thrift server LogDevice Admin API Service
I0917 02:14:31.913995    4395 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:14:31.915393    4594 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:14:31.916433    4395 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:14:31.917104    4566 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:14:31.918155    4395 [logdeviced-main] Listener.cpp:39] startAcceptingConnections() Start called
I0917 02:14:31.918914    4566 [not-set] Listener.cpp:63] setupAsyncSocket() Setup called
I0917 02:14:31.920102    4395 [logdeviced-main] main.cpp:464] main() Listeners initialized
I0917 02:14:31.955404    5235 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N0 changed from UNHEALTHY to UNDEFINED
I0917 02:14:31.958513    5235 [ld:srv:WF0] FailureDetector.cpp:1541] onGossipMessageSent() Could not send gossip to WF0:N3:1 (127.0.0.1:48421): CONNFAILED: connection failed. Trying another node.
I0917 02:14:32.017286    5235 [ld:srv:WF0] FailureDetector.cpp:682] processFlags() Received suspect-state-finished message from N4:1 with instance id:1631844871814, sent_time:1631844872010ms
I0917 02:14:32.018745    5235 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N4 changed from DEAD to STARTING
I0917 02:14:32.021833    5228 [ld:srv:WG4] RebuildingSupervisor.cpp:256] onNodeStateChanged() Cancelling rebuilding trigger for node N4
W0917 02:14:32.023276    5228 [ld:srv:WG4] RebuildingSupervisor.cpp:734] adjustRebuildingThrottle() Exiting throttling mode: #draining=0, #rebuilding=0, #triggers=1, #adjusted cluster size=5, threshold=35%, actual=20%
I0917 02:14:32.024874    5228 [ld:srv:WG4] RebuildingSupervisor.cpp:751] adjustRebuildingThrottle() Not triggering new rebuildings until 2021-09-17 02:34:32.024: It's too soon after exiting throttling mode
I0917 02:14:32.025514    5235 [ld:srv:WF0] FailureDetector.cpp:717] processFlags() N4 transitioned to STARTING as a result of receiving suspect-state-finished message, FD State:(gossip: 0, instance-id: 1631844871814, failover: 0, starting: 1)
I0917 02:14:32.027653    5235 [ld:srv:WF0] FailureDetector.cpp:682] processFlags() Received bringup message from N4:1 with instance id:1631844871814, sent_time:1631844872011ms
I0917 02:14:32.029405    5235 [ld:srv:WF0] FailureDetector.cpp:1845] resetVersions() Resetting RSM and NCM Versions for N4
I0917 02:14:32.064815    5235 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N1 changed from UNHEALTHY to HEALTHY
I0917 02:14:32.065521    5235 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N2 changed from UNDEFINED to HEALTHY
I0917 02:14:32.094299    5235 [ld:srv:WF0] FailureDetector.cpp:682] processFlags() Received suspect-state-finished message from N3:1 with instance id:1631844871925, sent_time:1631844872083ms
I0917 02:14:32.095714    5235 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N3 changed from DEAD to STARTING
I0917 02:14:32.098212    5228 [ld:srv:WG4] RebuildingSupervisor.cpp:256] onNodeStateChanged() Cancelling rebuilding trigger for node N3
I0917 02:14:32.098412    5235 [ld:srv:WF0] FailureDetector.cpp:717] processFlags() N3 transitioned to STARTING as a result of receiving suspect-state-finished message, FD State:(gossip: 0, instance-id: 1631844871925, failover: 0, starting: 1)
I0917 02:14:32.100372    5235 [ld:srv:WF0] FailureDetector.cpp:682] processFlags() Received bringup message from N3:1 with instance id:1631844871925, sent_time:1631844872084ms
I0917 02:14:32.101287    5235 [ld:srv:WF0] FailureDetector.cpp:1845] resetVersions() Resetting RSM and NCM Versions for N3
I0917 02:14:32.272913    5235 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N0 changed from UNDEFINED to HEALTHY
I0917 02:14:32.502385    5200 [ld:srv:WG0] GET_SEQ_STATE_Message.cpp:238] getSequencer() Sequencer for log:4611686018427387900 is absent, state=UNAVAILABLE, cur_epoch=0, attempting to activate it upon receiving GSS (rqid:73014444033, ctx:sync-sequencer, min_epoch:none) from WG0:C5 (127.0.0.1:51394)
I0917 02:14:32.532716    5228 [ld:srv:WG4] GET_SEQ_STATE_Message.cpp:238] getSequencer() Sequencer for log:4611686018427387903 is absent, state=UNAVAILABLE, cur_epoch=0, attempting to activate it upon receiving GSS (rqid:30064771077, ctx:rsm, min_epoch:none) from WG4:C6 (127.0.0.1:51398)
I0917 02:14:32.537015    5200 [ld:srv:WG0] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387900, rqid:73014444033, ctx:sync-sequencer) from WG0:C5 (127.0.0.1:51394), but its data log sequencer's state is ACTIVATING
I0917 02:14:32.540660    5200 [ld:srv:WG0] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 4611686018427387900 epoch 9.
I0917 02:14:32.547075    5200 [ld:srv:WG0] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 4611686018427387900: {[E:1 (at 2021-09-17 02:14:32.510) since:1 (at 2021-09-17 02:10:15.391) R:[node: 2] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]], Until:9}.
I0917 02:14:32.551187    5228 [ld:srv:WG4] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387903, rqid:30064771077, ctx:rsm) from WG4:C6 (127.0.0.1:51398), but its data log sequencer's state is ACTIVATING
I0917 02:14:32.555154    5228 [ld:srv:WG4] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 4611686018427387903 epoch 9.
I0917 02:14:32.553794    5200 [ld:srv:WG0] AllSequencers.cpp:527] onEpochMetaDataFromEpochStore() Activated a sequencer for log 4611686018427387900 (reason: GET_SEQ_STATE) with epoch 9, metadata: [E:9 (at 2021-09-17 02:14:32.510) since:1 (at 2021-09-17 02:10:15.391) R:[node: 2] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]]. Activation Result: RECOVERY.
I0917 02:14:32.556509    5228 [ld:srv:WG4] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 4611686018427387903: {[E:1 (at 2021-09-17 02:14:32.539) since:1 (at 2021-09-17 02:10:15.661) R:[node: 3] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]], Until:9}.
I0917 02:14:32.558749    5228 [ld:srv:WG4] AllSequencers.cpp:527] onEpochMetaDataFromEpochStore() Activated a sequencer for log 4611686018427387903 (reason: GET_SEQ_STATE) with epoch 9, metadata: [E:9 (at 2021-09-17 02:14:32.539) since:1 (at 2021-09-17 02:10:15.661) R:[node: 3] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]]. Activation Result: RECOVERY.
I0917 02:14:32.562069    5228 [ld:srv:WG4] PeriodicReleases.cpp:416] execute() Start broadcasting releases for log 4611686018427387900.
I0917 02:14:32.562091    5225 [ld:srv:WG3] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 4611686018427387903 with next_epoch 9
I0917 02:14:32.562128    5203 [ld:srv:WG1] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 13835058055282163711 epoch 9.
I0917 02:14:32.564151    5066 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() skipped at least 10 log entries
I0917 02:14:32.565842    5066 [ld:s1:db-bg-fl] PartitionedRocksDBStore.cpp:6498] flushBackgroundThreadRun() Flushing: []. Total stats: active: 0.001MB in 1 memtables, flushing: 0.000MB, pinned: 0.000MB. Eval time: 0ms, flush init: 0ms, throttle eval: 0ms, npartitions: 1. Existing memtables: metadata A:0.001MB, []
I0917 02:14:32.564641    5228 [ld:srv:WG4] MetaDataLogTrimmer.cpp:115] schedulePeriodicTrimming() Will run periodic trimming every ~7200000 s for 4611686018427387903
I0917 02:14:32.565953    5203 [ld:srv:WG1] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 13835058055282163711: {[E:1 (at -inf) since:1 (at -inf) R:[node: 3] V:2 flags: params:{target:0 seed:0 signature:0} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]], Until:9}.
I0917 02:14:32.569504    5225 [ld:srv:WG3] LogRecoveryRequest.cpp:1018] sealLog() Starting recovery of epochs [7, 8] of log 4611686018427387903
I0917 02:14:32.573904    5228 [ld:srv:WG4] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 13835058055282163708 epoch 9.
I0917 02:14:32.575018    5203 [ld:srv:WG1] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 13835058055282163711 with next_epoch 9
I0917 02:14:32.583158    5228 [ld:srv:WG4] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 13835058055282163708: {[E:1 (at -inf) since:1 (at -inf) R:[node: 3] V:2 flags: params:{target:0 seed:0 signature:0} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]], Until:9}.
W0917 02:14:32.585084    5228 [ld:srv:WG4] Worker.cpp:1000] onStoppedRunning() skipped at least 10 log entries
I0917 02:14:32.586527    5203 [ld:srv:WG1] LogRecoveryRequest.cpp:1018] sealLog() Starting recovery of epochs [4, 8] of log 13835058055282163711
W0917 02:14:32.587192    5228 [ld:srv:WG4] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.011s, source: [Request: START_METADATA_LOG_RECOVERY]
I0917 02:14:32.600153    5228 [ld:srv:WG4] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 13835058055282163708 with next_epoch 9
I0917 02:14:32.594424    5200 [ld:srv:WG0] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 4611686018427387900 with next_epoch 9
I0917 02:14:32.606712    5203 [ld:srv:WG1] CONFIG_FETCH_Message.cpp:70] onReceived() CONFIG_FETCH messages received. E.g., from WG1:C10 (127.0.0.1:51452)
W0917 02:14:32.612992    5200 [ld:srv:WG0] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.019s, source: [Request: LOG_RECOVERY]
I0917 02:14:32.614014    5200 [ld:srv:WG0] PeriodicReleases.cpp:416] execute() Start broadcasting releases for log 4611686018427387903.
I0917 02:14:32.613206    5228 [ld:srv:WG4] LogRecoveryRequest.cpp:1018] sealLog() Starting recovery of epochs [7, 8] of log 13835058055282163708
I0917 02:14:32.615629    5200 [ld:srv:WG0] LogRecoveryRequest.cpp:1018] sealLog() Starting recovery of epochs [2, 8] of log 4611686018427387900
I0917 02:14:32.621123    5225 [ld:srv:WG3] CHECK_SEAL_onReceived.cpp:48] prepareReplyWithHighestSeal() Soft seal for log:4611686018427387899 is [epoch:0, seq:[invalid NodeID]], but normal seal is absent; src_rqid:17179869185
I0917 02:14:32.635818    5178 [ld:s0:default:0] RecordCache.cpp:257] getEpochRecordCache() RecordCache miss for log 4611686018427387900 epoch 2: cache not found but cannot declare no data. head epoch : 0, last_nonauthoritative_epoch: 2.
I0917 02:14:32.639662    5203 [ld:srv:WG1] Connection.cpp:724] close() Closing socket C10(127.0.0.1:51452). Reason: PEER_CLOSED: connection closed by peer
I0917 02:14:32.685891    5225 [ld:srv:WG3] EpochRecovery.cpp:566] startMutations() Log 4611686018427387903 epoch 7 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387903 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387903 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
I0917 02:14:32.704474    5203 [ld:srv:WG1] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163707 has already been cleaned. No purge needed. last clean epoch: 5, purge to: 5.
W0917 02:14:32.711555    5200 [ld:srv:WG0] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.024s, source: [Message sent/received: CHECK_SEAL_REPLY]
I0917 02:14:32.711661    5225 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387903, rqid:68719476739, ctx:start-message) from WG3:C20 (127.0.0.1:51534), but its data log sequencer's state is Recovery Incomplete
W0917 02:14:32.725180    5225 [ld:srv:WG3] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.014s, source: [Message sent/received: CHECK_SEAL_REPLY]
I0917 02:14:32.726172    5225 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:13835058055282163711, rqid:38654705669, ctx:start-message) from WG3:C21 (127.0.0.1:51536), but its data log sequencer's state is Recovery Incomplete
I0917 02:14:32.730772    5225 [ld:srv:WG3] Mutator.cpp:400] finalize() Mutator 4611686018427387903e7n1 completed (OK). Trace: W1{N1:S1,N3:S1,N4:S1};X:N1:S1:K;X:N3:S1:K;X:N4:S1:K;Y1:N1:S1:K;Y1:N3:S1:K;Y1:N4:S1:K;done:can_replicate:OK;
I0917 02:14:32.731475    5203 [ld:srv:WG1] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387903, rqid:30064771074, ctx:start-message) from WG1:C24 (127.0.0.1:51538), but its data log sequencer's state is Recovery Incomplete
I0917 02:14:32.735759    5225 [ld:srv:WG3] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 4611686018427387903 has already been cleaned. No purge needed. last clean epoch: 6, purge to: 6.
I0917 02:14:32.737130    5203 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 13835058055282163711 epoch 4 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163711 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163711 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
I0917 02:14:32.758367    5224 [ld:srv:WG2] RecordCache.cpp:257] getEpochRecordCache() RecordCache miss for log 4611686018427387900 epoch 2: cache not found but cannot declare no data. head epoch : 0, last_nonauthoritative_epoch: 2.
I0917 02:14:32.776213    5242 [ld:srv:WB1] ReplicatedStateMachine-inl.h:130] getSnapshot() [logsconfig] Fetching snapshot with ver:e0n1, (waiting_for_snapshot_:LSN_INVALID, version_:e0n1). sync_state_:0
I0917 02:14:32.795361    5235 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() skipped at least 2 log entries
I0917 02:14:32.795474    5200 [ld:srv:WG0] EpochRecovery.cpp:566] startMutations() Log 4611686018427387900 epoch 2 final digest before mutation: consensus LNG 0, start esn: 1, 1 entries, first esn: 1, last esn: 1, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:14:32.796402    5235 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N3 changed from UNDEFINED to HEALTHY
I0917 02:14:32.798673    5235 [ld:srv:WF0] ClusterState.cpp:184] setNodeStatus() Status of N4 changed from UNDEFINED to HEALTHY
I0917 02:14:32.798351    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:712] onGotDeltaLogTailLSN() [eventlog] Tail lsn of delta log is e9n0
I0917 02:14:32.816399    5203 [ld:srv:WG1] PurgeUncleanEpochs.cpp:592] complete() Successfully purged epochs [5, 5] for log 13835058055282163707.
I0917 02:14:32.799658    5228 [ld:srv:WG4] EpochRecovery.cpp:566] startMutations() Log 13835058055282163708 epoch 7 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163708 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163708 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:14:32.799975    5224 [ld:srv:WG2] RecordCache.cpp:257] getEpochRecordCache() RecordCache miss for log 4611686018427387900 epoch 2: cache not found but cannot declare no data. head epoch : 0, last_nonauthoritative_epoch: 2.
I0917 02:14:32.821842    5200 [ld:srv:WG0] Mutator.cpp:400] finalize() Mutator 4611686018427387900e2n1 completed (OK). Trace: W1{N0:S0,N3:S0};X:N0:S0:K;X:N3:S0:K;Y1:N3:S0:K;Y1:N0:S0:K;done:can_replicate:OK;
I0917 02:14:32.823955    5200 [ld:srv:WG0] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163708 has already been cleaned. No purge needed. last clean epoch: 6, purge to: 6.
I0917 02:14:32.835584    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:712] onGotDeltaLogTailLSN() [maintenancelog] Tail lsn of delta log is e8n0
I0917 02:14:32.892057    5228 [ld:srv:WG4] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 4611686018427387899 has already been cleaned. No purge needed. last clean epoch: 2, purge to: 2.
I0917 02:14:32.990087    5225 [ld:srv:WG3] EpochRecovery.cpp:566] startMutations() Log 4611686018427387903 epoch 8 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387903 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387903 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
I0917 02:14:32.991312    5203 [ld:srv:WG1] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163707 has already been cleaned. No purge needed. last clean epoch: 6, purge to: 6.
I0917 02:14:33.002321    5225 [ld:srv:WG3] Mutator.cpp:400] finalize() Mutator 4611686018427387903e8n1 completed (OK). Trace: W1{N3:S1,N1:S1,N4:S1};X:N3:S1:K;X:N1:S1:K;X:N4:S1:K;Y1:N3:S1:K;Y1:N1:S1:K;Y1:N4:S1:K;done:can_replicate:OK;
I0917 02:14:33.005713    5225 [ld:srv:WG3] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 4611686018427387903 has already been cleaned. No purge needed. last clean epoch: 7, purge to: 7.
I0917 02:14:33.062573    5200 [ld:srv:WG0] EpochRecovery.cpp:566] startMutations() Log 4611686018427387900 epoch 3 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:14:33.066337    5228 [ld:srv:WG4] EpochRecovery.cpp:566] startMutations() Log 13835058055282163708 epoch 8 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163708 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163708 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:14:33.071456    5200 [ld:srv:WG0] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 13835058055282163708 has already been cleaned. No purge needed. last clean epoch: 7, purge to: 7.
I0917 02:14:33.079145    5200 [ld:srv:WG0] Mutator.cpp:400] finalize() Mutator 4611686018427387900e3n1 completed (OK). Trace: W1{N0:S0,N4:S0};X:N0:S0:K;X:N4:S0:K;Y1:N4:S0:K;Y1:N0:S0:K;done:can_replicate:OK;
I0917 02:14:33.081347    5224 [ld:srv:WG2] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 4611686018427387900 has already been cleaned. No purge needed. last clean epoch: 2, purge to: 2.
I0917 02:14:33.094780    5228 [ld:srv:WG4] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 4611686018427387899 has already been cleaned. No purge needed. last clean epoch: 3, purge to: 3.
I0917 02:14:33.200782    5225 [ld:srv:WG3] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 4611686018427387903, next_epoch:9
I0917 02:14:33.220449    5254 [ld:srv:WB3] ClientReadStream.cpp:4571] operator()() Requested epoch metadata for log 4611686018427387903 epoch 2 is not available. Highest epoch known is 1.
I0917 02:14:33.230539    5254 [ld:srv:WB3] ClientReadStream.cpp:4571] operator()() Requested epoch metadata for log 4611686018427387903 epoch 3 is not available. Highest epoch known is 1.
W0917 02:14:33.231456    5254 [ld:srv:WB3] ClientReadStream.cpp:2476] updateCurrentMetaData() Existing epoch metadata has changed for log 4611686018427387903 on epoch 1, check metadata log!
I0917 02:14:33.279708    5254 [ld:srv:WB3] ClientReadStream.cpp:4571] operator()() Requested epoch metadata for log 4611686018427387903 epoch 8 is not available. Highest epoch known is 1.
I0917 02:14:33.290368    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:1052] onReachedDeltaLogTailLSN() [eventlog] Reached tail of delta log. deliver_while_replaying:0, stop_at_tail_:0
I0917 02:14:33.292879    5254 [ld:srv:WB3] EventLogStateMachine.cpp:93] publishRebuildingSet() [eventlog] Published new EventLogRebuildingSet version (1)
I0917 02:14:33.294235    5254 [ld:srv:WB3] EventLogStateMachine.cpp:341] trimNotSnapshotted() Trimming event log up to lsn e0n1
I0917 02:14:33.308036    5200 [ld:srv:WG0] GetEpochRecoveryMetadataRequest.cpp:172] checkAllFullyAuthoritativeNodesResponded() GetEpochRecoveryMetadataRequest for log 13835058055282163711 shard 1 epochs [2,3] purge_to 3 received reply from all fully authoritative nodes in thenodeset but still unable to make a decision. It is likely that some epoch has been recovered by non-authoritative recovery.
I0917 02:14:33.308648    5200 [ld:srv:WG0] GetEpochRecoveryMetadataRequest.cpp:199] buildEpochRecoveryStateMap() GetEpochRecoveryMetadataRequest for log 13835058055282163711 shard 1 epochs [2,3] purge_to 3 - All nodes responded but there is no valid response for epoch:2
I0917 02:14:33.310158    5200 [ld:srv:WG0] GetEpochRecoveryMetadataRequest.cpp:199] buildEpochRecoveryStateMap() GetEpochRecoveryMetadataRequest for log 13835058055282163711 shard 1 epochs [2,3] purge_to 3 - All nodes responded but there is no valid response for epoch:3
W0917 02:14:33.312726    5228 [ld:srv:WG4] Worker.cpp:1000] onStoppedRunning() skipped at least 10 log entries
W0917 02:14:33.313524    5228 [ld:srv:WG4] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.020s, source: [Message sent/received: CLEANED]
I0917 02:14:33.313773    5200 [ld:srv:WG0] PurgeUncleanEpochs.cpp:387] onGetEpochRecoveryMetadataComplete() GetEpochRecoveryMetadataRequest for log 13835058055282163711 did not get a valid response for epoch 3. Skipping purging for this epoch
I0917 02:14:33.357638    5200 [ld:srv:WG0] PurgeUncleanEpochs.cpp:387] onGetEpochRecoveryMetadataComplete() GetEpochRecoveryMetadataRequest for log 13835058055282163711 did not get a valid response for epoch 2. Skipping purging for this epoch
I0917 02:14:33.320938    5228 [ld:srv:WG4] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 13835058055282163708, next_epoch:9
W0917 02:14:33.358886    5200 [ld:srv:WG0] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.045s, source: [Request: WORKER_CALLBACK_HELPER]
I0917 02:14:33.365955    5200 [ld:srv:WG0] EpochRecovery.cpp:566] startMutations() Log 4611686018427387900 epoch 4 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:14:33.371759    5228 [ld:srv:WG4] PurgeUncleanEpochs.cpp:99] getPurgeEpochs() log 4611686018427387899 has already been cleaned. No purge needed. last clean epoch: 4, purge to: 4.
I0917 02:14:33.381430    5200 [ld:srv:WG0] Mutator.cpp:400] finalize() Mutator 4611686018427387900e4n1 completed (OK). Trace: W1{N4:S0,N2:S0};X:N4:S0:K;X:N2:S0:K;Y1:N4:S0:K;Y1:N2:S0:K;done:can_replicate:OK;
I0917 02:14:33.592516    5203 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 13835058055282163711 epoch 5 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163711 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163711 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
I0917 02:14:33.626404    5200 [ld:srv:WG0] EpochRecovery.cpp:566] startMutations() Log 4611686018427387900 epoch 5 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:14:33.639206    5200 [ld:srv:WG0] Mutator.cpp:400] finalize() Mutator 4611686018427387900e5n1 completed (OK). Trace: W1{N2:S0,N4:S0};X:N2:S0:K;X:N4:S0:K;Y1:N2:S0:K;Y1:N4:S0:K;done:can_replicate:OK;
I0917 02:14:33.707950    5200 [ld:srv:WG0] PurgeUncleanEpochs.cpp:592] complete() skipped at least 10 log entries
I0917 02:14:33.708501    5200 [ld:srv:WG0] PurgeUncleanEpochs.cpp:592] complete() Successfully purged epochs [4, 4] for log 13835058055282163711.
I0917 02:14:33.719577    5203 [ld:srv:WG1] MetaDataLogWriter.cpp:308] runAppender() MetaDataLogWriter cannnot accept a new metadata append due to an existing write / recovery in progress. Failing the append with E::NOBUF. Some info: logid: 4611686018427387903, state: 1, current_epoch: 9, last_writer_epoch: 0, recovery_only: true metadata_last_released: LSN_INVALID.
I0917 02:14:33.728979    5203 [ld:srv:WG1] MetaDataLogWriter.cpp:308] runAppender() MetaDataLogWriter cannnot accept a new metadata append due to an existing write / recovery in progress. Failing the append with E::NOBUF. Some info: logid: 4611686018427387903, state: 1, current_epoch: 9, last_writer_epoch: 0, recovery_only: true metadata_last_released: LSN_INVALID.
I0917 02:14:33.760685    5225 [ld:srv:WG3] GetEpochRecoveryMetadataRequest.cpp:226] buildEpochRecoveryStateMap() GetEpochRecoveryMetadataRequest for log 13835058055282163706 shard 0 epochs [2,5] purge_to 5 got at least one reply with E::EMPTY, for the epoch 2.
I0917 02:14:33.762259    5225 [ld:srv:WG3] GetEpochRecoveryMetadataRequest.cpp:226] buildEpochRecoveryStateMap() GetEpochRecoveryMetadataRequest for log 13835058055282163706 shard 0 epochs [2,5] purge_to 5 got at least one reply with E::EMPTY, for the epoch 3.
I0917 02:14:33.763201    5225 [ld:srv:WG3] GetEpochRecoveryMetadataRequest.cpp:226] buildEpochRecoveryStateMap() GetEpochRecoveryMetadataRequest for log 13835058055282163706 shard 0 epochs [2,5] purge_to 5 got at least one reply with E::EMPTY, for the epoch 4.
I0917 02:14:33.763910    5225 [ld:srv:WG3] GetEpochRecoveryMetadataRequest.cpp:226] buildEpochRecoveryStateMap() GetEpochRecoveryMetadataRequest for log 13835058055282163706 shard 0 epochs [2,5] purge_to 5 got at least one reply with E::EMPTY, for the epoch 5.
I0917 02:14:33.782124    5225 [ld:srv:WG3] PurgeUncleanEpochs.cpp:401] onGetEpochRecoveryMetadataComplete() Nothing to purge. Epoch is empty locally and EpochRecoveryMetadata consensus is also EMPTY for log: 13835058055282163706,epoch:5
W0917 02:14:33.827008    5203 [ld:srv:WG1] Worker.cpp:1000] onStoppedRunning() skipped at least 3 log entries
W0917 02:14:33.827777    5203 [ld:srv:WG1] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.012s, source: [Message sent/received: CLEANED]
I0917 02:14:33.831135    5203 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 13835058055282163711 epoch 6 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163711 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163711 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
I0917 02:14:33.845190    5225 [ld:srv:WG3] PurgeUncleanEpochs.cpp:592] complete() skipped at least 1 log entries
I0917 02:14:33.846410    5225 [ld:srv:WG3] PurgeUncleanEpochs.cpp:592] complete() Successfully purged epochs [1, 1] for log 4611686018427387898.
W0917 02:14:33.862024    5200 [ld:srv:WG0] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.010s, source: [Message sent/received: CLEANED]
I0917 02:14:33.865431    5200 [ld:srv:WG0] EpochRecovery.cpp:566] startMutations() Log 4611686018427387900 epoch 6 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:14:33.875624    5200 [ld:srv:WG0] Mutator.cpp:400] finalize() Mutator 4611686018427387900e6n1 completed (OK). Trace: W1{N4:S0,N3:S0};X:N4:S0:K;X:N3:S0:K;Y1:N4:S0:K;Y1:N3:S0:K;done:can_replicate:OK;
I0917 02:14:34.076347    5225 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:567] continueExecution() skipped at least 10 log entries
I0917 02:14:34.077161    5225 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387900, rqid:94489280515, ctx:store-message) from WG3:C30 (127.0.0.1:51612), but its data log sequencer's state is Recovery Incomplete
I0917 02:14:34.085209    5203 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 13835058055282163711 epoch 7 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163711 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163711 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
I0917 02:14:34.085887    5200 [ld:srv:WG0] EpochRecovery.cpp:566] startMutations() Log 4611686018427387900 epoch 7 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:14:34.090029    5225 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387900, rqid:77309411355, ctx:store-message) from WG3:C13 (127.0.0.1:51472), but its data log sequencer's state is Recovery Incomplete
I0917 02:14:34.097116    5254 [ld:srv:WB3] ClientReadStream.cpp:4571] operator()() Requested epoch metadata for log 4611686018427387899 epoch 2 is not available. Highest epoch known is 1.
W0917 02:14:34.098105    5254 [ld:srv:WB3] ClientReadStream.cpp:2476] updateCurrentMetaData() Existing epoch metadata has changed for log 4611686018427387899 on epoch 1, check metadata log!
I0917 02:14:34.103283    5254 [ld:srv:WB3] ClientReadStream.cpp:4571] operator()() Requested epoch metadata for log 4611686018427387899 epoch 3 is not available. Highest epoch known is 1.
W0917 02:14:34.106968    5254 [ld:srv:WB3] ClientReadStream.cpp:2476] updateCurrentMetaData() Existing epoch metadata has changed for log 4611686018427387899 on epoch 1, check metadata log!
I0917 02:14:34.103447    5200 [ld:srv:WG0] Mutator.cpp:400] finalize() Mutator 4611686018427387900e7n1 completed (OK). Trace: W1{N1:S0,N2:S0};X:N1:S0:K;X:N2:S0:K;Y1:N2:S0:K;Y1:N1:S0:K;done:can_replicate:OK;
I0917 02:14:34.122533    5254 [ld:srv:WB3] ClientReadStream.cpp:4571] operator()() Requested epoch metadata for log 4611686018427387899 epoch 6 is not available. Highest epoch known is 1.
W0917 02:14:34.126699    5254 [ld:srv:WB3] ClientReadStream.cpp:2476] updateCurrentMetaData() Existing epoch metadata has changed for log 4611686018427387899 on epoch 1, check metadata log!
I0917 02:14:34.143317    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:1052] onReachedDeltaLogTailLSN() [maintenancelog] Reached tail of delta log. deliver_while_replaying:0, stop_at_tail_:0
I0917 02:14:34.144469    5254 [ld:srv:WB3] ClusterMaintenanceStateMachine.cpp:112] onUpdate() [maintenancelog] Published new ClusterMaintenanceState version (1)
I0917 02:14:34.307782    5200 [ld:srv:WG0] EpochRecovery.cpp:566] startMutations() Log 4611686018427387900 epoch 8 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387900 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:14:34.311347    5203 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 13835058055282163711 epoch 8 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163711 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163711 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
I0917 02:14:34.317744    5200 [ld:srv:WG0] Mutator.cpp:400] finalize() Mutator 4611686018427387900e8n1 completed (OK). Trace: W1{N1:S0,N3:S0};X:N1:S0:K;X:N3:S0:K;Y1:N1:S0:K;Y1:N3:S0:K;done:can_replicate:OK;
I0917 02:14:34.374416    5200 [ld:srv:WG0] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387900, rqid:38654705672, ctx:store-message) from WG0:C5 (127.0.0.1:51394), but its data log sequencer's state is Recovery Incomplete
I0917 02:14:34.462055    5225 [ld:srv:WG3] PurgeUncleanEpochs.cpp:592] complete() skipped at least 10 log entries
I0917 02:14:34.462945    5225 [ld:srv:WG3] PurgeUncleanEpochs.cpp:592] complete() Successfully purged epochs [4, 4] for log 4611686018427387898.
I0917 02:14:34.503951    5200 [ld:srv:WG0] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 4611686018427387900, next_epoch:9
I0917 02:14:34.542758    5203 [ld:srv:WG1] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 13835058055282163711, next_epoch:9
I0917 02:14:34.588030    5203 [ld:srv:WG1] GET_SEQ_STATE_Message.cpp:238] getSequencer() Sequencer for log:4611686018427387901 is absent, state=UNAVAILABLE, cur_epoch=0, attempting to activate it upon receiving GSS (rqid:21474836491, ctx:rsm, min_epoch:none) from WG1:C36 (127.0.0.1:51678)
I0917 02:14:34.607360    5203 [ld:srv:WG1] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387901, rqid:21474836491, ctx:rsm) from WG1:C36 (127.0.0.1:51678), but its data log sequencer's state is ACTIVATING
I0917 02:14:34.608797    5203 [ld:srv:WG1] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 4611686018427387901 epoch 6.
I0917 02:14:34.609930    5203 [ld:srv:WG1] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 4611686018427387901: {[E:1 (at 2021-09-17 02:14:34.592) since:1 (at 2021-09-17 02:10:15.609) R:[node: 2] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]], Until:6}.
I0917 02:14:34.610733    5203 [ld:srv:WG1] AllSequencers.cpp:527] onEpochMetaDataFromEpochStore() Activated a sequencer for log 4611686018427387901 (reason: GET_SEQ_STATE) with epoch 6, metadata: [E:6 (at 2021-09-17 02:14:34.592) since:1 (at 2021-09-17 02:10:15.609) R:[node: 2] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]]. Activation Result: RECOVERY.
I0917 02:14:34.610763    5224 [ld:srv:WG2] PeriodicReleases.cpp:416] execute() Start broadcasting releases for log 4611686018427387901.
I0917 02:14:34.614892    5203 [ld:srv:WG1] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 13835058055282163709 epoch 6.
I0917 02:14:34.615164    5224 [ld:srv:WG2] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 4611686018427387901 with next_epoch 6
I0917 02:14:34.615695    5203 [ld:srv:WG1] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 13835058055282163709: {[E:1 (at -inf) since:1 (at -inf) R:[node: 3] V:2 flags: params:{target:0 seed:0 signature:0} N:{N0:S1,N1:S1,N2:S1,N3:S1,N4:S1} W:[]], Until:6}.
I0917 02:14:34.617823    5203 [ld:srv:WG1] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 13835058055282163709 with next_epoch 6
I0917 02:14:34.621566    5224 [ld:srv:WG2] LogRecoveryRequest.cpp:1018] sealLog() Starting recovery of epochs [2, 5] of log 4611686018427387901
I0917 02:14:34.627482    5203 [ld:srv:WG1] LogRecoveryRequest.cpp:1018] sealLog() Starting recovery of epochs [2, 5] of log 13835058055282163709
I0917 02:14:34.740408    5225 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:567] continueExecution() Received a GET_SEQ_STATE message (log:4611686018427387901, rqid:68719476751, ctx:start-message) from WG3:C38 (127.0.0.1:51698), but its data log sequencer's state is Recovery Incomplete
I0917 02:14:34.741379    5203 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 13835058055282163709 epoch 2 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163709 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163709 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
I0917 02:14:34.742976    5224 [ld:srv:WG2] EpochRecovery.cpp:566] startMutations() Log 4611686018427387901 epoch 2 final digest before mutation: consensus LNG 1, start esn: 2, 1 entries, first esn: 2, last esn: 2, bridge esn: 3, tail esn: 2, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 2021-09-17 02:10:19.558, last timestamp from seals: 2021-09-17 02:10:19.558. Tail record before this epoch: [L:4611686018427387901 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:4611686018427387901 N:e2n1 T:1631844619558 OM:{246:0} F:528], final tail record: [L:4611686018427387901 N:e2n2 T:1631844671944 AOM:{246:274} F:16]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
I0917 02:14:34.749744    5224 [ld:srv:WG2] Mutator.cpp:400] finalize() Mutator 4611686018427387901e2n2 completed (OK). Trace: W1{N1:S1,N3:S1};X:N1:S1:K;X:N3:S1:K;Y1:N1:S1:K;Y1:N3:S1:K;done:can_replicate:OK;
I0917 02:14:34.755581    5224 [ld:srv:WG2] Mutator.cpp:400] finalize() Mutator 4611686018427387901e2n3 completed (OK). Trace: W1{N0:S1,N4:S1};X:N0:S1:K;X:N4:S1:K;Y1:N4:S1:K;Y1:N0:S1:K;done:can_replicate:OK;
I0917 02:14:34.851302    5200 [ld:srv:WG0] PurgeUncleanEpochs.cpp:592] complete() skipped at least 2 log entries
I0917 02:14:34.852120    5200 [ld:srv:WG0] PurgeUncleanEpochs.cpp:592] complete() Successfully purged epochs [1, 1] for log 13835058055282163709.
I0917 02:14:34.963800    5224 [ld:srv:WG2] EpochRecovery.cpp:566] startMutations() Log 4611686018427387901 epoch 3 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 2021-09-17 02:11:11.944, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387901 N:e2n2 T:1631844671944 AOM:{246:274} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387901 N:e2n2 T:1631844671944 AOM:{246:274} F:16]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
W0917 02:14:34.969010    5203 [ld:srv:WG1] Worker.cpp:1000] onStoppedRunning() skipped at least 8 log entries
W0917 02:14:34.970361    5203 [ld:srv:WG1] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.015s, source: [Message sent/received: CLEANED]
I0917 02:14:34.972186    5224 [ld:srv:WG2] Mutator.cpp:400] finalize() Mutator 4611686018427387901e3n1 completed (OK). Trace: W1{N1:S1,N4:S1};X:N1:S1:K;X:N4:S1:K;Y1:N1:S1:K;Y1:N4:S1:K;done:can_replicate:OK;
I0917 02:14:34.974273    5203 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 13835058055282163709 epoch 3 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163709 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163709 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
W0917 02:14:35.192055    5224 [ld:srv:WG2] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.011s, source: [Message sent/received: CLEANED]
I0917 02:14:35.197491    5224 [ld:srv:WG2] EpochRecovery.cpp:566] startMutations() Log 4611686018427387901 epoch 4 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 2021-09-17 02:11:11.944, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387901 N:e2n2 T:1631844671944 AOM:{246:274} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387901 N:e2n2 T:1631844671944 AOM:{246:274} F:16]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
I0917 02:14:35.204279    5224 [ld:srv:WG2] Mutator.cpp:400] finalize() Mutator 4611686018427387901e4n1 completed (OK). Trace: W1{N1:S1,N0:S1};X:N1:S1:K;X:N0:S1:K;Y1:N0:S1:K;Y1:N1:S1:K;done:can_replicate:OK;
I0917 02:14:35.204843    5203 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 13835058055282163709 epoch 4 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163709 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163709 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
I0917 02:14:35.422083    5224 [ld:srv:WG2] EpochRecovery.cpp:566] startMutations() Log 4611686018427387901 epoch 5 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 2021-09-17 02:11:11.944, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387901 N:e2n2 T:1631844671944 AOM:{246:274} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387901 N:e2n2 T:1631844671944 AOM:{246:274} F:16]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
I0917 02:14:35.430767    5224 [ld:srv:WG2] Mutator.cpp:400] finalize() Mutator 4611686018427387901e5n1 completed (OK). Trace: W1{N3:S1,N0:S1};X:N3:S1:K;X:N0:S1:K;Y1:N0:S1:K;Y1:N3:S1:K;done:can_replicate:OK;
I0917 02:14:35.432122    5203 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 13835058055282163709 epoch 5 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S1, N1:S1, N2:S1, N3:S1, N4:S1}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163709 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163709 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S1,N1:S1,N2:S1,N3:S1,N4:S1 MUTATION}.
I0917 02:14:35.651038    5224 [ld:srv:WG2] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 4611686018427387901, next_epoch:6
I0917 02:14:35.668704    5203 [ld:srv:WG1] LogRecoveryRequest.cpp:1463] complete() Log recovery completed for log 13835058055282163709, next_epoch:6
W0917 02:14:35.714103    5225 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:812] shouldRedirectOrFail() No sequencer node found for log:25 (status=NOTFOUND), replying with E::NOSEQUENCER.
W0917 02:14:35.715203    5225 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:812] shouldRedirectOrFail() No sequencer node found for log:37 (status=NOTFOUND), replying with E::NOSEQUENCER.
W0917 02:14:35.717467    5225 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:812] shouldRedirectOrFail() No sequencer node found for log:45 (status=NOTFOUND), replying with E::NOSEQUENCER.
W0917 02:14:35.719796    5225 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:812] shouldRedirectOrFail() No sequencer node found for log:49 (status=NOTFOUND), replying with E::NOSEQUENCER.
W0917 02:14:35.721673    5225 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:812] shouldRedirectOrFail() No sequencer node found for log:94 (status=NOTFOUND), replying with E::NOSEQUENCER.
I0917 02:14:35.729514    5225 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:238] getSequencer() Sequencer for log:4611686018427387902 is absent, state=UNAVAILABLE, cur_epoch=0, attempting to activate it upon receiving GSS (rqid:21474836604, ctx:historical-metadata, min_epoch:none) from WG3:C44 (127.0.0.1:51732)
W0917 02:14:35.757456    5225 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:812] shouldRedirectOrFail() skipped at least 10 log entries
W0917 02:14:35.759641    5225 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:812] shouldRedirectOrFail() No sequencer node found for log:17 (status=NOTFOUND), replying with E::NOSEQUENCER.
W0917 02:14:35.761476    5225 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:812] shouldRedirectOrFail() No sequencer node found for log:53 (status=NOTFOUND), replying with E::NOSEQUENCER.
W0917 02:14:35.763757    5225 [ld:srv:WG3] GET_SEQ_STATE_Message.cpp:812] shouldRedirectOrFail() No sequencer node found for log:91 (status=NOTFOUND), replying with E::NOSEQUENCER.
I0917 02:14:35.765201    5228 [ld:srv:WG4] Appender.cpp:1099] onTimeout() Appender 4611686018427387903e9n1 hit a STORE timeout(10ms), wave 1, recipient set: {N3:S1: STORED, N2:S1: OUTSTANDING, N1:S1: OUTSTANDING}
I0917 02:14:35.765653    5225 [ld:srv:WG3] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 4611686018427387902 epoch 6.
I0917 02:14:35.769137    5225 [ld:srv:WG3] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 4611686018427387902: {[E:1 (at 2021-09-17 02:14:35.749) since:1 (at 2021-09-17 02:10:15.636) R:[node: 3] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]], Until:6}.
I0917 02:14:35.772148    5200 [ld:srv:WG0] PeriodicReleases.cpp:416] execute() Start broadcasting releases for log 4611686018427387902.
I0917 02:14:35.772282    5225 [ld:srv:WG3] AllSequencers.cpp:527] onEpochMetaDataFromEpochStore() Activated a sequencer for log 4611686018427387902 (reason: GET_SEQ_STATE) with epoch 6, metadata: [E:6 (at 2021-09-17 02:14:35.749) since:1 (at 2021-09-17 02:10:15.636) R:[node: 3] V:2 flags:WRITTEN_IN_METADATALOG params:{target:127 seed:0 signature:d4a0e78fab29e4f6} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]]. Activation Result: RECOVERY.
I0917 02:14:35.775285    5203 [ld:srv:WG1] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 4611686018427387902 with next_epoch 6
I0917 02:14:35.775964    5224 [ld:srv:WG2] Sequencer.cpp:126] completeActivationWithMetaData() Activating sequencer for log 13835058055282163710 epoch 6.
I0917 02:14:35.777120    5224 [ld:srv:WG2] Sequencer.cpp:242] completeActivationWithMetaData() Historical epoch metadata for log 13835058055282163710: {[E:1 (at -inf) since:1 (at -inf) R:[node: 3] V:2 flags: params:{target:0 seed:0 signature:0} N:{N0:S0,N1:S0,N2:S0,N3:S0,N4:S0} W:[]], Until:6}.
I0917 02:14:35.779926    5224 [ld:srv:WG2] LogRecoveryRequest.cpp:390] start() Started LogRecoveryRequest of log 13835058055282163710 with next_epoch 6
I0917 02:14:35.780752    5254 [ld:srv:WB3] EventLogStateMachine.cpp:171] applyDelta() Applying delta ts=2021-09-17 02:14:35.754, e9n1: SHARD_NEEDS_REBUILD(nodeIdx=2, shardIdx=1, source=N2, details=RebuildingCoordinator, flags=TIME_RANGED|FILTER_RELOCATE_SHARDS)[A:{{[2021-09-17 02:10:00.557,2021-09-17 02:10:30.557]}}]
I0917 02:14:35.785314    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:841] onDeltaRecord() [eventlog] Applied delta record with lsn=e9n1 ts=2021-09-17 02:14:35.754
I0917 02:14:35.786318    5254 [ld:srv:WB3] EventLogStateMachine.cpp:93] publishRebuildingSet() [eventlog] Published new EventLogRebuildingSet version (38654705665)
I0917 02:14:35.788484    5203 [ld:srv:WG1] LogRecoveryRequest.cpp:1018] sealLog() Starting recovery of epochs [2, 5] of log 4611686018427387902
I0917 02:14:35.792571    5224 [ld:srv:WG2] LogRecoveryRequest.cpp:1018] sealLog() Starting recovery of epochs [2, 5] of log 13835058055282163710
I0917 02:14:35.808628    5242 [ld:srv:WB1] LogBasedRSMSnapshotStore.cpp:298] onGotLastReleased() Returning EMPTY for log:4611686018427387900, last_released_real_lsn:LSN_INVALID
I0917 02:14:35.809863    5242 [ld:srv:WB1] ReplicatedStateMachine-inl.h:83] operator()() [logsconfig] getSnapshot()'s callback. st:EMPTY, snapshot blob size:0, attrs:(base_ver:LSN_INVALID, ts:0)
I0917 02:14:35.810872    5242 [ld:srv:WB1] ReplicatedStateMachine-inl.h:667] onBaseSnapshotRetrieved() [logsconfig] Base snapshot has version:e0n1, delta_log_read_ptr:LSN_INVALID
I0917 02:14:35.811772    5242 [ld:srv:WB1] ReplicatedStateMachine-inl.h:684] getDeltaLogTailLSN() [logsconfig] Retrieving tail lsn of delta log...
I0917 02:14:35.820461    5242 [ld:srv:WB1] ReplicatedStateMachine-inl.h:712] onGotDeltaLogTailLSN() [logsconfig] Tail lsn of delta log is e6n0
I0917 02:14:35.823690    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n1 ts=2021-09-17 02:14:35.812 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:35.831519    5254 [ld:srv:WB3] EventLogStateMachine.cpp:171] applyDelta() Applying delta ts=2021-09-17 02:14:35.823, e9n2: SHARD_NEEDS_REBUILD(nodeIdx=2, shardIdx=0, source=N2, details=RebuildingCoordinator, flags=TIME_RANGED|FILTER_RELOCATE_SHARDS)[A:{{[2021-09-17 02:10:00.550,2021-09-17 02:10:30.550]}}]
I0917 02:14:35.835968    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:841] onDeltaRecord() [eventlog] Applied delta record with lsn=e9n2 ts=2021-09-17 02:14:35.823
I0917 02:14:35.837474    5254 [ld:srv:WB3] EventLogStateMachine.cpp:93] publishRebuildingSet() [eventlog] Published new EventLogRebuildingSet version (38654705666)
I0917 02:14:35.847509    5242 [ld:srv:WB1] ClientReadStream.cpp:4571] operator()() Requested epoch metadata for log 4611686018427387901 epoch 2 is not available. Highest epoch known is 1.
I0917 02:14:35.849789    5242 [ld:srv:WB1] ReplicatedStateMachine-inl.h:841] onDeltaRecord() [logsconfig] Applied delta record with lsn=e2n1 ts=2021-09-17 02:10:19.558
I0917 02:14:35.850883    5242 [ld:srv:WB1] ReplicatedStateMachine-inl.h:841] onDeltaRecord() [logsconfig] Applied delta record with lsn=e2n2 ts=2021-09-17 02:11:11.944
I0917 02:14:35.855841    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n2 ts=2021-09-17 02:14:35.840 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:35.868424    5235 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N2 changed from STARTING to FULLY_STARTED
I0917 02:14:35.876880    5242 [ld:srv:WB1] ReplicatedStateMachine-inl.h:1052] onReachedDeltaLogTailLSN() [logsconfig] Reached tail of delta log. deliver_while_replaying:0, stop_at_tail_:0
I0917 02:14:35.879419    5242 [ld:srv:WB1] LogsConfigManager.cpp:193] operator()() Received update from LogsConfigStateMachine, version 8589934594
I0917 02:14:35.881370    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n3 ts=2021-09-17 02:14:35.869 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:35.884028    5242 [ld:srv:WB1] AllSequencers.cpp:981] noteConfigurationChanged() Acquiring lock for sequencer map took 0ms
I0917 02:14:35.893698    5200 [ld:srv:WG0] Appender.cpp:1099] onTimeout() Appender 4611686018427387903e9n3 hit a STORE timeout(10ms), wave 1, recipient set: {N1:S1: OUTSTANDING, N4:S1: OUTSTANDING, N3:S1: OUTSTANDING}
I0917 02:14:35.895833    5242 [ld:srv:WB1] UpdateableConfigTmpl.h:442] updateIfEqual() Config updated. Timings: Acquiring lock: 0ms, applying overrides: 0ms, hook execution: 0ms, config update: 0ms, notifications: 13ms
I0917 02:14:35.896035    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1740] publishDirtyShards() Publishing dirty shards.
I0917 02:14:35.900315    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1036] writeRemoveMaintenance() Remove maintenance request for shard 1, reason=Node came back with data intact
I0917 02:14:35.898821    5242 [ld:srv:WB1] LogsConfigManager.cpp:331] updateLogsConfig() Published new LogsConfig (fully loaded? yes) version (8589934594) from LogsConfigManager
I0917 02:14:35.901994    5254 [ld:srv:WB3] EventLogWriter.cpp:86] writeEvent() (Queue size: 0) Enqueueing event SHARD_NEEDS_REBUILD(nodeIdx=0, shardIdx=1, source=N0, details=RebuildingCoordinator, flags=TIME_RANGED|FILTER_RELOCATE_SHARDS)[A:{{[2021-09-17 02:09:59.858,2021-09-17 02:10:29.858]}}]
I0917 02:14:35.904478    5254 [ld:srv:WB3] EventLogWriter.cpp:50] writeNextEventInQueue() (Queue size: 1) Writing event SHARD_NEEDS_REBUILD(nodeIdx=0, shardIdx=1, source=N0, details=RebuildingCoordinator, flags=TIME_RANGED|FILTER_RELOCATE_SHARDS)[A:{{[2021-09-17 02:09:59.858,2021-09-17 02:10:29.858]}}]
I0917 02:14:35.905311    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1036] writeRemoveMaintenance() Remove maintenance request for shard 0, reason=Node came back with data intact
I0917 02:14:35.905690    5224 [ld:srv:WG2] EpochRecovery.cpp:566] startMutations() Log 13835058055282163710 epoch 2 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163710 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163710 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:14:35.906511    5254 [ld:srv:WB3] EventLogWriter.cpp:86] writeEvent() (Queue size: 1) Enqueueing event SHARD_NEEDS_REBUILD(nodeIdx=0, shardIdx=0, source=N0, details=RebuildingCoordinator, flags=TIME_RANGED|FILTER_RELOCATE_SHARDS)[A:{{[2021-09-17 02:09:59.859,2021-09-17 02:10:29.859]}}]
I0917 02:14:35.909914    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:777] scheduleRestartForShard() Scheduling a restart for shard 0 after 1ms
I0917 02:14:35.908812    5203 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 4611686018427387902 epoch 2 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387902 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387902 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:14:35.911713    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:777] scheduleRestartForShard() Scheduling a restart for shard 1 after 1ms
I0917 02:14:35.914566    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:2064] subscribeToEventLog() Subscribed to EventLog
W0917 02:14:35.915691    5254 [ld:srv:WB3] Worker.cpp:1000] onStoppedRunning() skipped at least 10 log entries
W0917 02:14:35.921952    5254 [ld:srv:WB3] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.020s, source: [Request: LOGS_CONFIG_UPDATED]
I0917 02:14:35.924794    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:683] trySlideGlobalWindow() Moving global window from +inf to -inf for shard 0 and rebuilding set N2:S0[A:{{[2021-09-17 02:10:00.550,2021-09-17 02:10:30.550]}}]
I0917 02:14:35.926063    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:683] trySlideGlobalWindow() Moving global window from +inf to -inf for shard 1 and rebuilding set N2:S1[A:{{[2021-09-17 02:10:00.557,2021-09-17 02:10:30.557]}}]
I0917 02:14:35.928254    5254 [ld:srv:WB3] EventLogStateMachine.cpp:171] applyDelta() Applying delta ts=2021-09-17 02:14:35.881, e9n3: SHARD_NEEDS_REBUILD(nodeIdx=1, shardIdx=1, source=N1, details=RebuildingCoordinator, flags=TIME_RANGED|FILTER_RELOCATE_SHARDS)[A:{{[2021-09-17 02:10:00.088,2021-09-17 02:10:30.088]}}]
I0917 02:14:35.933006    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:841] onDeltaRecord() [eventlog] Applied delta record with lsn=e9n3 ts=2021-09-17 02:14:35.881
I0917 02:14:35.936052    5254 [ld:srv:WB3] EventLogStateMachine.cpp:93] publishRebuildingSet() [eventlog] Published new EventLogRebuildingSet version (38654705667)
W0917 02:14:35.936766    5203 [ld:srv:WG1] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.010s, source: [Message sent/received: MUTATED]
I0917 02:14:35.939051    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:777] scheduleRestartForShard() Scheduling a restart for shard 1 after 1ms
I0917 02:14:35.947628    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:683] trySlideGlobalWindow() Moving global window from +inf to -inf for shard 1 and rebuilding set N1:S1[A:{{[2021-09-17 02:10:00.088,2021-09-17 02:10:30.088]}}],N2:S1[A:{{[2021-09-17 02:10:00.557,2021-09-17 02:10:30.557]}}]
I0917 02:14:35.950682    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n4 ts=2021-09-17 02:14:35.881 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:35.952460    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n5 ts=2021-09-17 02:14:35.892 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:35.957035    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:1179] operator()() [eventlog] Successfully wrote delta with lsn e9n5
I0917 02:14:35.958297    5254 [ld:srv:WB3] EventLogWriter.cpp:67] operator()() Wrote record with lsn e9n5
I0917 02:14:35.963306    5254 [ld:srv:WB3] EventLogWriter.cpp:50] writeNextEventInQueue() (Queue size: 1) Writing event SHARD_NEEDS_REBUILD(nodeIdx=0, shardIdx=0, source=N0, details=RebuildingCoordinator, flags=TIME_RANGED|FILTER_RELOCATE_SHARDS)[A:{{[2021-09-17 02:09:59.859,2021-09-17 02:10:29.859]}}]
I0917 02:14:35.965424    5254 [ld:srv:WB3] EventLogStateMachine.cpp:171] applyDelta() Applying delta ts=2021-09-17 02:14:35.929, e9n4: SHARD_NEEDS_REBUILD(nodeIdx=1, shardIdx=0, source=N1, details=RebuildingCoordinator, flags=TIME_RANGED|FILTER_RELOCATE_SHARDS)[A:{{[2021-09-17 02:10:00.092,2021-09-17 02:10:30.092]}}]
I0917 02:14:35.971466    5235 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N0 changed from STARTING to FULLY_STARTED
I0917 02:14:35.979329    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:841] onDeltaRecord() [eventlog] Applied delta record with lsn=e9n4 ts=2021-09-17 02:14:35.929
I0917 02:14:35.980710    5235 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N1 changed from STARTING to FULLY_STARTED
I0917 02:14:35.982262    5254 [ld:srv:WB3] EventLogStateMachine.cpp:93] publishRebuildingSet() [eventlog] Published new EventLogRebuildingSet version (38654705668)
W0917 02:14:35.984693    5235 [ld:srv:WF0] Worker.cpp:1000] onStoppedRunning() skipped at least 1 log entries
W0917 02:14:35.991247    5235 [ld:srv:WF0] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.013s, source: [Message sent/received: GOSSIP]
I0917 02:14:35.985306    5203 [ld:srv:WG1] Mutator.cpp:400] finalize() Mutator 4611686018427387902e2n1 completed (OK). Trace: W1{N1:S0,N0:S0,N3:S0};X:N1:S0:K;X:N0:S0:K;X:N3:S0:K;Y1:N1:S0:K;Y1:N0:S0:K;Y1:N3:S0:K;done:can_replicate:OK;
I0917 02:14:35.988482    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:777] scheduleRestartForShard() Scheduling a restart for shard 0 after 1ms
W0917 02:14:35.996301    5254 [ld:srv:WB3] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.031s, source: [Message sent/received: RECORD]
I0917 02:14:35.997949    5254 [ld:srv:WB3] EventLogStateMachine.cpp:171] applyDelta() Applying delta ts=2021-09-17 02:14:35.933, e9n5: SHARD_NEEDS_REBUILD(nodeIdx=0, shardIdx=1, source=N0, details=RebuildingCoordinator, flags=TIME_RANGED|FILTER_RELOCATE_SHARDS)[A:{{[2021-09-17 02:09:59.858,2021-09-17 02:10:29.858]}}]
I0917 02:14:35.999115    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:841] onDeltaRecord() [eventlog] Applied delta record with lsn=e9n5 ts=2021-09-17 02:14:35.933
I0917 02:14:36.003314    5254 [ld:srv:WB3] EventLogStateMachine.cpp:93] publishRebuildingSet() [eventlog] Published new EventLogRebuildingSet version (38654705669)
I0917 02:14:36.004896    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:777] scheduleRestartForShard() Scheduling a restart for shard 1 after 1ms
I0917 02:14:36.005996    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:683] trySlideGlobalWindow() Moving global window from +inf to -inf for shard 0 and rebuilding set N1:S0[A:{{[2021-09-17 02:10:00.092,2021-09-17 02:10:30.092]}}],N2:S0[A:{{[2021-09-17 02:10:00.550,2021-09-17 02:10:30.550]}}]
I0917 02:14:36.007492    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:1179] operator()() [maintenancelog] Successfully wrote delta with lsn e8n6
I0917 02:14:36.008880    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:1179] operator()() [eventlog] Successfully wrote delta with lsn e9n6
I0917 02:14:36.010998    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1036] writeRemoveMaintenance() Remove maintenance request for shard 1, reason=Shard came back with data intact
I0917 02:14:36.014828    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1869] abortForMyShard() Request to abort rebuilding of my shard 1 because: data is intact. But shard is dirty. Downgrading to time ranged rebuild.
I0917 02:14:36.011917    5200 [ld:srv:WG0] PurgeUncleanEpochs.cpp:592] complete() skipped at least 8 log entries
I0917 02:14:36.017102    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1871] abortForMyShard() EventLogRebuildingSet NodeInfo: [A:{{[2021-09-17 02:09:59.858,2021-09-17 02:10:29.858]}}]
I0917 02:14:36.019795    5200 [ld:srv:WG0] PurgeUncleanEpochs.cpp:592] complete() Successfully purged epochs [1, 1] for log 13835058055282163710.
I0917 02:14:36.021233    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1872] abortForMyShard() Local dirty ranges: [A:{{[2021-09-17 02:09:59.858,2021-09-17 02:10:29.858]}}]
I0917 02:14:36.024149    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1883] abortForMyShard() Dirty ranges for my shard 1 successfully published
I0917 02:14:36.029801    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:472] noteRangesPublished() Noting dirty ranges published for shard 1
I0917 02:14:36.032737    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:683] trySlideGlobalWindow() Moving global window from +inf to -inf for shard 1 and rebuilding set N0:S1[A:{{[2021-09-17 02:09:59.858,2021-09-17 02:10:29.858]}}],N1:S1[A:{{[2021-09-17 02:10:00.088,2021-09-17 02:10:30.088]}}],N2:S1[A:{{[2021-09-17 02:10:00.557,2021-09-17 02:10:30.557]}}]
I0917 02:14:36.034117    5254 [ld:srv:WB3] EventLogStateMachine.cpp:171] applyDelta() Applying delta ts=2021-09-17 02:14:35.965, e9n6: SHARD_NEEDS_REBUILD(nodeIdx=0, shardIdx=0, source=N0, details=RebuildingCoordinator, flags=TIME_RANGED|FILTER_RELOCATE_SHARDS)[A:{{[2021-09-17 02:09:59.859,2021-09-17 02:10:29.859]}}]
I0917 02:14:36.035330    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:841] onDeltaRecord() [eventlog] Applied delta record with lsn=e9n6 ts=2021-09-17 02:14:35.965
I0917 02:14:36.036308    5254 [ld:srv:WB3] EventLogStateMachine.cpp:93] publishRebuildingSet() [eventlog] Published new EventLogRebuildingSet version (38654705670)
I0917 02:14:36.037121    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:777] scheduleRestartForShard() Scheduling a restart for shard 0 after 1ms
I0917 02:14:36.038024    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n6 ts=2021-09-17 02:14:35.928 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.039106    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n7 ts=2021-09-17 02:14:35.952 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.040036    5254 [ld:srv:WB3] MaintenanceLogWriter.cpp:55] operator()() Wrote record with lsn e8n6
I0917 02:14:36.040918    5254 [ld:srv:WB3] EventLogWriter.cpp:67] operator()() Wrote record with lsn e9n6
I0917 02:14:36.042385    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1036] writeRemoveMaintenance() Remove maintenance request for shard 0, reason=Shard came back with data intact
I0917 02:14:36.043996    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1869] abortForMyShard() Request to abort rebuilding of my shard 0 because: data is intact. But shard is dirty. Downgrading to time ranged rebuild.
I0917 02:14:36.045080    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1871] abortForMyShard() EventLogRebuildingSet NodeInfo: [A:{{[2021-09-17 02:09:59.859,2021-09-17 02:10:29.859]}}]
I0917 02:14:36.048994    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1872] abortForMyShard() Local dirty ranges: [A:{{[2021-09-17 02:09:59.859,2021-09-17 02:10:29.859]}}]
I0917 02:14:36.049798    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1883] abortForMyShard() Dirty ranges for my shard 0 successfully published
I0917 02:14:36.051040    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:472] noteRangesPublished() Noting dirty ranges published for shard 0
I0917 02:14:36.056591    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:683] trySlideGlobalWindow() Moving global window from +inf to -inf for shard 0 and rebuilding set N0:S0[A:{{[2021-09-17 02:09:59.859,2021-09-17 02:10:29.859]}}],N1:S0[A:{{[2021-09-17 02:10:00.092,2021-09-17 02:10:30.092]}}],N2:S0[A:{{[2021-09-17 02:10:00.550,2021-09-17 02:10:30.550]}}]
I0917 02:14:36.058341    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n8 ts=2021-09-17 02:14:36.040 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.064399    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:1179] operator()() [maintenancelog] Successfully wrote delta with lsn e8n9
I0917 02:14:36.065247    5254 [ld:srv:WB3] MaintenanceLogWriter.cpp:55] operator()() Wrote record with lsn e8n9
I0917 02:14:36.068027    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n9 ts=2021-09-17 02:14:36.059 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.078095    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n10 ts=2021-09-17 02:14:36.063 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.086011    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:1179] operator()() [maintenancelog] Successfully wrote delta with lsn e8n11
I0917 02:14:36.089155    5254 [ld:srv:WB3] MaintenanceLogWriter.cpp:55] operator()() Wrote record with lsn e8n11
I0917 02:14:36.092267    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n11 ts=2021-09-17 02:14:36.068 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.105513    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n12 ts=2021-09-17 02:14:36.080 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.112903    5254 [ld:srv:WB3] EventLogStateMachine.cpp:171] applyDelta() Applying delta ts=2021-09-17 02:14:36.083, e9n7: SHARD_NEEDS_REBUILD(nodeIdx=4, shardIdx=1, source=N4, details=RebuildingCoordinator, flags=TIME_RANGED|FILTER_RELOCATE_SHARDS)[A:{{[2021-09-17 02:10:00.332,2021-09-17 02:10:30.332]}}]
I0917 02:14:36.113744    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:841] onDeltaRecord() [eventlog] Applied delta record with lsn=e9n7 ts=2021-09-17 02:14:36.083
I0917 02:14:36.114750    5254 [ld:srv:WB3] EventLogStateMachine.cpp:93] publishRebuildingSet() [eventlog] Published new EventLogRebuildingSet version (38654705671)
I0917 02:14:36.116236    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:777] scheduleRestartForShard() Scheduling a restart for shard 1 after 1ms
I0917 02:14:36.118062    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1036] writeRemoveMaintenance() Remove maintenance request for shard 1, reason=Shard came back with data intact
I0917 02:14:36.120167    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1869] abortForMyShard() Request to abort rebuilding of my shard 1 because: data is intact. But shard is dirty. Downgrading to time ranged rebuild.
I0917 02:14:36.122831    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1871] abortForMyShard() EventLogRebuildingSet NodeInfo: [A:{{[2021-09-17 02:09:59.858,2021-09-17 02:10:29.858]}}]
I0917 02:14:36.124023    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1872] abortForMyShard() Local dirty ranges: [A:{{[2021-09-17 02:09:59.858,2021-09-17 02:10:29.858]}}]
I0917 02:14:36.124830    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1880] abortForMyShard() Cluster already rebuilding the correct dirty ranges for my shard 1. Converting abort into no-op.
I0917 02:14:36.130091    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:683] trySlideGlobalWindow() Moving global window from +inf to -inf for shard 1 and rebuilding set N0:S1[A:{{[2021-09-17 02:09:59.858,2021-09-17 02:10:29.858]}}],N1:S1[A:{{[2021-09-17 02:10:00.088,2021-09-17 02:10:30.088]}}],N2:S1[A:{{[2021-09-17 02:10:00.557,2021-09-17 02:10:30.557]}}],N4:S1[A:{{[2021-09-17 02:10:00.332,2021-09-17 02:10:30.332]}}]
I0917 02:14:36.131550    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:1179] operator()() [maintenancelog] Successfully wrote delta with lsn e8n14
I0917 02:14:36.135308    5254 [ld:srv:WB3] MaintenanceLogWriter.cpp:55] operator()() Wrote record with lsn e8n14
I0917 02:14:36.138821    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n13 ts=2021-09-17 02:14:36.084 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.144129    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n14 ts=2021-09-17 02:14:36.092 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.146292    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n15 ts=2021-09-17 02:14:36.094 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.153091    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:1179] operator()() [maintenancelog] Successfully wrote delta with lsn e8n16
I0917 02:14:36.154281    5254 [ld:srv:WB3] MaintenanceLogWriter.cpp:55] operator()() Wrote record with lsn e8n16
I0917 02:14:36.161719    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n16 ts=2021-09-17 02:14:36.144 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.189873    5203 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 4611686018427387902 epoch 3 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387902 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387902 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:14:36.191376    5254 [ld:srv:WB3] EventLogStateMachine.cpp:171] applyDelta() Applying delta ts=2021-09-17 02:14:36.125, e9n8: SHARD_NEEDS_REBUILD(nodeIdx=4, shardIdx=0, source=N4, details=RebuildingCoordinator, flags=TIME_RANGED|FILTER_RELOCATE_SHARDS)[A:{{[2021-09-17 02:10:00.337,2021-09-17 02:10:30.337]}}]
I0917 02:14:36.201344    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:841] onDeltaRecord() [eventlog] Applied delta record with lsn=e9n8 ts=2021-09-17 02:14:36.125
I0917 02:14:36.207122    5254 [ld:srv:WB3] EventLogStateMachine.cpp:93] publishRebuildingSet() [eventlog] Published new EventLogRebuildingSet version (38654705672)
I0917 02:14:36.214651    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:777] scheduleRestartForShard() Scheduling a restart for shard 0 after 1ms
I0917 02:14:36.215881    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n17 ts=2021-09-17 02:14:36.171 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.217746    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1036] writeRemoveMaintenance() Remove maintenance request for shard 0, reason=Shard came back with data intact
I0917 02:14:36.216542    5224 [ld:srv:WG2] EpochRecovery.cpp:566] startMutations() Log 13835058055282163710 epoch 3 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163710 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163710 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:14:36.218804    5203 [ld:srv:WG1] Mutator.cpp:400] finalize() Mutator 4611686018427387902e3n1 completed (OK). Trace: W1{N0:S0,N4:S0,N3:S0};X:N0:S0:K;X:N4:S0:K;X:N3:S0:K;Y1:N3:S0:K;Y1:N4:S0:K;Y1:N0:S0:K;done:can_replicate:OK;
W0917 02:14:36.229832    5203 [ld:srv:WG1] Worker.cpp:1000] onStoppedRunning() skipped at least 10 log entries
I0917 02:14:36.219948    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1869] abortForMyShard() Request to abort rebuilding of my shard 0 because: data is intact. But shard is dirty. Downgrading to time ranged rebuild.
W0917 02:14:36.240041    5203 [ld:srv:WG1] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.011s, source: [Message sent/received: MUTATED]
I0917 02:14:36.248699    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1871] abortForMyShard() EventLogRebuildingSet NodeInfo: [A:{{[2021-09-17 02:09:59.859,2021-09-17 02:10:29.859]}}]
I0917 02:14:36.261079    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1872] abortForMyShard() Local dirty ranges: [A:{{[2021-09-17 02:09:59.859,2021-09-17 02:10:29.859]}}]
I0917 02:14:36.265716    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1880] abortForMyShard() Cluster already rebuilding the correct dirty ranges for my shard 0. Converting abort into no-op.
I0917 02:14:36.273497    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:683] trySlideGlobalWindow() Moving global window from +inf to -inf for shard 0 and rebuilding set N0:S0[A:{{[2021-09-17 02:09:59.859,2021-09-17 02:10:29.859]}}],N1:S0[A:{{[2021-09-17 02:10:00.092,2021-09-17 02:10:30.092]}}],N2:S0[A:{{[2021-09-17 02:10:00.550,2021-09-17 02:10:30.550]}}],N4:S0[A:{{[2021-09-17 02:10:00.337,2021-09-17 02:10:30.337]}}]
W0917 02:14:36.277329    5254 [ld:srv:WB3] Worker.cpp:1000] onStoppedRunning() Slow request/timer callback: 0.060s, source: [Message sent/received: RECORD]
I0917 02:14:36.278910    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n18 ts=2021-09-17 02:14:36.182 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.281753    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n19 ts=2021-09-17 02:14:36.225 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.284049    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n20 ts=2021-09-17 02:14:36.230 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.287004    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n21 ts=2021-09-17 02:14:36.237 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.289056    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n22 ts=2021-09-17 02:14:36.271 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.290920    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n23 ts=2021-09-17 02:14:36.276 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.315371    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n24 ts=2021-09-17 02:14:36.285 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.316708    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n25 ts=2021-09-17 02:14:36.285 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.318873    5254 [ld:srv:WB3] EventLogStateMachine.cpp:171] applyDelta() Applying delta ts=2021-09-17 02:14:36.286, e9n9: SHARD_NEEDS_REBUILD(nodeIdx=3, shardIdx=1, source=N3, details=RebuildingCoordinator, flags=TIME_RANGED|FILTER_RELOCATE_SHARDS)[A:{{[2021-09-17 02:09:59.669,2021-09-17 02:10:29.669]}}]
I0917 02:14:36.320119    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:841] onDeltaRecord() [eventlog] Applied delta record with lsn=e9n9 ts=2021-09-17 02:14:36.286
I0917 02:14:36.325766    5254 [ld:srv:WB3] EventLogStateMachine.cpp:93] publishRebuildingSet() [eventlog] Published new EventLogRebuildingSet version (38654705673)
I0917 02:14:36.329076    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:777] scheduleRestartForShard() Scheduling a restart for shard 1 after 1ms
I0917 02:14:36.335401    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1036] writeRemoveMaintenance() Remove maintenance request for shard 1, reason=Shard came back with data intact
I0917 02:14:36.338525    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1869] abortForMyShard() Request to abort rebuilding of my shard 1 because: data is intact. But shard is dirty. Downgrading to time ranged rebuild.
I0917 02:14:36.340258    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1871] abortForMyShard() EventLogRebuildingSet NodeInfo: [A:{{[2021-09-17 02:09:59.858,2021-09-17 02:10:29.858]}}]
I0917 02:14:36.341163    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1872] abortForMyShard() Local dirty ranges: [A:{{[2021-09-17 02:09:59.858,2021-09-17 02:10:29.858]}}]
I0917 02:14:36.341975    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1880] abortForMyShard() Cluster already rebuilding the correct dirty ranges for my shard 1. Converting abort into no-op.
I0917 02:14:36.343134    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:683] trySlideGlobalWindow() Moving global window from +inf to -inf for shard 1 and rebuilding set N0:S1[A:{{[2021-09-17 02:09:59.858,2021-09-17 02:10:29.858]}}],N1:S1[A:{{[2021-09-17 02:10:00.088,2021-09-17 02:10:30.088]}}],N2:S1[A:{{[2021-09-17 02:10:00.557,2021-09-17 02:10:30.557]}}],N3:S1[A:{{[2021-09-17 02:09:59.669,2021-09-17 02:10:29.669]}}],N4:S1[A:{{[2021-09-17 02:10:00.332,2021-09-17 02:10:30.332]}}]
I0917 02:14:36.344167    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:1179] operator()() [maintenancelog] Successfully wrote delta with lsn e8n26
I0917 02:14:36.347817    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n26 ts=2021-09-17 02:14:36.291 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.348779    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n27 ts=2021-09-17 02:14:36.304 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.350752    5254 [ld:srv:WB3] MaintenanceLogWriter.cpp:55] operator()() Wrote record with lsn e8n26
I0917 02:14:36.352301    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n28 ts=2021-09-17 02:14:36.342 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.364117    5254 [ld:srv:WB3] EventLogStateMachine.cpp:171] applyDelta() Applying delta ts=2021-09-17 02:14:36.350, e9n10: SHARD_NEEDS_REBUILD(nodeIdx=3, shardIdx=0, source=N3, details=RebuildingCoordinator, flags=TIME_RANGED|FILTER_RELOCATE_SHARDS)[A:{{[2021-09-17 02:09:59.666,2021-09-17 02:10:29.666]}}]
I0917 02:14:36.368122    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:841] onDeltaRecord() [eventlog] Applied delta record with lsn=e9n10 ts=2021-09-17 02:14:36.350
I0917 02:14:36.368683    5254 [ld:srv:WB3] EventLogStateMachine.cpp:93] publishRebuildingSet() [eventlog] Published new EventLogRebuildingSet version (38654705674)
I0917 02:14:36.370635    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:777] scheduleRestartForShard() Scheduling a restart for shard 0 after 1ms
I0917 02:14:36.373642    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:1179] operator()() [maintenancelog] Successfully wrote delta with lsn e8n29
I0917 02:14:36.374699    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n29 ts=2021-09-17 02:14:36.353 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.378331    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n30 ts=2021-09-17 02:14:36.358 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.385689    5254 [ld:srv:WB3] MaintenanceLogWriter.cpp:55] operator()() Wrote record with lsn e8n29
I0917 02:14:36.388316    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1036] writeRemoveMaintenance() Remove maintenance request for shard 0, reason=Shard came back with data intact
I0917 02:14:36.389318    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1869] abortForMyShard() Request to abort rebuilding of my shard 0 because: data is intact. But shard is dirty. Downgrading to time ranged rebuild.
I0917 02:14:36.397700    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1871] abortForMyShard() EventLogRebuildingSet NodeInfo: [A:{{[2021-09-17 02:09:59.859,2021-09-17 02:10:29.859]}}]
I0917 02:14:36.397774    5235 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N3 changed from STARTING to FULLY_STARTED
I0917 02:14:36.405478    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1872] abortForMyShard() Local dirty ranges: [A:{{[2021-09-17 02:09:59.859,2021-09-17 02:10:29.859]}}]
I0917 02:14:36.407075    5235 [ld:srv:WF0] ClusterState.cpp:165] setNodeState() State of N4 changed from STARTING to FULLY_STARTED
I0917 02:14:36.408364    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:1880] abortForMyShard() Cluster already rebuilding the correct dirty ranges for my shard 0. Converting abort into no-op.
I0917 02:14:36.410335    5254 [ld:srv:WB3] RebuildingCoordinator.cpp:683] trySlideGlobalWindow() Moving global window from +inf to -inf for shard 0 and rebuilding set N0:S0[A:{{[2021-09-17 02:09:59.859,2021-09-17 02:10:29.859]}}],N1:S0[A:{{[2021-09-17 02:10:00.092,2021-09-17 02:10:30.092]}}],N2:S0[A:{{[2021-09-17 02:10:00.550,2021-09-17 02:10:30.550]}}],N3:S0[A:{{[2021-09-17 02:09:59.666,2021-09-17 02:10:29.666]}}],N4:S0[A:{{[2021-09-17 02:10:00.337,2021-09-17 02:10:30.337]}}]
I0917 02:14:36.412699    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n31 ts=2021-09-17 02:14:36.362 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.414170    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n32 ts=2021-09-17 02:14:36.375 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.426010    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:1179] operator()() [maintenancelog] Successfully wrote delta with lsn e8n34
I0917 02:14:36.428309    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n33 ts=2021-09-17 02:14:36.412 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.431277    5254 [ld:srv:WB3] MaintenanceLogWriter.cpp:55] operator()() Wrote record with lsn e8n34
I0917 02:14:36.432873    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n34 ts=2021-09-17 02:14:36.412 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.434547    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n35 ts=2021-09-17 02:14:36.417 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.452551    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n36 ts=2021-09-17 02:14:36.429 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.456499    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n37 ts=2021-09-17 02:14:36.433 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.466747    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n38 ts=2021-09-17 02:14:36.453 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.469138    5203 [ld:srv:WG1] EpochRecovery.cpp:566] startMutations() Log 4611686018427387902 epoch 4 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 1, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:4611686018427387902 N:LSN_INVALID T:0 AOM:{246:0} F:0], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:4611686018427387902 N:LSN_INVALID T:0 AOM:{246:0} F:0]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:14:36.477380    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n39 ts=2021-09-17 02:14:36.463 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.488827    5224 [ld:srv:WG2] EpochRecovery.cpp:566] startMutations() Log 13835058055282163710 epoch 4 final digest before mutation: consensus LNG 0, start esn: 1, 0 entries, first esn: 0, last esn: 0, bridge esn: 0, tail esn: 0, mutation set fmajority results: AUTHORITATIVE_COMPLETE, mutation set: {N0:S0, N1:S0, N2:S0, N3:S0, N4:S0}, last timestamp: 0, last timestamp from seals: 0. Tail record before this epoch: [L:13835058055282163710 N:e1n1 T:0 AOM:{246:0} F:16], Tail record from sealed: [L:0 N:LSN_INVALID T:0 AOM:{} F:0](Invalid), final tail record: [L:13835058055282163710 N:e1n1 T:0 AOM:{246:0} F:16]. Recovery set state when mutation started: {m:N0:S0,N1:S0,N2:S0,N3:S0,N4:S0 MUTATION}.
I0917 02:14:36.495181    5203 [ld:srv:WG1] Mutator.cpp:400] finalize() Mutator 4611686018427387902e4n1 completed (OK). Trace: W1{N1:S0,N2:S0,N0:S0};X:N1:S0:K;X:N2:S0:K;X:N0:S0:K;Y1:N2:S0:K;Y1:N0:S0:K;Y1:N1:S0:K;done:can_replicate:OK;
I0917 02:14:36.507900    5242 [ld:srv:WB1] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [logsconfig] Could not apply delta record with lsn=e6n1 ts=2021-09-17 02:14:36.493 on base with version e2n2: EXISTS, Adding LogGroup with the path "/test_logs" which already exists in the tree!
I0917 02:14:36.508023    5254 [ld:srv:WB3] ReplicatedStateMachine-inl.h:835] onDeltaRecord() [maintenancelog] Could not apply delta record with lsn=e8n40 ts=2021-09-17 02:14:36.496 on base with version e0n1: NOTFOUND, The filter did not match any maintenances
I0917 02:14:36.517741    5200 [ld:srv:WG0] Connection.cpp:724] close() Closing socket C35(127.0.0.1:51660). Reason: PEER_CLOSED: connection closed by peer
I0917 02:14:36.519271    5203 [ld:srv:WG1] Connection.cpp:724] close() Closing socket C41(127.0.0.1:51704). Reason: PEER_CLOSED: connection closed by peer
I0917 02:14:36.520368    5228 [ld:srv:WG4] Connection.cpp:724] close() Closing socket C40(127.0.0.1:51702). Reason: PEER_CLOSED: connection closed by peer
I0917 02:14:36.555840    5228 [ld:srv:WG4] Connection.cpp:724] close() Closing socket C47(127.0.0.1:51780). Reason: PEER_CLOSED: connection closed by peer
I0917 02:14:36.558327    5203 [ld:srv:WG1] Connection.cpp:724] close() Closing socket C39(127.0.0.1:51700). Reason: PEER_CLOSED: connection closed by peer
I0917 02:14:36.559323    5225 [ld:srv:WG3] ProtocolHandler.cpp:127] notifyErrorOnSocket() skipped at least 10 log entries
I0917 02:14:36.564449    5225 [ld:srv:WG3] ProtocolHandler.cpp:127] notifyErrorOnSocket() Socket C31(127.0.0.1:51614) hit error AsyncSocketException: Socket read end of file., type = End of file
I0917 02:14:36.561433    5224 [ld:srv:WG2] RecoveryNode.cpp:401] onDisconnect() Lost connection to N4:S0 while waiting for reply in state CLEANING during recovery of epoch 4 of log 13835058055282163710: PEER_CLOSED: connection closed by peer
I0917 02:14:36.598792    5224 [ld:srv:WG2] RecoveryNode.cpp:401] onDisconnect() Lost connection to N3:S0 while waiting for reply in state CLEANING during recovery of epoch 4 of log 13835058055282163710: PEER_CLOSED: connection closed by peer
I0917 02:14:36.615278    5235 [ld:srv:WF0] FailureDetector.cpp:1541] onGossipMessageSent() Could not send gossip to WF0:N4:1 (127.0.0.1:48410): CONNFAILED: connection failed. Trying another node.
I0917 02:14:36.634150    5224 [ld:srv:WG2] RecoveryNode.cpp:401] onDisconnect() Lost connection to N2:S0 while waiting for reply in state CLEANING during recovery of epoch 4 of log 13835058055282163710: PEER_CLOSED: connection closed by peer
I0917 02:14:36.658915    5224 [ld:srv:WG2] RecoveryNode.cpp:401] onDisconnect() Lost connection to N1:S0 while waiting for reply in state CLEANING during recovery of epoch 4 of log 13835058055282163710: PEER_CLOSED: connection closed by peer
I0917 02:14:36.661752    5203 [ld:srv:WG1] RecoveryNode.cpp:401] onDisconnect() Lost connection to N1:S0 while waiting for reply in state CLEANING during recovery of epoch 4 of log 4611686018427387902: PEER_CLOSED: connection closed by peer
